# log-archive-send.conf
#
# The values specified here are used to control the behavior of the
# log-archive-send.pl script.
#
# Copyright (c) 2016 GroundWork, Inc. (www.gwos.com).  All rights reserved.
# Use of this software is subject to commercial license terms.

# ====================================================================
# GroundWork Log Archive Capture and Sending Configuration Parameters
# ====================================================================

# Archive, or even just purge old rows from, the log tables?
#
# This option is turned off (the value "no", but without quotes) in the default
# configuration file simply so the script can be safely installed before it is
# locally configured.  To get the software to run, it must be turned on here
# (the value "yes", but without quotes) once the rest of the setup is correct
# for your site.
# [yes/no]
enable_processing = no

# Possible debug_level values:
# 0 = no info of any kind printed, except for startup/shutdown
#     messages and major errors
# 1 = print just error info and summary statistical data
# 2 = also print basic debug info
# 3 = print detailed debug info
debug_level = 1

# Where to log audit and debug messages.
logfile = "/usr/local/groundwork/foundation/container/logs/log-archive-send.log"

# Whether to send captured data to an archive database to reside there.
# This flag shoud be turned on ("yes") if you wish to use the full capability
# of the log-archive software.  It should be turned off ("no") if you wish
# to use only the automated data-purging capabilities of this software.
# [yes/no]
send_data_to_archive_database = yes

# Everything we need to know to access the runtime database.
runtime_dbtype = postgresql
runtime_dbhost = localhost
runtime_dbport = 5432
runtime_dbname = gwcollagedb
runtime_dbuser = collage
runtime_dbpass = gwrk

# The following two values will be used in messages sent to the Foundation
# (and appearing in the Event Console).  If the log-archive-send.pl script
# runs on the same machine as the foundation_host defined below, then you can
# safely set source_script_machine to "localhost", provided that you generally
# monitor your GroundWork Monitor server under that designation.  In all other
# conditions, you should give the actual hostname of the server on which the
# log-archive-send.pl script runs, as that host is known to your monitoring
# system (as an unqualified or fully-qualified hostname).
#
# The source_script_ip_address reflects the IP address of source_script_machine.
# You can default this value by setting it to an empty string ("").  If you
# set source_script_machine to "localhost", an empty source_script_ip_address
# will default to "127.0.0.1".  If you set source_script_machine to some
# other value, an empty source_script_ip_address will default to some
# arbitrary IP address assigned to this host (whichever IP address is found
# first in the normal name-system lookup of the source_script_machine).
# Especially if your source_script_machine has multiple network interfaces,
# you may wish to override this default with a particular IP address of the
# source_script_machine as this machine is known to your monitoring system.
#
source_script_machine    = "my-source-server.my-domain.com"
source_script_ip_address = ""

# The machine where all the dumped data will be sent, and where the data
# injection scripting will be run.  This is usually, but not necessarily,
# where the archive database itself resides.  Specify "localhost" if the
# log-archive-send.pl and log-archive-receive.pl scripts will be run on the same
# machine, regardless of where the runtime and archive databases themselves
# physically reside.  Otherwise, specify the hostname of the machine on which
# the log-archiving scripting that will populate the archive database (that is,
# log-archive-receive.conf and log-archive-receive.pl) resides.
target_script_machine = localhost

# Specification of what to archive is split into several levels, based both on
# the perceived importance of the tables and on their referential relationships.
# Because of the foreign-key references between these tables, they must be
# restored into the target database in this order:  primary, secondary,
# tertiary.  And to that end, to ensure that we will have captured all the
# references we need when we dump these tables, we save them in this order:
# tertiary, secondary, primary.

# Note that the set of primary tables would have been derived automatically
# by the code from the lists of secondary and tertiary tables, were it not
# for this statement in the PostgreSQL 9.1.2 release notes, found at:
# http://www.postgresql.org/docs/9.1/static/release-9-1-2.html
#
#     E.6.2. Changes
#
#     Fix bugs in information_schema.referential_constraints view (Tom Lane)
#
#     This view was being insufficiently careful about matching the foreign-key
#     constraint to the depended-on primary or unique key constraint.  That could
#     result in failure to show a foreign-key constraint at all, or showing it
#     multiple times, or claiming that it depends on a different constraint than
#     the one it really does.
#
# Since we want the log-archive software to work reliably on GWMEE 6.7.0 systems
# that include PostgreSQL 9.1.1, we are forced to list these tables explicitly
# instead of allowing the software to automatically discover the full set of
# ancillary tables that must be archived.
#
# Possibly, we could get around that problem by directly using the underlying
# information_schema database tables instead of this view, to find the ancillary
# tables we care about.  We have not yet spent the time to investigate
# that possibility, as our time so far has been spent on the rest of the
# functionality in the log-archive software.

# Here we specify the set of tables which are needed to support the secondary
# tables to be archived because of foreign-key references to the primary tables
# within the secondary tables.  In contrast to the secondary tables, the
# entirety of each of the tables listed here will be archived on every pass.
#
# It is vital that you maintain the specified ordering of tables listed here.
primary_table = hoststatus
primary_table = servicestatus
primary_table = host
primary_table = typerule
primary_table = statetype
primary_table = severity
primary_table = priority
primary_table = performancedatalabel
primary_table = operationstatus
primary_table = monitorstatus
primary_table = device
primary_table = component
primary_table = checktype
primary_table = applicationtype

# Here we specify those tables which are the critical part of archiving, and
# the respective "timestamp without time zone" fields within those tables
# that define data-related time intervals upon which archiving will be based.
# In particular, the specified fields will determine not only what data we
# extract, but also what data we delete from these tables.
#
# It is vital that you maintain the specified ordering of tables listed here.
secondary_table_and_field = "logmessage.reportdate"
secondary_table_and_field = "logperformancedata.lastchecktime"
secondary_table_and_field = "auditlog.logtimestamp"
secondary_table_and_field = "comment.createdon"

# Here we specify an additional set of tables to archive, mostly those that
# reference the secondary tables and may also be useful for reporting purposes.
#
# Normally, the entirety of each of the tables listed here will be archived on
# every pass.  However, if there is any special joining to some other table to
# be applied during data capture, it must be specified as a single ON-clause
# join condition after a semicolon appended to the table name.  In that case,
# additional filtering may be applied within the sending script, to guarantee
# consistency of the captured data across tables.
#
# It is vital that you maintain the specified ordering of tables listed here.
tertiary_table_and_join = "logmessageproperty;logmessageid=logmessage.logmessageid"
tertiary_table_and_join = "hostgroupcollection"
tertiary_table_and_join = "applicationentityproperty"
tertiary_table_and_join = "hostgroup"
tertiary_table_and_join = "propertytype"
tertiary_table_and_join = "entitytype"
tertiary_table_and_join = "hostname"
tertiary_table_and_join = "hostidentity"
tertiary_table_and_join = "hostblacklist"

# Specify here which tables are used for message and performance data.  These
# specifications are used to correlate parameters for runtime-database deletion
# with the specific tables affected by those parameters, and to identify which
# statistics should be emphasized in status messages sent to the Event Console.
#
# If there is more than one table in a given category, you can repeat its option
# as above to specify the full list of tables.  These values must be configured
# above as secondary tables, with associated time fields.
message_data_table = "logmessage"
message_data_table = "auditlog"
message_data_table = "comment"
performance_data_table = "logperformancedata"

# We expect that the archiving logic will evolve over time, and as such the
# format of the dumpfile we create might change over time.  So we preemptively
# defined a configuration variable to specify that format.  Currently, the
# only supported format is "copy".  Later releases might support other formats
# as well.
dumpfile_format = copy

# How many rows to buffer and copy to a dump file at a time, when capturing data
# from a table.  This is normally set to a reasonably large number like 10000 so
# the file creation is efficient.
dump_copy_block_rows = 10000

# Here we specify the minimum number of hours of new data that must be included
# in the time span for each archiving cycle.  This provides a constraint against
# the archiving running too soon after the last successful run, and thereby
# wasting a lot of system resources.  However, the more significant constraint
# is that the endpoint of each cycle is always set to 00:00:00 today, which
# effectively prevents additional cycles from running until the day after a
# successful archiving run.  This parameter will become more directly applicable
# if we extend the sending script to allow command-line specification of the
# archiving period.
#
# This interval is normally set to a bit less than a day, such as 18 hours, to
# allow for short days which occur at Daylight Savings Time transitions.
minimum_additional_hours_to_archive = 18

# Here we specify the minimum number of days of old data (going back from
# 00:00:00 today) are to be included in each secondary-table dump file, under
# nominal conditions.  If the cron job that runs the archive sending runs every
# night, this can be set to 1 day.  If the cron job is set to run less often,
# or if you simply want some degree of overlap in the data in the dump files
# from successive runs, you can set this to a larger number of days.  This
# period establishes the minimum amount of data in the dump file; additional
# data will also be included, depending on the information we find in the
# log_archive_source_state_file about the success of previous archiving runs
# and the time periods they covered.
#
# Note that this parameter applies mainly to dumping of the secondary tables.
# Dump files for the primary and tertiary tables will always include all
# available data in the runtime database for those tables, except when a join
# condition is specified above for a tertiary table.
dump_days_minimum = 2

# Here we specify the farthest we will go back in time for the data in each
# secondary-table dump file, even if data prior to that point never got
# archived.  Normally we set this to a near-infinite value, like 10000, as we
# want to sweep up all old data not previously deleted, even if that data has
# already been archived in a previous pass (so there will be significant overlap
# in the data archived in successive passes).  In some specialized cases, we
# might want to be more restrictive, so we allow that control here.  In general,
# this value should be kept very high to allow for the possibility that the cron
# job on any given night might end up being skipped for reasons outside the
# control of the log-archive scripting.  You don't want this limitation to force
# some unarchived data to be skipped because the value was set too low.
#
# Note that this parameter applies only to dumping of the secondary tables.
# Dump files for the primary and tertiary tables will always include all
# available data in the runtime database for those tables, except when a join
# condition is specified for a tertiary table.
dump_days_maximum = 10000

# This parameter controls how long message data should be retained in the
# runtime database after it is last updated there.  Keeping such data available
# in the runtime database for some period can be important for operational
# purposes.
#
# See the documentation for a full explanation of data retention and this
# parameter.  If you wish to be cautious upon first installation of the
# archiving software, set this to a very high value, like 10000, until you are
# confident that the log-archive mechanism is set up and working properly at
# your site.  Then turn it down to whatever actual number of days for which it
# makes sense to retain this data in the runtime database.  This value should
# generally be set at least as large as dump_days_minimum.
operationally_useful_days_for_messages = 92

# This parameter controls how long performance data should be retained in the
# runtime database after it is last updated there.  The copy of the performance
# data in the runtime database has no real operational utility, as it is only
# saved there for reporting purposes, which will now be better served by the
# archive database.  So this parameter is usually set to 0.  However, in the
# GWMEE 7.0.1 release, we set it to a higher value to keep the data still
# available in the runtime database until we have reports that access the
# archive database instead.
#
# See the documentation for a full explanation of data retention and this
# parameter.  If you wish to be cautious upon first installation of the
# archiving software, set this to a very high value, like 10000, until you are
# confident that the log-archive mechanism is set up and working properly at
# your site.  Then turn it down to whatever actual number of days for which it
# makes sense to retain this data in the runtime database.  This value should
# generally be set at least as large as dump_days_minimum.
operationally_useful_days_for_performance_data = 92

# This parameter provides the ability to maintain historical message data in the
# runtime database until it has lain in the archive database for this long.  See
# the documentation for a full explanation of why you would want this level of
# redundancy.
post_archiving_retention_days_for_messages = 2

# This parameter provides the ability to maintain historical performance data in
# the runtime database until it has lain in the archive database for this long.
# See the documentation for a full explanation of why you would want this level
# of redundancy.
post_archiving_retention_days_for_performance_data = 2

# How many days of dumped-data files to retain in the source-machine filesystem,
# as a means of allowing manually-initiated recovery procedures.  For safety,
# the code insists that this value be at least 1.  This may be a floating-point
# number, but because our config-file parser currently only accepts expressions
# containing integers, such a number must be specified as a calculation (e.g.,
# use 1 + 1/2 to generate 1.5).  Specifying a fractional day can help to avoid
# any boundary conditions regarding the exact times when successive daily passes
# of the log-archive-send.pl script are run and attempt to delete the files from
# earlier passes.
source_dumpfile_retention_days = 11 + 1/2

# The base directory in which subdirectories will be managed for storing
# data dumps from the runtime database.  Subdirectories will have the form
# "YYYY-MM-DD", representing the date on which the data-dump files within
# them were taken.  This does not necessarily relate to any timestamps
# within those data dumps.  The dump-file names within those subdirectories
# will be of the form "{tablename}.dump.{timestamp}", where the timestamp
# is specified in the form "YYYY-MM-DD_hh.mm.ss".  Once again, the timestamp
# in each filename will represent when the data-dump file was taken, and it
# does not necessarily relate to any timestamps within the dump file itself.
log_archive_source_data_directory = "/usr/local/groundwork/core/archive/log-archive"

# The equivalent base directory on the target machine, under which
# subdirectories will be created to hold data dumps from the source machine.
log_archive_target_data_directory = "/usr/local/groundwork/core/archive/log-archive"

# Location of the log-archiving generational state file, if needed.  This file
# stores information about how far we get in each archiving cycle, because that
# will be useful in calculating the set of data to be handled in the following
# cycle.  This particularly helps with separating data into day-by-day dump
# files, and knowing which have been handled to completion and which remain to
# be completely sent to the receiving system and archived there.
log_archive_source_state_file = "/usr/local/groundwork/core/archive/var/log-archive-send.state"

# The location of Foundation on the runtime server, needed so we can send
# "archiving run complete" and perhaps other log-archiving status events to the
# Event Log.  If the log-archive-send.pl script is run on the runtime machine
# (a common case), specify foundation_host as "localhost".  Otherwise, the
# foundation_host value specified here must be the hostname of the runtime
# server, or you can set foundation_host to an empty string ("") if you wish to
# disable such messages.
foundation_host = "my-groundwork-runtime-server.my-domain.com"
foundation_port = 4913
