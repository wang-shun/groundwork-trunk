#!/usr/bin/perl -w --
#!/usr/local/groundwork/perl/bin/perl -w --

# FIX MINOR:
# We will use the generic OS-supplied Perl for initial testing, until we package
# this all up for delivery.  At that time, we will evaluate the situation to see
# if there is some reason to use or not use a particular version of Perl.
#
# Resolution of this issue for running this against GW 6.1.X:  We must use the
# OS-supplied Perl for the time being because it typically contains the syscall.ph
# file, which our current GroundWork Perl does not (GWMON-8508).
#
# That means that all of our extra Perl packages must be compiled using the
# OS-supplied Perl, which means that other scripting which references those
# packages must typically also run the same version of Perl.

# Replication state engine for Disaster Recovery control.
# Copyright (c) 2010 GroundWork Open Source, Inc. ("GroundWork").  All rights
# reserved.  Use is subject to GroundWork commercial license terms.

# ================================================================
# Development tasks.
# ================================================================

# Critical issues:
# (*) Problem when trying to remove NMS RPMs:
#     rpm -e `rpm -qa | fgrep groundwork-nms`
#     error: Failed dependencies:
#         perl(RPC::PlClient) >= 0.2000 is needed by (installed) groundwork-disaster-recovery-0.0.1-15823.x86_64
#         perl(RPC::PlServer) >= 0.2001 is needed by (installed) groundwork-disaster-recovery-0.0.1-15823.x86_64
#     Somehow the groundwork-disaster-recovery RPM went onto the system without
#     requiring these modules.  So how is it that I cannot remove the NMS RPMs
#     afterward?
# (*) GWMON-8412:  See Mark's comments about the variability of cron jobs, and
#     get to the bottom of what we really need to recognize and support while
#     capturing and deploying cacti and nedi applications and databases.
# (*) Restart nedi cron jobs if capture/deploy action scripts killed them.
# (*) To replace a cron script, have the RPM make a timestamped copy and a
#     named-extension copy of the existing script, as well as add a related
#     dummy script -- but only on a first install for the extra copies.
# (*) I've seen this happen when shutting down the Replication Engine, when the
#     foundation.deploy script was running when the shutdown request came in:
#         [Wed May 12 16:44:36 2010] 7264: !!! Child process PID:14942 reaped: 
#         [Wed May 12 16:44:36 2010] 7264: !!! Your program may not be using sig_child() to reap processes.
#         7264: !!! In extreme cases, your program can force a system reboot
#         7264: !!! if this resource leakage is not corrected.
#         [Wed May 12 16:44:36 2010] === Shutdown requested; terminating (process 7264). ===
#     Track down whatever resources are being killed that ought to wait until
#     such a script finishes before going down.
# (*) I've seen this happen once when shutting down the Replication Engine:
#         [Sun May  2 20:45:49 2010] === Shutdown requested; terminating (process 17094). ===
#         panic: leave_scope inconsistency.
#     The cause of this panic ought to be tracked down.  In theory it shouldn't
#     matter much because it happened on the way out, but it might be indicative
#     of a more serious problem.
# (*) Action scripts included in RPM -- better .svn filtering?
# (*) Compare our implementation of blocking of replication actions to the
#     replication_is_enabled local configuration setting, and probably make
#     logic changes in just the same places that flag is tested.
# (*) Handle possible app, db linkages that might cause operational collisions
#     (schedule around this issue, for now).
# (*) Replication failures are clearly visible in the Event Console on the
#     DR system, but not in the Event Console on the Primary system, which
#     is where the administrators are far more likely to be looking.  Make
#     sure that capture failures, for instance, do generate local Foundation
#     errors.  Test by using too-low capture timeouts.  For failures that
#     happen entirely on the remote side, perhaps the simplest way to notice
#     them is to check on each heartbeat analysis if the other side has some
#     objects in a "stalled" state, and warn locally (log, foundation) if so.

# High-priority issues:
# (*) Deal somehow with a running monarch_discover.cgi in the various monarch
#     application and database capture/deploy scripts.
# (*) put in checks to ensure that the local host does not resolve to 127.0.0.1
# (*) Verify my primary/secondary hostname qualification checks (vs. loopback,
#     and comparing primary/secondary configured hostnames).
# (*) The Replication State Engine always assumes it is safe to restart the
#     application and database objects after a sync operation.  But what if
#     the user has intentionally brought them down?  Even if we take no action
#     to recognize that situation within the Replication State Engine, we need
#     to provide documentation to tell the administrators that they must now
#     block sync operations before taking an object out of service, lest it be
#     brought back on-line at an inopportune time.

# Medium-priority issues:
# (*) We have seen the following happen.  A deploy action sometimes needs to
#     take down a system facility such as nagios or gwservices, in order to
#     install a new configuration from the remote side.  While it is down, for
#     some reason the administrator bounces the Replication State Engine.  This
#     interrupts the deploy operation mid-stream, which means that it may not
#     have completed all its actions, and the configuration it leaves behind
#     may be incomplete or inconsistent.  When the Replication State Engine
#     starts up again, it may not notice that the system facility is no longer
#     running, so it will stay down until the next deploy action for that
#     facility, which will install another new configuration and bring it back
#     up again.  But it could be a long time before that next deploy action is
#     scheduled to run.
#     >> In some cases, the downed facility may be otherwise noticed.  For
#     instance, if nagios is down, the check-heartbeat-nagios check initiated
#     from the remote side will find that and alter the state on the remote
#     side in a manner which will be soon noticed by humans, who can get the
#     situation corrected.  We should check to make sure any similar facilities
#     are also in some way set up to verify on a regular basis that they are
#     running (outside of the deploy periods when we know they might be
#     expected to be down).
#     >> Also note that it is for reasons such as this that we have a full,
#     complex protocol for saving configurations before we roll them out of
#     production.  Now that we have seen a particular situation that can arise
#     that might create the need to bring back such a saved configuration, we
#     should look at completing the "rollback" command and related capabilities.
#     >> Another thing we could do to reduce the impact of this scenario is to
#     enhance the mechanism for shutting down the Replication State Engine,
#     so if a deploy operation is in progress, we try to wait a short while
#     for it to complete, or perhaps signal it to get quickly to a safe state.
#     >> And we could go further, by using a two-phase-commit protocol for the
#     deploy action, to mark the action as in-progress, and then remove the
#     flag at the end of the action.  That would allow the Replication State
#     Engine to, when it starts up, either complete or roll back any incomplete
#     action.
# (*) Figure out why, if a child process group is terminated with a signal and
#     on_child_signal() is called, it can take several tens of seconds before
#     on_child_close() is finally called.  Perhaps some descendant of the child
#     script does not immediately die for some reason.
# (*) We ought to have a mechanism for queueing messages sent to Foundation that
#     don't make it through ("cannot open TCP socket 4913 to host localhost")
#     because Foundation is currently down (quite possibly because one of our
#     own replication actions has temporarily brought it down).
# (*) What if we receive a result for an operation that already timed out?
#     Did our timeout kill the local child tree?  What about a remote callback?
#     Do we need a remote cancel and also suppress for awhile to see if we
#     get a response either to the initial request or to the cancel request?
# (*) Check to see if, when a child process exits, whether we turn off any
#     pending timeout for that child, and drop the child PID and so forth
#     from the parent session's heap.
# (*) Test from scratch the creation of all app and db directories needed for
#     sync operation after initial RPM install.
# (*) "replication_is_enabled" and other leftover state data from the original
#     implementation should be either cleaned out or populated.
# (*) In a future release, we might consider including part of the analyze()
#     code with respect to who has Notification Authority Control and such
#     into the check_replication plugin, to generate notifications if the
#     Notification Authority Control configuration is seen to be bad.
#     More generally, we ought to scan through the Replication Engine
#     script and figure out what other kinds of conditions ought to be
#     brought to the attention of Nagios, and how.  For instance, if serious
#     replication errors have recently occurred, that could be included in
#     the check_replication result as well.
# (*) Extend the output of "status heartbeat" to be more descriptive about
#     current state:
#     (+) the initial startup period waiting for stable heartbeat data from
#         the remote system
#     (+) whether the system is considering a state transition
#     (+) whether the system is operating in normal mode or in failure mode
#     (+) a note on whether the remote state is stale
#     (+) a summary note on whether any applications or databases are stalled
#         or appear to be in a permanently-active state
#     Or make such detail be the output of the "status engine" command, which
#     is currently suppressed.

# Low-priority issues:
# (*) If the Replication Engine is stopped via SIGTERM, it can shut down most
#     facilities right away, but not exit until some child process completes.
#     One of the things it releases quickly is the listening sockets.  Thus
#     another copy of the Replication Engine can start right away and not be
#     prevented from doing so by being unable to bind to the sockets.  We
#     should either provide some other locking mechanism to prevent concurrent
#     Replication Engines from running, or simply arrange so that the sockets
#     are the last thing let go of before the Replication Engine finally exits.
# (*) GWMON-8573:  Status Viewer extension to directly update a remote system
#     with several types of data, with possible store-and-forward semantics
#     in the event of a failure to communicate.  (In the initial delivery,
#     this is handled instead by periodically replicating the Nagios state
#     retention file (nagios/var/nagiosstatus.sav), say once per hour.  This
#     will leave an hour of vulnerability during which such changes might
#     never make it over to the DR side.)
# (*) Application stop/start scripts working and invoked where they need to be,
#     in particular within capture and deploy operations.  This may take some
#     restructuring of the program logic, to take such operations out of the
#     sequence of actions managed in the replication state engine and push them
#     down into the action scripting.  (The stop and start actions are now
#     buried within capture and deploy operations as necessary. The separate
#     stop and start scripting still runs but is implemented in each case as
#     a no-op. This can be cleaned up in some future release.)
# (*) We could use an interactive commmand to list out the next scheduled
#     replication time for each application and database.
# (*) Add the REPLICATION ApplicationType to our base-product fresh-install-seed
#     and migration scripting.
# (*) Extend the grammar on many commands so that typing a partial command,
#     stopping before all the parameters are supplied, outputs a description
#     of the rest of the command parameters and may also list the particular
#     choices there, such as the available application or database names.
#     This would be preferable to the current behavior of just telling you
#     to type "help {command}" without further details.
# (*) Try to build on a uname -i == i686 machine, to ensure we have prepared the
#     necessary filelist.
# (*) Try to build on Ubuntu (cope with bad uname output).
# (*) Look at the pro/trunk/monitor-nms/nms-2.1.2/conf/patch-nms-2.1.2.sh script
#     for an alternative construction for installing a cron job from a script.
# (*) Verify that in POE::Wheel::Run->new(), the NoSetPgrp action is invoked
#     before the NoSetSid action, so the latter call does not fail because it
#     is already a process group leader.  (Checked:  setsid() is called if the
#     conduit is a pty, and setpgrp() is called otherwise.  They're not both
#     called.  Figure out if that makes sense.  The documentation should be
#     clearer about this, as it just mentions both options without saying when
#     each will really be invoked or ignored.)
# (*) At replication state engine startup time, verify that all action scripts
#     are available and executable.  (I believe we end up checking this later
#     on when the scripts are eventually invoked, but dealing with errors up
#     front when people are more likely to be around to fix the problems than
#     when some of these scripts run in the middle of the night seems sensible.)
# (*) Figure out what to do about Commit operations temporarily overriding the
#     Notification Authority setting established by DR processing.
#     (Actually, the resolution for now is simple -- merely providing advice to
#     enable notifications on the Primary system and disable them on the DR
#     system, as would have been the case in the usual Standby setup.  These
#     represent default states that will be temporarily established immediately
#     after Nagios is bounced after a Commit operation.  The Disaster Recovery
#     Replication Engine will re-assert its control shortly thereafter, enabling
#     or disabling notifications on each side as calculated by the regular
#     heartbeat analysis.  This may leave a small window [under a minute] where
#     notifications might "misbehave" before Replication steps in, but with
#     these defaults, it seems unlikely that this will be a production issue.
#     Also, typically the Monarch setting won't even take effect right after a
#     Commit, given that the value in the Nagios state retention file will
#     generally be used instead.)

# ================================================================
# Bug reports.
# ================================================================

# (*) Suggest that some variant of the known_kernels(), kernel_is_known(),
#     and kernel_is_connected() routines in this code be incorporated into
#     the standard release of POE::Component::IKC::Responder so applications
#     can find out this information in a standard, supported manner.
# (*) Report a problem using the POE::Component::IKC::Responder->subscribe()
#     callback.  If you specify a state rather than a coderef, it's not at all
#     clear what session the state will be relative to, and what you have to
#     do to get it pinned down to a particular session so your state handler
#     actually gets called.  Conversely, if you use a coderef, you need to
#     know that the callback will be called with the specified arrayref in
#     $_[0] rather than in $_[ARG0].
# (*) Report a problem with the POE::Component::IKC::Responder->subscribe()
#     documentation.  The doc says the subscription receipt timeout is set to
#     120 seconds, but the code sets it at 60 seconds.
# (*) Report the issue of Responder->register() not supporting independent
#     unidirectional channels between servers A and B, as noted in comments
#     later in this script, instead forcing the use of a single peer-to-peer
#     connection.  This fact must be documented, not just with respect to how
#     it affects your application architecture, but also in the details of how
#     this pattern must be implemented with careful use of "name" and "aliases"
#     parameters in the Component::IKC::Client->spawn() call when making the
#     connection.  Also note possible race conditions when such peers both try
#     to contact each other at the same time, and document a known-working
#     pattern of how this can be handled with complete safety.  Also note the
#     implications when one side or the other wants to shut down its direction
#     of the communication, given that it might have been originally established
#     by a client from the other side connecting to your local server.  Is this
#     even possible, and how do you do it?
# (*) Report a problem in the POE::Wheel::Run documentation, with respect to how
#     the on_child_close() and on_child_signal() routines delete the wheel.  In
#     particular, if the code in the SYNOPSIS is followed, then the later text
#     under CloseEvent that says that the CHLD handler may be called before or
#     after CloseEvent is simply wrong.  The CHLD handler will never be called
#     "before" the CloseEvent handler, because if the CHLD handler runs first,
#     as sometimes does happen, the entire wheel will be destroyed, and its
#     CloseEvent handler will therefore never be called afterward.  Provide an
#     alternative construction, such as we must now use in our own code below,
#     and explain the conditions under which it might be useful (i.e., when you
#     need to use the on_child_close() routine to clean up after all the child
#     output has been processed, and for whatever reason the on_child_signal()
#     handler cannot be used for that purpose).  (If on_child_signal() could be
#     so used, what would be the point of having on_child_close() at all?)
#     For example, suppose that the direct child is just a startup script, and
#     you're interested in capturing the entire output of a grandchild, which
#     may persist for quite some time after the child has exited.
# (*) Report a somewhat different problem with POE::Wheel::Run, that should
#     at least be documented as a possible issue for application developers.
#     Even with a corrected means of calling it, with the fixes for how
#     on_child_close() and on_child_signal() destroy only their own copy of
#     the wheel, we have sometimes seen only on_child_signal() called, and
#     on_child_close() never called thereafter.  It seems that the direct child
#     process dies, but it kept open some file descriptor from the parent and
#     passed it on to one of its descendant processes that probably doesn't
#     know anything about it and never closes it.  With that connection to some
#     descendant process still hanging around, the file descriptor(s) in the
#     parent are never closed and the on_child_close() routine will not get
#     called until the descendant process eventually dies.  Possibly there
#     ought to be some option to POE::Wheel::Run (CloseOnCall, perhaps?) that
#     would effectively cause the death of the direct child process to trigger
#     a call to the on_child_close() routine so only the output of the direct
#     child process is seen by the parent process.
# (*) Report a bug in the POE-1.287 POE::Wheel::Run.  In a process running with
#     different euid/egid and ruid/rgid values, it is the ruid/rgid values that
#     are inherited by a child process rather than the euid/egid values.  This is
#     highly unexpected and needs to be either fixed or documented.
# (*) Report a bug in POE::Kernel, in that sometimes an event which is scheduled
#     via $_[KERNEL]->alarm() apparently executes one second early (that is, while
#     the system clock is still reading a time which is one second earlier than the
#     time at which the event is scheduled to execute).  For instance an event which
#     is scheduled to be invoked at 13:40:00 might instead execute at 13:39:59.
#     This can throw off calculations of when the next such event ought to occur,
#     since it might appear that the next regular event ought to run at 13:40:00.
#     At the very least, this possibility needs to be documented; it can generally
#     be coped with as long as the programmer knows it might happen.
# (*) Report a bug in the CPAN PathTools-3.31 Cwd module.  realpath() and friends
#     will chdir() in the course of analyzing the path, and if they cannot chdir()
#     back again at the end to the original starting point, they will return undef
#     and leave the process in some unknown working directory.  This is bad in a
#     couple of ways.  First, both aspects of this behavior are undocumented, and
#     second, changing the working directory in this way could lead to unexpected
#     trashing of the filesystem.
# (*) POE emits the following message:
#         <sg> Kernel now running in a different process (is=15868 was=15867).
#         You must call call $poe_kernel->has_forked in the child process.
#     Report the "call call" duplication bug.
# (*) Report this problem with the POE::Wheel::Run documentation:  where is
#     the ErrorEvent handled?  In the parent process, or in the child process?
#     It may make a lot of difference.  And what happens if an exec() in the
#     child fails?  Does the child process continue to run (and effectively do
#     the same kinds of things that the parent had already scheduled) unless it
#     intentionally dies immediately at that point by programmer control?
#     Apparently what happens on an exec() failure is that the die() messages
#     get written to the child's STDOUT and are seen there on the parent's
#     STDOUT handler, but the child continues to run thereafter if you're
#     running POE::Wheel::Run->new() inside an eval{} and don't take care to
#     directly die afterward if you're not the parent and an error is detected.
#     At the very least, shouldn't the error messages appear on STDERR instead?
# (*) We have tried to make this call:
#         $_[KERNEL]->post('IKC', 'call', "poe:/$unqualified_local_hostname-${object}s/stop_object",
#         { object => $object, abbrev => $abbrev, name => $name, script => $stop_script, }, 'poe:object_stop_callback');
#     and it fails with a message like this:
#         31743: Unknown kernel 'HASH(0x15fdf520)'.
#         31743: Known kernels: work.groundwork.groundworkopensource.com-4ba27d1e00007bff (work-command-server, work-heartbeat-client, work-heartbeat-server), honor.groundwork.groundworkopensource.com-4ba26c3e000029f5 (172.28.115.60:43211, honor.groundwork.groundworkopensource.com:43211, honor-heartbeat-server) at /home/gherteg/svn/monitor-dr/bin/../perl/lib/POE/Component/IKC/Responder.pm line 764.
#     (where the line number is very slightly off, as I had added some warn
#     statements).  The reason is that we have 'default' => {} within $self,
#     and the previous "my $name=$to->{kernel}||$self->{'default'};" statement
#     caused $name to be assigned a hashref rather than a string.  It's that
#     $self->{'default'} reference that ought to be changed.  Also, this
#     does not seem to match the POE::Component::IKC::Specifier documentation,
#     which states that I should be able to omit the kernel name as shown
#     in my specifier.  Admittedly, I did not have a default IKC foreign
#     kernel established at the time, but the POE::Component::IKC::Responder
#     documentation was not clear that I would need one.
# (*) Report a POE::Component::IKC::Responder bug:  the monitor subscribe
#     callback is documented as returning a specifier in ARG3, when in fact
#     that specifier is returned in ARG4 instead.
# (*) Report a POE::Component::IKC::Server bug:  lines 530 and 544 are
#     both triggered on the same error event when one gets an error such
#     as EADDRNOTAVAIL.  One or the other should be disabled.
#     530     warn __PACKAGE__, " $$: encountered $operation error $errnum: $errstr\n";
#     544     warn __PACKAGE__, " $$: encountered $operation error $errnum: $errstr\n";
# (*) Complain to the Parse::RecDescent maintainer that there is no
#     convenient way to override just the initial skip setting before
#     any tokens in the rule are matched.  For instance, if we want to
#     match a sequence of words on a line, where we want to insist on
#     having whitespace occur between words but not necessarily before
#     the first word, then we need to uglify all similar productions:
#         Word <skip: qr/\s+|\Z/> Word(s?) /\Z/  # works
#         Word <skip: qr/\s+/> Word(s?) /\Z/     # fails
#         $Parse::RecDescent::skip = qr/\s+|\Z/; # fails; presumably requires space before first word, too
#         $Parse::RecDescent::skip = qr/\s+/;    # fails; presumably requires space before first word, too
#     Compare the CSV example in the doc, and why that won't work as
#     obviously intended, because it would require a leading comma
#     before the first item in the CSV list.  Perhaps we need a
#     <skiplater> directive, which is planted at one place and takes
#     effect only after the next terminal is recognized.  In effect,
#     what we want is some kind of skip assertion that will match
#     only if we are at the start of the entire string being matched
#     for the production, equivalent to Perl's \A (though that doesn't
#     work, because at the time of the skip matching, the next text
#     characters are treated as the beginning of the string regardless
#     of whether they are at the start of the entire string).
#     I tried using:
#	  do {
#	      use re 'eval';
#	      $Parse::RecDescent::skip = qr/(?(?{$Parse::RecDescent::thiscolumn==1})\s*|\s+)/;
#	  };
#     but even then I get:
#         Eval-group not allowed at runtime, use re 'eval' in regex
#         m/\A((?-xism:(?(?{$Parse::RecDescent::thiscolumn==1})\s*|\s+)))/ at (eval 190) line 3425.
#     and no use of "use re 'eval';" (either when defining the global
#     skip pattern, or when building the parser, or when invoking the
#     parser, or even globally for the entire program) seems to help.
#     Do thorough testing of these assertions before complaining.

# ================================================================
# Documentation.
# ================================================================

# To do:
# (*) Do a more complete job of validating the configuration data, to prevent
#     possible software-upgrade problems in the future.  Also look in particular
#     at how our saved state needs to evolve during an upgrade, so we don't get
#     stuck with obsolete objects and miss new objects, for instance.
# (*) Look carefully at how message consolidation works, to see if messages
#     such as these should be consolidated, and what to do if they are but we
#     don't want them to be so error states don't disappear from the Console
#     by a subsequent OK status for some other object's actions.
#         The replication deploy script for application "snmp-trap-handling" has failed.
#         The replication deploy script for application "cacti" has failed.
# (*) Running "eval" in certain scripts is probably a security hole, done only
#     as a quick-and-dirty step during initial development.  Revamp the way
#     the Replication Engine works so that is no longer needed, probably by
#     executing similar operations straight out of the Replication Engine as
#     direct child processes.
# (*) Originally, we thought that monitoring of the GW Monitor product would
#     feed into the Replication State Engine, and affect its decisions.  Take
#     another look at that idea, and whether it still makes sense.  Are there
#     any decisions the Replication State Engine makes now, or should make, that
#     should be affected by the results of such monitoring, beyond the analysis
#     made on the basis of remote state data returned by heartbeats?
# (*) Future optimization:  When we get back a replica from the remote system,
#     check to see if there are any differences against the local system before
#     stopping the local system and putting the remote copy into production.
#     This should avoid unnecessary application bounces in the common case where
#     the primary-system configuration has not been changed since the last sync
#     operation was run.  Perhaps a generalized model of an app.diff script
#     could be used to make the determination of whether or not to deploy a new
#     configuration.  Possibly this could even be done with a db.diff script, if
#     we compare to the previous well-formatted database dump and just ignore
#     certain unimportant rows, such as session info.
# (*) Walk through all remaining messages, classify them by severity level, and
#     include the severity level in the message.  Then write them conditionally
#     by the configured debug level.
# (*) We have seen that if we start the remote server and then suspend it,
#     so it has locked the port on the other side, and then we start the
#     local server, the local server will pause a long time trying to
#     connect to the other side.  This bears some investigation.  Possibly
#     we might need a timeout on the connection attempt, if we can figure
#     out how to abort it if it's happening in a different event handler.
#     Also check to see what happens if we are already connected and remote
#     side gets suspended.
# (*) We once saw this serialization in our state file:
#         has_notification_authority: ~
#     Figure out what that means and how it might have gotten that way.
#     Is this the representation for a null value?
# (*) When we get near the end of the development, turn on POE tracing to
#     see if we have any memory leaks (heap growth, leftover sessions after
#     remote-connection droppage, etc.).  Look at "top" memory statistics
#     when running this script for a short time and for a long time.
# (*) A general SIGTERM or equivalent signal sent to this parent process is not
#     automatically killing any child processes, so the script continues to wait
#     until they return before itself dying.  At that point, since the sessions
#     that would have reaped the child processes are now gone, the parent process
#     complains that there is no sig_child() in place to reap them.  We should
#     instrument the parent process to perhaps kill any child processes when it
#     is itself killed, perhaps by cycling through and calling all the outstanding
#     timeout handlers, rather than waiting an indefinite time for the child
#     processes to go away on their own.  One hook into doing this might be the
#     _stop event for the '$action-$object-$name' session.
# (*) Handle SIGTERM so we at least log a shutdown message (and the
#     reason for the shutdown), if not actually clean up gracefully.
#     See how a signal like this (and SIGHUP, SIGINT, and SIGQUIT, perhaps)
#     can be gracefully handled by POE.  What we'd like to have happen is
#     for an external termination signal to be converted into an internal
#     shutdown signal, which each session can interpret in its own way to
#     safely complete whatever actions it is currently engaged in, close
#     and release whatever resources it holds, and then stop itself.
# (*) Perhaps parent sessions will not exit naturally until all their child
#     sessions do first.  So as part of signal handling, provide a routine
#     to log the tree of sessions by calling POE::API::Peek, so we can tell
#     what their relationships are as we figure out how to shut them down
#     gracefully.
# (*) Possibly, once the whole script is working, certain major sections of it
#     could be refactored into Replication::Session, Replication::Parsing,
#     and Replication::Command packages.  Or something similar.
# (*) Test the resilience of the POE connections to garbage injection from
#     hostile network attackers.  Is there some mechanism already extant
#     within POE to authenticate incoming connections?
# (*) Consider whether it might have been easier to use
#     POE::Component::ControlPort to provide the commands interface.

# Notes:
# (*) See this:
#         http://blog.gmane.org/gmane.comp.lang.perl.poe
#     for some general issues people have encountered with POE,
#     and a place to ask for advice if you run into issues.

# Notes on object sync sequences:
#
# Here are some early notes on this topic.  We need to crawl through them
# and ensure that we have handled all the related issues, even if the
# implementation has significantly changed since these early scratchings.
#
# log -> file + Foundation
#
# To sync an app:
# * check if already active, or if blocked; if so, don't start the sequence,
#   but log to Foundation and still reschedule for a future attempt
# * set active
# * submit a remote capture request with a callback (only if in Slave mode)
# * set a timeout for the remote capture operation
# * reschedule the next sync attempt
# * when blocked, perhaps drop scheduling; when unblocked, make sure a new
#   attempt is scheduled again if we dropped the scheduling sequence earlier
#
# upon capture (object sync) timeout:
# * set inactive, or maybe stalled; log a message
#
# upon capture callback:
# * if active and not blocked (otherwise, log and stop the sequence)
# * check return status from the remote capture
# * if files are bad, log it and stop
# * if files are good, call stop script (with stop callback and timeout)
#
# upon stop timeout:
# * set inactive or stalled; log a message
#
# upon stop callback:
# * if stopped badly, set inactive or stalled, log and stop the sequence
# * move files around (backup and deploy)
# * call start script (with child callback and timeout)
#
# upon start timeout:
# * set inactive or stalled; log a message
#
# upon start callback:
# * if started badly, set inactive or stalled; log
# * if started okay, set inactive, log
#
# Think about the logging of actions for various objects all under the
# same "replication" service.  If we overwrite bad status from one
# object with good status from some other object, the operator will
# miss what's going on.

# ================================================================
# Program notes.
# ================================================================

# As long as we don't do anything potentially damaging during startup,
# there is no reason to try to impose some kind of lock on this script
# to prevent more than one copy from running at a time.  That's because
# a second copy will be being unable to open the server ports, and that
# condition will be fatal.

# ================================================================
# Perl setup.
# ================================================================

use strict;

# We have a conundrum here.  We wanted to use FindBin to find out where our
# added Perl modules live.  But FindBin pulls in the Cwd module, which is
# one of the Perl modules we need to upgrade from what would otherwise be
# referenced (because versions 2.19 and 3.12, the versions that ship with
# the OS on some platforms, are buggy).  Cwd is therefore one of our added
# Perl modules and should not be referenced until after we have set the
# include path to find it, using the result of the FindBin module.  Oops.
#
# Here is the simple solution, hardcoding the path before importing Cwd so
# the right version will be already present when FindBin reaches for it:
#
# use lib '/usr/local/groundwork/replication/perl/lib';
# use Cwd 3.31 'realpath';
#
# But we don't like that solution because it would make it impossible to
# run this script in a development mode on a machine where you don't have
# at least an initial version of the whole package already installed.
#
# An alternative might be to use Dir::Self, but again, that would be an
# extension we wouldn't have access to until we call it to get the path
# to it, since it's not in the standard Perl distribution.
#
# See http://www.lowlevelmanager.com/2010/04/use-libs-findbinbin-considered-harmful.html
# for further info.  It points out that FindBin is clumsy in the first
# place, and that an alternative does exist, which we considered using
# here.  But File::Spec->rel2abs() internally requires Cwd, so this is
# no better than FindBin for our purposes.
#
# BEGIN {
#     use File::Basename;
#     use File::Spec;
#     $Bin = dirname( File::Spec->rel2abs( __FILE__ ) );
# }
#
# See also http://use.perl.org/~Aristotle/journal/33995 for further info,
# and the following construction:
#
# use File::Spec::Functions qw( catpath splitpath rel2abs );
# use lib catpath( ( splitpath( rel2abs $0 ) )[ 0, 1 ] );
#
# However, as noted above, File::Spec->rel2abs() internally requires Cwd,
# so this is of no help to us in our quest to determine the path before
# using Cwd.  Also, I've seen some advice that __FILE__ should be used
# instead of $0 because the latter can be faked by the parent program.
# But consider all cases:  perl -e, perl scriptname, scriptname, CGI,
# etc., and test in each of them, using both relative and absolute
# pathnames, for a fully robust solution.
#
# Here is another alternative (but still subject to old-Cwd problems):
#
# use Cwd 'abs_path';
# use File::Basename;
# use lib dirname( abs_path $0 );
#
# See http://perldoc.perl.org/perlfaq8.html#How-do-I-add-the-directory-my-program-lives-in-to-the-module/library-search-path?
# for perhaps the most reviewed answer.  But again, the solutions shown
# there still depend on Cwd, directly or indirectly.

# Old construction, now deprecated:
# use FindBin qw($Bin);

# Sigh.  Nothing above seems to work to solve our problem.  So we just pull in Cwd from
# our RPM-installed copy, then establish a $Bin value after that.  This allows us to
# continue to run a development version, using its own config file and other resources.
# But it also means we cannot do so until we install the RPM, to make available this
# alternate version of Cwd.
my $CwdBin;
BEGIN {
    $CwdBin = '/usr/local/groundwork/replication/bin';
}
use lib "$CwdBin/../perl/lib";
use Cwd 3.31 'realpath';
my $Bin;
BEGIN {
    use File::Basename;
    use File::Spec;
    $Bin = dirname( File::Spec->rel2abs( __FILE__ ) );
}

# This is where we'll pick up any Perl packages not in the standard Perl
# distribution, to make this a self-contained package anchored in a single
# directory.
use lib "$Bin/../perl/lib";

# POE debug and trace control, for development purposes
BEGIN {
    package POE::Kernel;
    use constant USE_SIGCHLD    => 1;
    # use constant TRACE_EVENTS   => 1;
    # use constant TRACE_SESSIONS => 1;
    # use constant ASSERT_STATES  => 1;
}

use Getopt::Std;
use Config;
use Errno qw(ECONNREFUSED ECHILD);
use Sys::Hostname;
use Time::Local 'timelocal_nocheck';
use POSIX qw(:signal_h);
use POE;
use POE::API::Peek;
use POE::Component::IKC::Responder;  # make $ikc available here
use POE::Component::IKC::Server;
use POE::Component::IKC::Client;
use POE::Component::IKC::Specifier;
use POE::Wheel::Run;
use Replication::Daemon;
use Replication::Config qw(:DEFAULT log_configuration);
use Replication::Help;
use Replication::Logger;
use Replication::State qw(:DEFAULT log_replication_state);
use Replication::Foundation;
use Replication::Nagios;
use Replication::Utility;
use Replication::Database;
use Replication::Application;
use Replication::Syscall;
use Parse::RecDescent;
use boolean;

my $PROGNAME = "replication_state_engine";

# Be sure to update this as changes are made to this script!
my $VERSION = '0.3.0';

# ================================================================
# Convenience definitions.
# ================================================================

# Indexes into the configured application and database action-script
# timeouts arrays, with these index values fixed by convention.
use constant WARNING_TIMEOUT => 0;
use constant ABANDON_TIMEOUT => 1;
use constant SIGTERM_TIMEOUT => 1;
use constant SIGKILL_TIMEOUT => 2;

# ================================================================
# Command-line execution options and working variables.
# ================================================================

# FIX THIS:  This setting should be 0 in production.  We only have it
# set to 1 for development use, so we can see what kinds of warnings
# we might be suppressing and need to deal with otherwise.
my $spill_warnings = 1;

# FIX THIS
my $spawn_ticktock_session = 0;

# Boolean substates:
#   engine_running
#   heartbeat_running
#   notify_grabbed
#   notify_released
#   notify_dynamic
#   config_grabbed
#   sync_all_blocked
#   commit_running
#   rollback_running
#
my $replication_state_path = "$Bin/../var/REPLICATION_STATE";
my %replication_state = ();

# ================================================================
# Working variables.
# ================================================================

# Variables used to analyze heartbeats and calculate normal/failure mode transitions.
my $start_time                     = time();
my $first_good_remote_state_time   = undef;
my $consecutive_good_remote_states = 0;

my $default_config_file = "$Bin/../config/replication.conf";
my $config_file         = undef;
my $config              = undef;

my $days_per_week      = 7;
my $hours_per_day      = 24;
my $minutes_per_hour   = 60;
my $seconds_per_minute = 60;
my $seconds_per_hour   = $seconds_per_minute * $minutes_per_hour;
my $seconds_per_day    = $seconds_per_hour   * $hours_per_day;
my $seconds_per_week   = $seconds_per_day    * $days_per_week;

my %time_in_seconds = (
    minutes => $seconds_per_minute,
    hours   => $seconds_per_hour,
    days    => $seconds_per_day,
    weeks   => $seconds_per_week,
);

my $replica_user    = undef;
my $replica_machine = undef;

# This is the base directory that we expect all configured included trees
# and files to start with.  It's basically a simple security mechanism to
# try to limit the effect of replication to a single tree which is known
# to contain the files of interest.  This is not a perfect firewall, but
# it should at least prevent some simple slip-ups.
my $working_path = '/usr/local/groundwork/';

# Where to find scripts for common tasks such as cleanup.
my $scripts_path = "$Bin/../scripts";

my $local_command_host    = undef;
my $local_heartbeat_host  = undef;
my $local_command_port    = undef;
my $local_heartbeat_port  = undef;
my $remote_heartbeat_host = undef;
my $remote_command_port   = undef;
my $remote_heartbeat_port = undef;

my $pending_config_base_dir = undef;
my $backups_config_base_dir = undef;

my $help       = undef;
my $foundation = undef;
my $nagios     = undef;

my $show_help          = 0;
my $show_version       = 0;
my $run_interactively  = 0;
my $reflect_log_to_tty = 0;

my  $debug_level   = undef;
our $DEBUG_NONE    = undef;
our $DEBUG_FATAL   = undef;
our $DEBUG_ERROR   = undef;
our $DEBUG_WARNING = undef;
our $DEBUG_NOTICE  = undef;
our $DEBUG_STATS   = undef;
our $DEBUG_INFO    = undef;
our $DEBUG_DEBUG   = undef;
our $DEBUG_FERVID  = undef;

my $logfile      = undef;
my $statefile    = undef;
my $actions_base = undef;

my $log_rotation_period    = 3600;	# seconds
my $max_logfile_size       = undef;
my $max_logfiles_to_retain = undef;

my $i_am_primary = undef;

my   $qualified_primary_server   = undef;
my $unqualified_primary_server   = undef;
my   $qualified_secondary_server = undef;
my $unqualified_secondary_server = undef;

my   $qualified_local_hostname  = undef;
my $unqualified_local_hostname  = undef;
my   $qualified_remote_hostname = undef;
my $unqualified_remote_hostname = undef;

my $last_remote_heartbeat_server_connect_time = 0;	# UNIX timestamp of the last successful connection.

my    $connected_to_remote_heartbeat_server = 0;	# true ==> we have reached over and connected to the remote server
my   $subscribed_to_remote_heartbeat_server = 0;	# true ==> we have reached over and subscribed to the remote server
my  $connected_from_remote_heartbeat_client = 0;	# true ==> the remote client has reached over and connected to us
my $subscribed_from_remote_heartbeat_client = 0;	# true ==> the remote client has reached over and subscribed to us

my %app_name  = ();
my @app_names = ();
my %db_name   = ();
my @db_names  = ();

my %initialized = ();

my %last_ticket = ();
my $last_ticket_number = 0;

my $blank_line = ' ';

my $poe_api_peek = POE::API::Peek->new();

my $got_remote_state_timeout = 0;

my $parent_pid  = $$;
my $parent_pgrp = undef;
my $parent_sid  = undef;

my $heartbeat_client_session_id = undef;

# These settings are new, and ought to be migrated into the config file instead of being hardcoded here.
# They're only here as a concession to make it easier for the first customer to upgrade to this release.
# These relationships must hold:
#     0 <= $max_antiquated_heartbeats <= $max_archaic_heartbeats <= $max_ancient_heartbeats
# If you set these variables to non-zero values, typical values might be
# antiquated => 20, archaic => 25, ancient => 30.
my $max_antiquated_heartbeats = 20;  # (!= 0) => force reconnect if this many heartbeats missed and remote kernel is not known.
my $max_archaic_heartbeats    = 25;  # (!= 0) => force reconnect if this many heartbeats missed and remote kernel is not connected.
my $max_ancient_heartbeats    = 30;  # (!= 0) => force reconnect if this many heartbeats missed.

# ================================================================
# UI Command Grammar.
# ================================================================

# Herewith follow several attempts to make whitespace between grammar tokens
# non-optional after the first token on each line.
#
# Unfortunately, this also makes whitespace before the first token
# in each rule also mandatory, so we cannot use this setting.
# $Parse::RecDescent::skip = qr/\s+|(?=\Z)/;
#
# This generates a Perl error we cannot seem to get around:
#     Eval-group not allowed at runtime, use re 'eval' in regex
#     m/\A((?-xism:(?(?{$Parse::RecDescent::thiscolumn==1})\s*|(\s+|(?=\Z)))))/ at (eval 190) line 3425.
# $Parse::RecDescent::skip = qr/(?(?{$Parse::RecDescent::thiscolumn==1})\s*|(\s+|(?=\Z)))/;
#
# Until we can make something like the previous pattern work, we make a space before
# each token mandatory, except at the end of the line.  To make this work, the incoming
# text handed to the parser must be prefixed with a space character.  That's ugly, but
# we'll have to accept it as a workaround for the time being.  The alternative is to use
# productions like this, that reset the skip pattern after the first token:
#	'help' <skip: qr/\s+|\Z/> Word(s?) /\Z/
# but that makes the grammar a lot uglier throughout.
$Parse::RecDescent::skip = qr/\s+|(?=\Z)/;

# This grammar is designed to parse exactly one command line, without worrying about
# production actions being mistakenly fired early because further parsing invalidated
# the user input.  To that end, rather than using a separate top-level target defined
# as "Command /\Z/", which might cause backup if the end-of-input marker is not found
# as expected, we distribute the end-of-input marker to each production.  That makes
# the grammar uglier in a sense, but avoids use having to use <defer> everywhere.

# In a future release, we can imagine replacing most of the literal keywords here with
# nonterminal symbols that will allow convenient unambiguous abbreviations.  To that
# end, perhaps we should design the command language to avoid conflicting initial
# letters in most positional keywords, wherever possible and semantically sensible.
# That might allow us to reduce many commands to single letters.

# FIX LATER:  some of these commands might be generalized in the future to accept
# more than one application or database on the same command line

# Sometimes needed here, to spill out a tiny bit of diagnostics when we mangle the grammar.
# $::RD_HINT = 1;

my $command_grammar = q{
    Command:
	  'help' Word(s?)					/\Z/ {::provide_help($item[2])}
	| 'about'						/\Z/ {::display_metadata()}
	| 'login' Credentials(?)				/\Z/ {::log_in(@{$item[2]}[0], @{$item[2]}[1])}
	| 'engine' 'start'					/\Z/ {::start_engine()}
	| 'engine' 'stop'					/\Z/ {::stop_engine()}
	| 'alias' 'all' ('app'|'db')(?)				/\Z/ {::display_all_aliases(@{$item[3]}[0])}
	| 'alias' 'app' Application				/\Z/ {::display_application_aliases($item[3])}
	| 'alias' 'db' Database					/\Z/ {::display_database_aliases($item[3])}
	| 'status' 'all' ('local'|'remote')(?) ('app'|'db')(?)	/\Z/ {::display_all_status(${$item[3]}[0], ${$item[4]}[0])}
	| 'status' ('local'|'remote')(?) 'app' Application(s)	/\Z/ {::display_application_status(@{$item[2]}[0], $item[4])}
	| 'status' ('local'|'remote')(?) 'db' Database(s)	/\Z/ {::display_database_status(@{$item[2]}[0], $item[4])}
	| 'status' ('local'|'remote')(?) 'engine'		/\Z/ {::display_engine_status(@{$item[2]}[0])}
	| 'status' ('local'|'remote')(?) 'notify'		/\Z/ {::display_notification_status(@{$item[2]}[0])}
	| 'status' ('local'|'remote')(?) 'config'		/\Z/ {::display_configuration_status(@{$item[2]}[0])}
	| 'status' 'heartbeat'					/\Z/ {::display_heartbeat_status()}
	| 'notify' 'grab'					/\Z/ {::grab_notifications()}
	| 'notify' 'dynamic'					/\Z/ {::dynamic_notifications()}
	| 'notify' 'release'					/\Z/ {::release_notifications()}
	| 'config' ('forced')(?) 'grab'				/\Z/ {::grab_configuration(@{$item[2]}[0])}
	| 'config' 'release'					/\Z/ {::release_configuration()}
	# Calling the pulse_heartbeat() routine should be conditional; only call pulse_heartbeat()
	# if one is not already in progress, rather than depending on that function to figure it out.
	# The difference is in having the function always do exactly what it says it will, and in how
	# we report a command response to the user.
	| 'pulse'						/\Z/ {::pulse_heartbeat()}
	| 'block' 'all' ('app'|'db')(?)				/\Z/ {::block_all_syncs(@{$item[3]}[0])}
	| 'block' 'app' Application(s)				/\Z/ {::block_application_syncs($item[3])}
	| 'block' 'db' Database(s)				/\Z/ {::block_database_syncs($item[3])}
	| 'unblock' 'all' ('app'|'db')(?)			/\Z/ {::unblock_all_syncs(@{$item[3]}[0])}
	| 'unblock' 'app' Application(s)			/\Z/ {::unblock_application_syncs($item[3])}
	| 'unblock' 'db' Database(s)				/\Z/ {::unblock_database_syncs($item[3])}
	| 'diff' 'all' ('app'|'db')(?)				/\Z/ {::display_all_diffs(@{$item[3]}[0])}
	| 'diff' 'app' Application(s)				/\Z/ {::display_application_diffs($item[3])}
	| 'diff' 'db' Database(s)				/\Z/ {::display_database_diffs($item[3])}
	| 'sync' 'all' ('app'|'db')(?)				/\Z/ {::synchronize_all(@{$item[3]}[0])}
	| 'sync' 'app' Application(s)				/\Z/ {::synchronize_applications($item[3])}
	| 'sync' 'db' Database(s)				/\Z/ {::synchronize_databases($item[3])}
	| 'commit' 'app' Application				/\Z/ {::commit_application($item[3])}
	| 'commit' 'db' Database				/\Z/ {::commit_database($item[3])}
	| 'list' 'app' Application				/\Z/ {::list_application($item[3])}
	| 'list' 'db' Database					/\Z/ {::list_database($item[3])}
	| 'rollback' 'app' Application Timestamp(?)		/\Z/ {::rollback_application($item[3], @{$item[4]}[0])}
	| 'rollback' 'db' Database Timestamp(?)			/\Z/ {::rollback_database($item[3], @{$item[4]}[0])}
	# quit and exit, while shown in the help message, are to be recognized and executed client-side, not here.

    Word:		/\S+/
    Credentials:	Username Password(?) {[@item]}
    Username:		/\S+/
    Password:		/\S+/
    Application:	/[A-Za-z][\w-]*/
    Database:		/[A-Za-z]\w*/
    Timestamp:		/\d{4}-\d{2}-\d{2}\@\d{2}:\d{2}:\d{2}|\d{4}-\d{2}-\d{2}\.\d{2}_\d{2}_\d{2}/
};

# Some of the commands above might not be implemented yet, but this hash
# will at least give us a basic filter for what is valid.  The construction
# (pattern matching) is a bit fragile, as it depends on consistent spacing
# when we define command words and assumes that this spacing pattern will
# not be followed for token alternatives within a command.  Still, it makes
# our maintenance job a bit easier, by automatically analyzing the grammar.
my %is_valid_command = ();
$is_valid_command{help} = 1;
while ($command_grammar =~ /[|]\s+'(\w+)'\s+/cg) {
    $is_valid_command{$1} = 1;
}

my $command_parser = new Parse::RecDescent($command_grammar);

# ================================================================
# Remote Replication Server Heartbeat Grammar.
# ================================================================

my $heartbeat_grammar = q{
    Heartbeat:
	  'thub' Word(s?)					/\Z/ {::run_heartbeat($item[2])}

    Word:		/\S+/
    Application:	/[A-Za-z][\w-]*/
    Database:		/[A-Za-z]\w*/
};

my $heartbeat_parser = new Parse::RecDescent($heartbeat_grammar);

# ================================================================
# Program.
# ================================================================

# If we're somehow accidentally running as the superuser, switch to
# safe (and) effective uid/gid values that we like, to avoid both
# contaminating owner/group metadata for created files, and accessing
# any privileged system resources we shouldn't be allowed to touch.
# This also changes the real uid/gid values, as we have found that
# the POE-1.287 POE::Wheel::Run implementation will cause any spawned
# child processes to run with euid/egid values that are the ruid/rgid
# values of the parent process, which is rather unexpected.
if (not run_as_nagios()) {
    spill_message "ERROR:  $PROGNAME is not running as the \"nagios\" user";
    # I suppose we ought to exit now, in a production environment.  This is currently
    # not done only to allow us to run this script in a development mode.
    # exit 1;
}

# Change our working directory to a safe place that we know the nagios user should be able to
# access, because the realpath() call inside parse_command_line() performs internal chdir()
# operations and eventually tries to get back to where it started.  If that starting place is
# in fact inaccessible to the nagios user, the realpath() call will fail.  This chdir() can be
# considered to be one simplistic part of what make_daemon() does, and it will be overridden
# by make_daemon() if we are not running interactively.
if (not chdir '/tmp') {
    spill_message "FATAL:  $PROGNAME cannot change directory to /tmp";
    exit 1;
}

my $command_line_status = parse_command_line();
if (!$command_line_status) {
    spill_message "FATAL:  $PROGNAME either cannot understand its command-line parameters or cannot find its config file";
    exit 1;
}

if ($show_version) {
    print_version();
}

if ($show_help) {
    print_usage();
}

if ($show_version || $show_help) {
    exit 0;
}

# Daemonize, if we don't have a command-line argument saying not to.
if (!$run_interactively) {
    make_daemon();
    # If we are operating as a daemon, we will have forked, so our PID will be different now.
    $parent_pid = $$;
    # Apparently, we need to tell POE about the change, too, so it can reset some internal data.
    $poe_kernel->has_forked();
}

# We have to wait until after make_daemon() is called to capture these values,
# as they are likely to have changed when that routine ran.
$parent_pgrp = getpgrp(0);
$parent_sid  = getsid(0);

eval {
    # The replication config file currently doesn't contain any sensitive information
    # such as passwords, so we allow it to be world-readable.  If it did contain such
    # data, we would want to call Replication::Config->secure_new() here instead of
    # Replication::Config->new(), to check the permissions at run time as an extra
    # security measure.
    Replication::Config->new ($config_file);
};
if ($@) {
    chomp $@;
    spill_message $@;
    die "\n";
}
$config = load_configuration();
if (not defined $config) {
    spill_message "FATAL:  $PROGNAME cannot load configuration from $config_file";
    die "\n";
};

# Global Debug Level Flag.
$debug_level = $config->{'debug-level'};

# Variables to be used as quick tests to see if we're interested in particular debug messages.
$DEBUG_NONE    = $debug_level == 0;  # turn off all debug info
$DEBUG_FATAL   = $debug_level >= 1;  # the application is about to die
$DEBUG_ERROR   = $debug_level >= 2;  # the application has found a serious problem, but will attempt to recover
$DEBUG_WARNING = $debug_level >= 3;  # the application has found an anomaly, but will try to handle it
$DEBUG_NOTICE  = $debug_level >= 4;  # the application wants to inform you of a significant event
$DEBUG_STATS   = $debug_level >= 5;  # the application wants to log statistical data for later analysis
$DEBUG_INFO    = $debug_level >= 6;  # the application wants to log a potentially interesting event
$DEBUG_DEBUG   = $debug_level >= 7;  # the application wants to log detailed debugging data
$DEBUG_FERVID  = $debug_level >= 8;  # the application wants to log an extreme amount of debugging data

$logfile      = $config->{'replication-log-file'};
$statefile    = $config->{'replication-state-file'};
$actions_base = $config->{'actions-base-dir'};

# Safety adjustments, to anchor relative pathnames in a safe place,
# given that we may well be running as a daemon where the current
# working directory is not where this script was started from.
# FIX LATER:  call realpath on these paths
$logfile      = "$Bin/../logs/$logfile"  if $logfile      !~ m{^/};
$statefile    = "$Bin/../var/$statefile" if $statefile    !~ m{^/};
$actions_base = "$Bin/../$actions_base"  if $actions_base !~ m{^/};

$max_logfile_size       = $config->{'max-logfile-size'};
$max_logfiles_to_retain = $config->{'max-logfiles-to-retain'};

# Convert to rough megabytes.  We use the conventional disk-drive manufacturer's definition of
# a megabyte (10^^6, a round million) rather than the binary definition (2^^20 = 1024 * 1024).
$max_logfile_size *= 1_000_000;

# Convert to a number, just in case we got some bad input.
$max_logfiles_to_retain += 0;

Replication::Logger->new ($logfile, $run_interactively, $reflect_log_to_tty, $max_logfile_size, $max_logfiles_to_retain);
open_logfile();
log_timed_message "=== Starting up (process $$). ===";
my $start_message = 'Replication is starting up.';
system_log $start_message;

# Clean up after Parse::RecDescent inappropriately grabbed several copies of one of our file descriptors when
# it was compiled in.  That will cause our daemon to unexpectedly continue to hang on to a terminal session.
# Redirect those now to our log file.  See https://rt.cpan.org/Public/Bug/Display.html?id=31076 for details.
do {
    package Parse::RecDescent;

    sub set_error_stream (*;$) {
	my $filehandle = shift;
	my $mode       = shift || '>';

	# Duplicate the given filehandle.
	$mode = "$mode&";

	open (ERROR, $mode, $filehandle);
	set_autoflush(\*ERROR);

	open (TRACE, $mode, $filehandle);
	set_autoflush(\*TRACE);

	open (TRACECONTEXT, $mode, $filehandle);
	set_autoflush(\*TRACECONTEXT);
    }
};
Parse::RecDescent::set_error_stream (Replication::Logger::LOG, '>>');

if (0) {
    log_message YAML::XS::Dump ($config);
}
if (0) {
    use Data::Dumper;
    log_message Dumper ($config);
}

my $got_bad_config = 0;

foreach my $config_key (
	'enable-processing',
	'debug-level',
	'remote-user',
	'primary-server',
	'secondary-server',
	'primary-command-port',
	'secondary-command-port',
	'primary-heartbeat-port',
	'secondary-heartbeat-port',
	'replication-state-file',
	'contact-heartbeat-period',
	'max-consecutive-bad-heartbeats-before-outage',
	'max-bad-heartbeats-before-flapping',
	'flapping-window-heartbeats',
	'min-consecutive-good-heartbeats-before-normal',
	'pending-config-base-dir',
	'backups-config-base-dir',
	'actions-base-dir',
	'nagios-command-pipe',
	'max-command-pipe-write-size',
	'max-command-pipe-wait-time',
	'replication-log-file',
	'max-logfile-size',
	'max-logfiles-to-retain'
    ) {
    if (not defined $config->{$config_key}) {
	log_timed_message "FATAL:  $config_key is not defined in the configuration file.";
	$got_bad_config = 1;
    }
}

if ($config->{'max-logfile-size'} <= 0) {
    log_timed_message "FATAL:  max-logfile-size must be a positive integer" if $DEBUG_FATAL;
    $got_bad_config = 1;
}

if ($config->{'max-logfiles-to-retain'} <= 0) {
    log_timed_message "FATAL:  max-logfiles-to-retain must be a positive integer" if $DEBUG_FATAL;
    $got_bad_config = 1;
}

if ($config->{'contact-heartbeat-period'} < 15) {
    log_timed_message "FATAL:  contact-heartbeat-period must be at least 15 (seconds), preferably around 60" if $DEBUG_FATAL;
    $got_bad_config = 1;
}

# We always allow at least one skipped heartbeat before we declare failure.
if ($config->{'max-consecutive-bad-heartbeats-before-outage'} <= 0) {
    log_timed_message "FATAL:  max-consecutive-bad-heartbeats-before-outage must be a positive integer" if $DEBUG_FATAL;
    $got_bad_config = 1;
}

# We need the flapping threshold to be larger than the outage threshold, for this relationship to make any sense.
if ($config->{'max-consecutive-bad-heartbeats-before-outage'} >= $config->{'max-bad-heartbeats-before-flapping'}) {
    log_timed_message "FATAL:  max-consecutive-bad-heartbeats-before-outage must be less than max-bad-heartbeats-before-flapping" if $DEBUG_FATAL;
    $got_bad_config = 1;
}

# We need the flapping window to be larger than the flapping threshold, for flapping detection to make any sense.
if ($config->{'max-bad-heartbeats-before-flapping'} >= $config->{'flapping-window-heartbeats'}) {
    log_timed_message "FATAL:  max-bad-heartbeats-before-flapping must be less than flapping-window-heartbeats" if $DEBUG_FATAL;
    $got_bad_config = 1;
}

# We need the recognition of when it's okay to transition back to normal mode to not be confused by continuing recognition of failure.
if ($config->{'min-consecutive-good-heartbeats-before-normal'} <=
    $config->{'flapping-window-heartbeats'} - $config->{'max-bad-heartbeats-before-flapping'}) {
    log_timed_message "FATAL:  min-consecutive-good-heartbeats-before-normal must be greater than the difference between flapping-window-heartbeats and max-bad-heartbeats-before-flapping" if $DEBUG_FATAL;
    $got_bad_config = 1;
}

# We need the recognition of when it's okay to transition back to normal mode to not be confused by continuing recognition of failure.
if ($config->{'min-consecutive-good-heartbeats-before-normal'} <= $config->{'max-consecutive-bad-heartbeats-before-outage'}) {
    log_timed_message "FATAL:  min-consecutive-good-heartbeats-before-normal must be greater than max-consecutive-bad-heartbeats-before-outage" if $DEBUG_FATAL;
    $got_bad_config = 1;
}

$replica_user = $config->{'remote-user'};

# We don't attempt to use the $replica_user right away to validate that we can
# access the remote system using this account, because the remote machine might
# be down at the moment.  We'll find out soon enough.  We do, though, insist that
# it be properly formed, to prevent possible security problems.
if (not defined $replica_user || $replica_user !~ /^\w+$/) {
    log_timed_message "FATAL:  remote-user is not properly defined" if $DEBUG_FATAL;
    $got_bad_config = 1;
}

$pending_config_base_dir = $config->{'pending-config-base-dir'};
$backups_config_base_dir = $config->{'backups-config-base-dir'};

if (not defined $pending_config_base_dir) {
    log_timed_message "FATAL:  pending-config-base-dir is not defined" if $DEBUG_FATAL;
    $got_bad_config = 1;
}

if (not defined $backups_config_base_dir) {
    log_timed_message "FATAL:  backups-config-base-dir is not defined" if $DEBUG_FATAL;
    $got_bad_config = 1;
}

$pending_config_base_dir =~ s{/$}{} if defined $pending_config_base_dir;
$backups_config_base_dir =~ s{/$}{} if defined $backups_config_base_dir;

# FIX THIS:  In a future version, perhaps add additional manufactured aliases,
# which each contain at least one more character than the shortest unambiguous
# abbreviation of each application or database, respectively.  Or impose the
# additional constraint that the shorter manufactured abbreviated aliases added
# here must differ in at least two characters from each other, not just having
# one extra character beyond the minimum difference to make them unambiguous.

if (defined $config->{'applications'}) {
    foreach my $app (@{ $config->{'applications'} }) {
	my $name = $app->{'application-name'};
	push @app_names, $name;
	$app_name{$name} = $name;
	if (defined $app->{'aliases'}) {
	    foreach my $alias (@{ $app->{'aliases'} }) {
		if ($app_name{$alias}) {
		    log_timed_message "FATAL:  $alias is already an alias for the $app_name{$alias} application," if $DEBUG_FATAL;
		    log_timed_message "        so it cannot be an alias as well for $name." if $DEBUG_FATAL;
		    $got_bad_config = 1;
		}
		else {
		    $app_name{$alias} = $name;
		}
	    }
	}
	if (!defined $app->{'cleanup-period'} || !defined $app->{'cleanup-phase'} ||
	    !defined(next_event_time ('cleanup', $app->{'cleanup-period'}, $app->{'cleanup-phase'}))) {
	    log_timed_message "FATAL:  application \"$name\" does not have a valid cleanup-period and cleanup-phase defined" if $DEBUG_FATAL;
	    $got_bad_config = 1;
	}
	if (!defined $app->{'sync-period'} || !defined $app->{'sync-phase'} ||
	    !defined(next_event_time ('sync', $app->{'sync-period'}, $app->{'sync-phase'}))) {
	    log_timed_message "FATAL:  application \"$name\" does not have a valid sync-period and sync-phase defined" if $DEBUG_FATAL;
	    $got_bad_config = 1;
	}
	$app->{'min-backup-age'} = valid_min_backup_age ($app->{'min-kept-backups'}, $app->{'min-backup-time'});
	if (!defined $app->{'min-kept-backups'} || !defined $app->{'min-backup-time'} || !defined $app->{'min-backup-age'}) {
	    log_timed_message "FATAL:  application \"$name\" does not have a valid min-kept-backups and min-backup-time defined" if $DEBUG_FATAL;
	    $got_bad_config = 1;
	}
	$got_bad_config |= timeouts_are_bad('application', $app, $name);
	my $include_trees = $app->{'include-trees'};
	my $exclude_trees = $app->{'exclude-trees'};
	my $include_files = $app->{'include-files'};
	my $exclude_files = $app->{'exclude-files'};
	if (!defined($include_trees) && !defined($include_files)) {
	    log_timed_message "FATAL:  application \"$name\" does not have either include-trees or include-files specified." if $DEBUG_FATAL;
	    $got_bad_config = 1;
	}
	else {
	    my $application_replication_patterns = app_replication_patterns ($name, $include_trees, $exclude_trees, $include_files, $exclude_files);
	    if (!defined($application_replication_patterns) || !@$application_replication_patterns) {
		$got_bad_config = 1;
	    }
	    else {
		# log_timed_message "FERVID:  replication patterns for \"$name\":  ", join (' ', @$application_replication_patterns) if $DEBUG_FERVID;
		# Save the patterns where we can find them when we span children.
		$config->{app}{$name}{'application-replication-patterns'} = $application_replication_patterns;
	    }
	}
    }
}
if (defined $config->{'databases'}) {
    foreach my $db (@{ $config->{'databases'} }) {
	my $name = $db->{'database-name'};
	push @db_names, $name;
	$db_name{$name} = $name;
	if (defined $db->{'aliases'}) {
	    foreach my $alias (@{ $db->{'aliases'} }) {
		if ($db_name{$alias}) {
		    log_timed_message "FATAL:  $alias is already an alias for the $db_name{$alias} database," if $DEBUG_FATAL;
		    log_timed_message "        so it cannot be an alias as well for $name." if $DEBUG_FATAL;
		    $got_bad_config = 1;
		}
		else {
		    $db_name{$alias} = $name;
		}
	    }
	}
	if ($db->{'replication-type'} ne 'continuous' &&
	    (!defined $db->{'sync-period'} || !defined $db->{'sync-phase'} ||
	    !defined(next_event_time ('sync', $db->{'sync-period'}, $db->{'sync-phase'})))) {
	    log_timed_message "FATAL:  database \"$name\" does not have continuous replication," if $DEBUG_FATAL;
	    log_timed_message "        but does not have a valid sync-period and sync-phase defined" if $DEBUG_FATAL;
	    $got_bad_config = 1;
	}
	if ($db->{'replication-type'} ne 'continuous' &&
	    (!defined $db->{'cleanup-period'} || !defined $db->{'cleanup-phase'} ||
	    !defined(next_event_time ('cleanup', $db->{'cleanup-period'}, $db->{'cleanup-phase'})))) {
	    log_timed_message "FATAL:  database \"$name\" does not have continuous replication," if $DEBUG_FATAL;
	    log_timed_message "        but does not have a valid cleanup-period and cleanup-phase defined" if $DEBUG_FATAL;
	    $got_bad_config = 1;
	}
	$db->{'min-backup-age'} = valid_min_backup_age ($db->{'min-kept-backups'}, $db->{'min-backup-time'});
	if (!defined $db->{'min-kept-backups'} || !defined $db->{'min-backup-time'} || !defined $db->{'min-backup-age'}) {
	    log_timed_message "FATAL:  database \"$name\" does not have a valid min-kept-backups and min-backup-time defined" if $DEBUG_FATAL;
	    $got_bad_config = 1;
	}
	$got_bad_config |= timeouts_are_bad('database', $db, $name);
	my $credentials_file = $db->{'credentials-file'};
	my $credentials_type = $db->{'credentials-type'};
	if (!defined($credentials_file) || !defined($credentials_type)) {
	    log_timed_message "FATAL:  database \"$name\" does not have proper credentials-file or credentials-type specified." if $DEBUG_FATAL;
	    $got_bad_config = 1;
	}
	else {
	    my $credentials = db_credentials( $name, $credentials_file, $credentials_type );
	    if (not defined $credentials) {
		$got_bad_config = 1;
	    }
	    else {
		my $db_credentials_file_path = db_credentials_file($credentials);
		# We save away the credentials file path so we can delete it when the script ends.
		$config->{db}{$name}{'database_credentials-file'} = $db_credentials_file_path;
		if (not defined $db_credentials_file_path) {
		    $got_bad_config = 1;
		}
		else {
		    my $include_tables = $db->{'include-tables'};
		    my $exclude_tables = $db->{'exclude-tables'};
		    if (!defined($include_tables) || (!ref($include_tables) && $include_tables eq '')) {
			log_timed_message "FATAL:  database \"$name\" does not have a proper include-tables specified." if $DEBUG_FATAL;
			$got_bad_config = 1;
		    }
		    else {
			my $database_dump_command = db_dump_command ($credentials, $db_credentials_file_path, $include_tables, $exclude_tables);
			if (not defined $database_dump_command) {
			    $got_bad_config = 1;
			}
			else {
			    # log_timed_message "FERVID:  dump command for \"$name\":  $database_dump_command" if $DEBUG_FERVID;
			    # FIX THIS:  this is for development debugging only, and should never be used in production;
			    # it is quite likely that the use of backticks will interfere with POE's recognition of child
			    # process death, a race condition which could lead to deadlock
			    if ($name eq 'monarch') {
				# log_timed_message "executing database dump command for $name";
				# The database dump command can either be passed to a bash script, where it can be eval'd ...
				# `/home/gherteg/svn/monitor-dr/bin/command "$database_dump_command" > /home/gherteg/svn/monitor-dr/bin/,monarch`;
				# ... or it can be executed directly from here:
				# `$database_dump_command > /home/gherteg/svn/monitor-dr/bin/,monarch`;
			    }
			    # Save the command where we can find it when we span children.
			    $config->{db}{$name}{'database-dump-command'} = $database_dump_command;
			}
			my $database_load_command = db_load_command ($credentials, $db_credentials_file_path);
			if (not defined $database_load_command) {
			    $got_bad_config = 1;
			}
			else {
			    # log_timed_message "FERVID:  load command for \"$name\":  $database_load_command" if $DEBUG_FERVID;
			    # Save the command where we can find it when we span children.
			    $config->{db}{$name}{'database-load-command'} = $database_load_command;
			}
		    }
		}
	    }
	}
    }
}
if (0) {
    if (keys %app_name) {
	log_message '=== Application Aliases:';
	foreach my $alias (sort keys %app_name) {
	    log_message "$alias => $app_name{$alias}";
	}
    }
    if (keys %db_name) {
	log_message '=== Database Aliases:';
	foreach my $alias (sort keys %db_name) {
	    log_message "$alias => $db_name{$alias}";
	}
    }
}

# "qualified" in the various variable names in this section might actually mean "unqualified",
# as we don't necessarily have control over the hostnames we see here.

$qualified_primary_server   = $config->{'primary-server'};
$qualified_secondary_server = $config->{'secondary-server'};
($unqualified_primary_server   = $qualified_primary_server  ) =~ s/\..*//;
($unqualified_secondary_server = $qualified_secondary_server) =~ s/\..*//;

# hostname() may return either a qualified or unqualified hostname.  So we need to cope with
# either flavor of returned value, and not assume we will receive a fully qualified hostname
# in response to the call.  Our checking assumes that the primary and secondary servers won't
# have the same hostnames residing in different domains, which migght confuse the logic.  So
# we test separately for that unlikely condition to verify that we made the right choice.

   $qualified_local_hostname = hostname();
($unqualified_local_hostname = $qualified_local_hostname) =~ s/\..*//;
if ( $qualified_primary_server eq     $qualified_local_hostname ||
     $qualified_primary_server eq   $unqualified_local_hostname ||
    ($qualified_primary_server =~ /^$unqualified_local_hostname\./ && $qualified_local_hostname eq $unqualified_local_hostname)) {
    $i_am_primary = 1;
    $local_command_host    = 'localhost';	# for security, only allow command connections from the same machine
    $local_heartbeat_host  = $config->{'primary-server'};
    $local_command_port    = $config->{'primary-command-port'};
    $local_heartbeat_port  = $config->{'primary-heartbeat-port'};
    $remote_heartbeat_host = $config->{'secondary-server'};
    $remote_command_port   = $config->{'secondary-command-port'};
    $remote_heartbeat_port = $config->{'secondary-heartbeat-port'};
}
elsif ( $qualified_secondary_server eq     $qualified_local_hostname ||
	$qualified_secondary_server eq   $unqualified_local_hostname ||
       ($qualified_secondary_server =~ /^$unqualified_local_hostname\./ && $qualified_local_hostname eq $unqualified_local_hostname)) {
    $i_am_primary = 0;
    $local_command_host    = 'localhost';	# for security, only allow command connections from the same machine
    $local_heartbeat_host  = $config->{'secondary-server'};
    $local_command_port    = $config->{'secondary-command-port'};
    $local_heartbeat_port  = $config->{'secondary-heartbeat-port'};
    $remote_heartbeat_host = $config->{'primary-server'};
    $remote_command_port   = $config->{'primary-command-port'};
    $remote_heartbeat_port = $config->{'primary-heartbeat-port'};
}
else {
    log_timed_message "FATAL:  Host '$unqualified_local_hostname' is neither primary nor secondary server specified in the replication config file."
      if $DEBUG_FATAL;
    $got_bad_config = 1;
}
   $qualified_remote_hostname = $remote_heartbeat_host;
($unqualified_remote_hostname = $qualified_remote_hostname) =~ s/\..*//;

# Ensure that the setup allows the opposing server to connect, by ruling out certain obvious misconfigurations.
my %server = ('primary-server' => $qualified_primary_server, 'secondary-server' => $qualified_secondary_server);
while (my ($server, $host) = each %server) {
    my ($name, $aliases, $addrtype, $length, @addrs) = gethostbyname($host);
    foreach my $addr (@addrs) {
	my $ip_address = sprintf ("%vd", $addr);
	if ($ip_address eq '127.0.0.1') {
	    log_timed_message "FATAL:  The $server must be a network-interface hostname, not something that resolves to the loopback address.";
	    $got_bad_config = 1;
	}
    }
}

# FIX MAJOR:  double-check this code
# Note:  If any of these tests cause you difficulty (say, you really want the primary and secondary
# machines to have the same hostname in different domains), the way to get there is to establish
# additional hostname aliases that differ, just for purposes of the Replication State Engine.
if ( $qualified_primary_server eq $qualified_secondary_server) {
    # We have either a both-qualified match, or a both-unqualified match (qualified
    # primary and secondary servers were specified as either {a.b, a.b} or {a, a}).
    log_timed_message "FATAL:  You cannot specify the same machine in the replication config file as both primary-server and secondary-server.";
    $got_bad_config = 1;
}
elsif ($unqualified_primary_server eq $unqualified_secondary_server) {
    # We have as qualified names either {a, a.d}, {a.b, a}, or {a.b, a.d}.
    if ($qualified_primary_server   eq $unqualified_primary_server ||
	$qualified_secondary_server eq $unqualified_secondary_server) {
	# We have as qualified names either {a, a.d} or {a.b, a}.
	log_timed_message "FATAL:  The primary-server and secondary-server cannot be the qualified and unqualified names of the same machine.";
	$got_bad_config = 1;
    }
    elsif ($qualified_local_hostname eq $unqualified_local_hostname) {
	# We have {a.b, a.d} as qualified names in the config file, which makes them ordinarily
	# distinguishable; but hostname() returned an unqualified name, which means that our
	# matching against the config file necessarily did not check the domain names.
	# FIX MAJOR:  explain why that is bad
	log_timed_message "FATAL:  The primary-server and secondary-server cannot have the same hostname in different domains.";
	$got_bad_config = 1;
    }
    else {
	# FIX MAJOR:  what is the case here, that is sufficiently okay that we don't complain?
	# We have ...
    }
}
# If we got through the thicket above unscathed, then we have as qualified primary and secondary
# names either {a, c}, {a.b, c}, {a, c.d}, {a.b, c.b}, or {a.b, c.d}, all of which are good pairs.

# This relationship must hold:
#     0 <= $max_antiquated_heartbeats <= $max_archaic_heartbeats <= $max_ancient_heartbeats
if (0                          > $max_antiquated_heartbeats ||
    $max_antiquated_heartbeats > $max_archaic_heartbeats    ||
    $max_archaic_heartbeats    > $max_ancient_heartbeats) {
    my $message  = "Replication has a bad configuration of max_XXX_heartbeats parameters and will not start.";
    my $response = log_dispatch (LOG_LEVEL_FATAL, SEVERITY_CRITICAL, $message) if $DEBUG_FATAL;
    system_log $message if $DEBUG_FATAL;
    log_shutdown();
    exit 1;
}

$replica_machine = $qualified_remote_hostname;

# FIX MAJOR:  build up the directory structures under the pending-config-base-dir and backups-config-base-dir directories
# pending-config-base-dir: /usr/local/groundwork/replication/pending/
# backups-config-base-dir: /usr/local/groundwork/replication/backups/

$help = Replication::Help->new ();
$foundation = Replication::Foundation->new ();

if ($got_bad_config) {
    my $message  = "Replication has a bad config file $config_file and will not start.";
    my $response = log_dispatch (LOG_LEVEL_FATAL, SEVERITY_CRITICAL, $message) if $DEBUG_FATAL;
    system_log $message if $DEBUG_FATAL;
    log_shutdown();
    exit 1;
}

$nagios = Replication::Nagios->new (
    $config->{'nagios-command-pipe'},
    $config->{'max-command-pipe-write-size'},
    $config->{'max-command-pipe-wait-time'}
);
if (not defined $nagios) {
    my $message  = "Replication cannot create a Replication::Nagios object and will not start.";
    my $response = log_dispatch (LOG_LEVEL_FATAL, SEVERITY_CRITICAL, $message) if $DEBUG_FATAL;
    system_log $message if $DEBUG_FATAL;
    log_shutdown();
    exit 1;
}

my $enable_processing = $config->{'enable-processing'};
if (!defined($enable_processing) || $enable_processing ne 'yes') {
    my $message  = "Disaster Recovery replication is not enabled in its config file.";
    my $response = log_dispatch (LOG_LEVEL_FATAL, SEVERITY_CRITICAL, $message) if $DEBUG_FATAL;
    system_log $message if $DEBUG_FATAL;
    log_shutdown();
    exit 1;
}

$foundation->send_message(SEVERITY_OK, REPLICATION_ENGINE, $start_message);

do {
    local $SIG{__DIE__} = \&log_die;
    Replication::State->new ($statefile);
    load_replication_state($i_am_primary, $start_time) || die "Cannot load replication state.\n";
    # FIX THIS
    # validate_replication_state() || die "Cannot validate replication state in $statefile .\n";
};

$SIG{__WARN__} = sub {
    my $msg = shift;
    chomp $msg;
    log_timed_message $msg if $spill_warnings;
    if ($msg =~ /POE::Component::IKC::Server \d+: encountered bind error 98: Address already in use/) {
	log_timed_message "FATAL:  Server port is already in use." if $DEBUG_FATAL;
	$poe_kernel->signal($poe_kernel, 'UIDESTROY');
    }
    if ($msg =~ /POE::Component::IKC::Server \d+: encountered bind error 99: Cannot assign requested address/) {
	log_timed_message "FATAL:  Issue with assigning address to socket." if $DEBUG_FATAL;
	log_timed_message "FATAL:  Is /etc/hosts set up correctly (does it match actual DNS values)?" if $DEBUG_FATAL;
	log_timed_message "FATAL:  Also check $config_file settings for \"primary-server\" and \"secondary-server\"." if $DEBUG_FATAL;
	$poe_kernel->signal($poe_kernel, 'UIDESTROY');
    }
};

# For some reason, it seems we must create the servers before creating the sessions that support them.
# The documentation is not at all clear on this, and in general suggests the opposite is true.  Actual
# experiments show this is the only supported order.  This is probably ripe for a bug report.

# Another thing that is not at all clear is how session names are to be associated with kernel aliases.
# Can we make a specific session only available under a particular alias?  That's what I've been trying
# to do.  Or is any known alias for that kernel useable with any session running under that kernel?
# In a program that supports multiple servers, running on different ports, I'd like to treat them as
# partitioning the available sessions.

# FIX THIS:  this session is a playground for trying to implement synchronous signal handling,
# so no important application operations are interrupted while they're partway through executing
POE::Session->create(
    inline_states => {
	_start => sub {
	    $_[KERNEL]->alias_set('signals');
	    # We'd like to set our own signal handler here, and play with it.
	    # But in practice, we see that the default SIGINT handler seems
	    # to invoke a gentle shutdown, while SIGHUP, SIGQUIT, and SIGTERM
	    # are more abrupt.  So for the moment, we're just going to turn
	    # the other signals into a SIGINT.
	    # $_[KERNEL]->sig( HUP  => 'terminate' );
	    # $_[KERNEL]->sig( INT  => 'terminate' );
	    # $_[KERNEL]->sig( QUIT => 'terminate' );
	    # $_[KERNEL]->sig( TERM => 'terminate' );

	    $_[KERNEL]->sig( HUP  => 'pretend_sigint' );
	    $_[KERNEL]->sig( INT  => 'pretend_sigint' );
	    $_[KERNEL]->sig( QUIT => 'pretend_sigint' );
	    $_[KERNEL]->sig( TERM => 'pretend_sigint' );
	},
	pretend_sigint => sub {
	    log_timed_message "Received SIG$_[ARG0] signal; shutting down.";
	    $_[KERNEL]->signal( $_[KERNEL], 'INT' );
	},
	terminate => sub {
	    # Propagate the signal by turning it from a terminal signal into a benign signal.
	    log_timed_message "Received SIG$_[ARG0] signal; shutting down.";
	    my @sessions = $poe_api_peek->session_list();
	    foreach my $session (@sessions) {
		log_timed_message 'Running ', $poe_api_peek->session_id_loggable($session);
	    }
	    # FIX THIS:  whatever we're trying here to signal a safe landing, it's not working
	    $_[KERNEL]->signal( $_[KERNEL], 'shutdown' );
	    $_[KERNEL]->post('IKC', 'shutdown' );
	    # $_[KERNEL]->sig_handled();
	},
	_stop => sub {
	    log_timed_message "DEBUG:  Session 'signals' is being stopped." if $DEBUG_DEBUG;
	    $_[KERNEL]->alias_remove('signals');
	},
    },
);

# FIX LATER:  Any failure to resolve the IP addresses in the following POE::Component::IKC::Server->spawn()
# calls (say, because DNS is temporarily inaccessible because of a transient network failure; or because the
# port is still in use from a previous invocation) is not being handled here.  Do so, in the interest of
# reliability.  It's okay to simply die and allow the script to be restarted later by an external agent.
# [There is no documented mechanism for such error detection at the application level.  I have asked on the
# POE mailing list how this is to be done.]

# Note that in the following POE::Component::IKC::Server->spawn() calls, the "name"
# parameter has nothing to do with the particular Session created by the spawn() that
# will act as the server.  It is not a name or alias for that Session.  Rather, it is
# simply a POE kernel alias, one that will ultimately be known by the opposing client's
# kernel so it can send messages to this kernel, as part of a full Session specifier
# that will also include a session alias for the particular session (not the Session
# created by POE::Component::IKC::Server) that will actually handle a particular event
# posting.  The Server session only handles transport of request and presumably response.

# For concurrent command access, we allow one connection on which to submit interactive
# commands, plus one connection to submit brief scripted commands, plus a few spares
# "just in case" (perhaps primarily for sessions that people have left suspended and/or
# idle and forgotten about).  But we don't allow an unlimited number of connections,
# since that could possibly be a resource hog and reflect some kind of bad practice.
POE::Component::IKC::Server->spawn(
    ip => $local_command_host,
    port => $local_command_port,
    name => "$unqualified_local_hostname-command-server",
    concurrency => 5,
    verbose => 1
);

POE::Component::IKC::Server->spawn(
    ip => $local_heartbeat_host,
    port => $local_heartbeat_port,
    name => "$unqualified_local_hostname-heartbeat-server",
    concurrency => 1,
    verbose => 1
);

# FIX THIS
if ($spawn_ticktock_session) {
    POE::Session->create(
	inline_states => {
	    _start => sub {
		$_[KERNEL]->alias_set('ticktock');
		$_[KERNEL]->post('IKC', 'publish', 'ticktock', [qw(call_now post_now)]);
		$_[KERNEL]->yield('next');
	    },
	    next => sub {
		# print "tick ...\n";
		$_[KERNEL]->delay(next => 1);
	    },
	    call_now => sub {
		# Return response to the caller's specified callback.
		time;
	    },
	    post_now => sub {
		# Return response to the caller's prearranged reception point.
		$_[KERNEL]->post('IKC', 'post', 'poe://replication-command-client/clock/time', time);
	    },
	    _stop => sub {
		log_timed_message "DEBUG:  Session 'ticktock' is being stopped." if $DEBUG_DEBUG;
		$_[KERNEL]->delay('next');
		$_[KERNEL]->alias_remove('ticktock');
	    },
	},
    );
}

POE::Session->create(
    inline_states => {
	_start => sub {
	    $_[KERNEL]->alias_set("$unqualified_local_hostname-heartbeat");
	    $_[KERNEL]->post('IKC', 'publish', "$unqualified_local_hostname-heartbeat",
	      [qw(pump_heart report_local_state receive_remote_state)]);
	    connect_to_heartbeat_server();
	    $_[KERNEL]->delay ('throb', $config->{'contact-heartbeat-period'});
	},
	pump_heart                   => \&execute_heartbeat,
	throb                        => \&heart_throb,
	report_local_state           => \&report_local_state,
	run_child_process            => \&run_child_process,
	nagios_check_callback        => \&nagios_check_callback,
	return_local_state           => \&return_local_state,
	receive_remote_state         => \&receive_remote_state,
	remote_state_receipt_timeout => \&remote_state_receipt_timeout,
	_stop => sub {
	    log_timed_message "DEBUG:  Session '$unqualified_local_hostname-heartbeat' is being stopped." if $DEBUG_DEBUG;
	    $_[KERNEL]->delay ('throb');
	    $_[KERNEL]->alias_remove("$unqualified_local_hostname-heartbeat");
	},
    },
);

POE::Session->create(
    inline_states => {
	_start => sub {
	    $_[KERNEL]->alias_set("$unqualified_local_hostname-sync");
	    $_[KERNEL]->yield('initialize_sync_session');
	},
	# remote_capture_object is for a remote system to call us; conversely,
	# remote_capture_callback is for us to receive status back after we have called a remote system's remote_capture_object
	initialize_sync_session => \&initialize_sync_session,
	remote_capture_object   => \&remote_capture_object,
	capture_object          => \&capture_object,
	run_child_process       => \&run_child_process,
	local_capture_timed_out => \&local_capture_timed_out,
	local_capture_is_done   => \&local_capture_is_done,
	return_capture_result   => \&return_capture_result,
	remote_capture_callback => \&remote_capture_callback,
	_stop => sub {
	    log_timed_message "DEBUG:  Session '$unqualified_local_hostname-sync' is being stopped." if $DEBUG_DEBUG;
	    $_[KERNEL]->alias_remove("$unqualified_local_hostname-sync");
	},
    },
);

POE::Session->create(
    inline_states => {
	_start => sub {
	    $_[KERNEL]->alias_set("$unqualified_local_hostname-analyzer");
	    $_[KERNEL]->delay ('analyze', $config->{'contact-heartbeat-period'});
	},
	analyze => \&analyze,
	_stop => sub {
	    log_timed_message "DEBUG:  Session '$unqualified_local_hostname-analyzer' is being stopped." if $DEBUG_DEBUG;
	    $_[KERNEL]->delay ('analyze');
	    $_[KERNEL]->alias_remove("$unqualified_local_hostname-analyzer");
	},
    },
);

POE::Session->create(
    inline_states => {
	_start => sub {
	    $_[KERNEL]->alias_set('commands');
	    $_[KERNEL]->post('IKC', 'publish', 'commands', [qw(execute)]);
	},
	execute => \&execute_command,
	shutdown => sub {
	    # FIX THIS:  Shut this session down, and prove that you've done so.
	    # So far in testing, this hasn't stopped the session.
	    log_message 'shutting down the commands session';
	    $_[KERNEL]->post('IKC', 'retract', 'commands', [qw(execute)]);
	},
	_stop => sub {
	    log_timed_message "DEBUG:  Session 'commands' is being stopped." if $DEBUG_DEBUG;
	    $_[KERNEL]->alias_remove('commands');
	},
    },
);

# This session is special in that its startup routine and later processing actions will
# add new states corresponding to individual applications, so those replication actions for
# the applications can be separately scheduled.  This allows us, for instance, to drop all
# outstanding future alarms for a given application, while retaining the actions scheduled
# for other applications.
# FIX THIS:  drop the timeout events from this session, as they no longer belong here
POE::Session->create(
    inline_states => {
	_start => sub {
	    $_[KERNEL]->alias_set("$unqualified_local_hostname-applications");
	    my $object = 'application';
	    my $abbrev = 'app';
	    $_[KERNEL]->delay ('object_cleanup_startup',     $config->{'contact-heartbeat-period'}, { object => $object, abbrev => $abbrev, });
	    $_[KERNEL]->delay ('object_replication_startup', $config->{'contact-heartbeat-period'}, { object => $object, abbrev => $abbrev, });
	},
	run_child_process           => \&run_child_process,
	object_cleanup_startup      => \&object_cleanup_startup,
	object_cleanup_shutdown     => \&object_cleanup_shutdown,
	object_replication_startup  => \&object_replication_startup,
	object_replication_shutdown => \&object_replication_shutdown,
	obtain_object               => \&obtain_object,
	stop_object                 => \&stop_object,
	deploy_object               => \&deploy_object,
	start_object                => \&start_object,
	cull_object                 => \&cull_object,
	object_obtain_callback      => \&object_obtain_callback,
	object_stop_callback        => \&object_stop_callback,
	object_deploy_callback      => \&object_deploy_callback,
	object_start_callback       => \&object_start_callback,
	object_obtain_timeout       => \&object_obtain_timeout,
	object_stop_timeout         => \&object_stop_timeout,	# FIX THIS:  probably now orphaned and ought to be deleted
	object_cleanup_callback     => \&object_cleanup_callback,
	shutdown => sub {
	    # FIX THIS:  Shut this session down, and prove that you've done so.
	    # We need this capability so we can land gracefully if we're in the
	    # middle of sync operations.
	    # So far in testing, this hasn't stopped the session.
	    log_message "shutting down the $unqualified_local_hostname-applications session";
	},
	_stop => sub {
	    log_timed_message "DEBUG:  Session '$unqualified_local_hostname-applications' is being stopped." if $DEBUG_DEBUG;
	    $_[KERNEL]->delay ('object_cleanup_startup');
	    $_[KERNEL]->delay ('object_replication_startup');
	    my $object = 'application';
	    my $abbrev = 'app';
	    $_[KERNEL]->call ($_[SESSION], 'object_cleanup_shutdown',     { object => $object, abbrev => $abbrev, });
	    $_[KERNEL]->call ($_[SESSION], 'object_replication_shutdown', { object => $object, abbrev => $abbrev, });
	    $_[KERNEL]->alias_remove("$unqualified_local_hostname-applications");
	},
    },
);

# This session does the same with databases.
# FIX THIS:  drop the timeout events from this session, as they no longer belong here
POE::Session->create(
    inline_states => {
	_start => sub {
	    $_[KERNEL]->alias_set("$unqualified_local_hostname-databases");
	    my $object = 'database';
	    my $abbrev = 'db';
	    $_[KERNEL]->delay ('object_cleanup_startup',     $config->{'contact-heartbeat-period'}, { object => $object, abbrev => $abbrev, });
	    $_[KERNEL]->delay ('object_replication_startup', $config->{'contact-heartbeat-period'}, { object => $object, abbrev => $abbrev, });
	},
	run_child_process           => \&run_child_process,
	object_cleanup_startup      => \&object_cleanup_startup,
	object_cleanup_shutdown     => \&object_cleanup_shutdown,
	object_replication_startup  => \&object_replication_startup,
	object_replication_shutdown => \&object_replication_shutdown,
	obtain_object               => \&obtain_object,
	stop_object                 => \&stop_object,
	deploy_object               => \&deploy_object,
	start_object                => \&start_object,
	cull_object                 => \&cull_object,
	object_obtain_callback      => \&object_obtain_callback,
	object_stop_callback        => \&object_stop_callback,
	object_deploy_callback      => \&object_deploy_callback,
	object_start_callback       => \&object_start_callback,
	object_obtain_timeout       => \&object_obtain_timeout,
	object_stop_timeout         => \&object_stop_timeout,	# FIX THIS:  probably now orphaned and ought to be deleted
	object_cleanup_callback     => \&object_cleanup_callback,
	shutdown => sub {
	    # FIX THIS:  Shut this session down, and prove that you've done so.
	    # We need this capability so we can land gracefully if we're in the
	    # middle of sync operations.
	    # So far in testing, this hasn't stopped the session.
	    log_message "shutting down the $unqualified_local_hostname-databases session";
	},
	_stop => sub {
	    log_timed_message "DEBUG:  Session '$unqualified_local_hostname-databases' is being stopped." if $DEBUG_DEBUG;
	    $_[KERNEL]->delay ('object_cleanup_startup');
	    $_[KERNEL]->delay ('object_replication_startup');
	    my $object = 'database';
	    my $abbrev = 'db';
	    $_[KERNEL]->call ($_[SESSION], 'object_cleanup_shutdown',     { object => $object, abbrev => $abbrev, });
	    $_[KERNEL]->call ($_[SESSION], 'object_replication_shutdown', { object => $object, abbrev => $abbrev, });
	    $_[KERNEL]->alias_remove("$unqualified_local_hostname-databases");
	},
    },
);

POE::Session->create(
    inline_states => {
	_start => sub {
	    $_[KERNEL]->alias_set('tracking');
	    $_[KERNEL]->post(
		IKC => 'monitor',
		"$unqualified_remote_hostname-heartbeat-client" => {
		    register    => 'record_foreign_client_connect',
		    unregister  => 'record_foreign_client_disconnect',
		    subscribe   => 'record_foreign_client_subscribe',
		    unsubscribe => 'record_foreign_client_unsubscribe'
		}
	    );
	    $_[KERNEL]->post(
		IKC => 'monitor',
		"$unqualified_remote_hostname-heartbeat-server" => {
		    register    => 'record_foreign_server_connect',
		    unregister  => 'record_foreign_server_disconnect',
		    subscribe   => 'record_foreign_server_subscribe',
		    unsubscribe => 'record_foreign_server_unsubscribe'
		}
	    );
	},
	record_foreign_client_connect     => \&record_foreign_client_connect,
	record_foreign_client_disconnect  => \&record_foreign_client_disconnect,
	record_foreign_client_subscribe   => \&record_foreign_client_subscribe,
	record_foreign_client_unsubscribe => \&record_foreign_client_unsubscribe,
	record_foreign_server_connect     => \&record_foreign_server_connect,
	record_foreign_server_disconnect  => \&record_foreign_server_disconnect,
	record_foreign_server_subscribe   => \&record_foreign_server_subscribe,
	record_foreign_server_unsubscribe => \&record_foreign_server_unsubscribe,
	_stop => sub {
	    log_timed_message "DEBUG:  Session 'tracking' is being stopped." if $DEBUG_DEBUG;
	    $_[KERNEL]->alias_remove('tracking');
	},
    },
);

POE::Session->create(
    inline_states => {
	_start => sub {
	    $_[KERNEL]->alias_set("$unqualified_local_hostname-log-rotation");
	    $_[KERNEL]->delay ('log_rotation', $log_rotation_period);
	},
	log_rotation => \&log_rotation,
	_stop => sub {
	    log_timed_message "DEBUG:  Session '$unqualified_local_hostname-log-rotation' is being stopped." if $DEBUG_DEBUG;
	    $_[KERNEL]->delay ('log_rotation');
	    $_[KERNEL]->alias_remove("$unqualified_local_hostname-log-rotation");
	},
    },
);

# We enclose the run-time environment in an eval{} not because we ordinarily
# expect to exercise exceptions, but because from time to time we have seen
# some of the libraries we call die unexpectedly on us.  Should that happen,
# we need to capture their fatal message and log it for later inspection.
eval {
    POE::Kernel->run();
};
if ($@) {
    chomp $@;
    log_timed_message 'FATAL:  The POE kernel has died unexpectedly:' if $DEBUG_FATAL;
    log_timed_message $@;
}

# Clean up a bit of debris we used while we were alive and kicking.
foreach my $name ( keys %{ $config->{db} } ) {
    my $db_credentials_file_path = $config->{db}{$name}{'database_credentials-file'};
    unlink $db_credentials_file_path if defined $db_credentials_file_path;
}

my $stop_message = 'Replication is shutting down.';
$foundation->send_message(SEVERITY_WARNING, REPLICATION_ENGINE, $stop_message);
system_log $stop_message;
log_shutdown();
exit 0;

# ================================================================
# Supporting subroutines.
# ================================================================

# See the Config(3pm) man page for details of this magic formulation.
sub system_signal_name {
    my $signal_number = shift;
    my %sig_num;
    my @sig_name;

    unless ( $Config{sig_name} && $Config{sig_num} ) {
	return undef;
    }

    my @names = split ' ', $Config{sig_name};
    @sig_num{@names} = split ' ', $Config{sig_num};
    foreach (@names) {
	$sig_name[ $sig_num{$_} ] ||= $_;
    }

    return $sig_name[$signal_number] || undef;
}

# Note:  The decomposed wait status reported here may be a bit surprising under
# certain circumstances having to do with the way the child process handles signals.
# In particular, for instance, if you are running a script using "#!/bin/bash -e"
# (which is strongly recommended, so your script doesn't just try to continue running
# when component commands have aborted), and the shell also traps the SIGTERM signal,
# then when the Replication State Engine sends SIGTERM to the script's process group,
# a subsidiary process will be generally killed by the SIGTERM, and the shell will
# exit "normally" (not at the hand of the signal) and report the exit status of that
# subsidiary process as its own exit status, so the shell which is our immediate
# child will not be reported as having been itself killed by the signal.

sub wait_status_message {
    my $wait_status   = shift;
    my $exit_status   = $wait_status >> 8;
    my $signal_number = $wait_status & 0x7F;
    my $dumped_core   = $wait_status & 0x80;
    my $signal_name   = system_signal_name($signal_number) || "$signal_number is unknown";
    my $message = "exit status $exit_status" . ( $signal_number ? " (signal $signal_name)" : '' ) . ( $dumped_core ? ' (with core dump)' : '' );
    return $message;
}

sub print_usage {
    print "usage:  $PROGNAME [-h] [-v] [-c config_file] [-i] [-o]\n";
    print "where:  -h:  print this help message\n";
    print "        -v:  print the version number\n";
    print "        -c config_file:  specify an alternate config file\n";
    print "             (default is $default_config_file)\n";
    print "        -i:  run interactively, not as a persistent daemon\n";
    print "        -o:  write log messages also to standard output\n";
    print "The -o option is illegal unless -i is also specified.\n";
}

sub print_version {
    print "$PROGNAME Version:  $VERSION\n";
    print "Copyright 2010 GroundWork Open Source, Inc. (\"GroundWork\").\n";
    print "All rights reserved.\n";
}

sub parse_command_line {
    # First, clean up the $default_config_file value in case we print usage.
    my $real_path = realpath ($default_config_file);
    $default_config_file = $real_path if $real_path;

    my %opts;
    if (not getopts('hvc:dio', \%opts)) {
	print_usage();
	return 0;
    }

    $show_help          = $opts{h};
    $show_version       = $opts{v};
    $config_file        = (defined $opts{c} && $opts{c} ne '') ? $opts{c} : $default_config_file;
    $run_interactively  = $opts{i};
    $reflect_log_to_tty = $opts{o};

    # Adjust the config file specification to be an absolute pathname,
    # partly so we could be a bit more cavalier when we specified it, and
    # partly because a relative pathname would be misleading considering
    # that our working directory will generally be different from where
    # we started, even if we don't run this script as a daemon.
    $config_file = "$Bin/../config/$config_file" if $config_file !~ m{^/};
    $real_path = realpath($config_file);
    if (!$real_path) {
	spill_message "FATAL:  The path to the $PROGNAME config file $config_file either does not exist or is inaccessible to this script running as ", (scalar getpwuid $>), '.';
	return 0;
    }
    $config_file = $real_path;

    if (!$run_interactively && $reflect_log_to_tty) {
	print_usage();
	return 0;
    }

    return 1;
}

# The min_kept_backups value doesn't affect the valid_min_backup_age() result, but we validate it here.
# Yes, this is a moderate hack.
sub valid_min_backup_age {
    my $min_kept_backups = shift;
    my $min_backup_time  = shift;

    return undef if $min_kept_backups !~ /^\d+$/;
    return undef if $min_backup_time  !~ /^(\d+)\s+(\w+)$/;

    my $count = $1;
    my $units = $2;

    return undef if not exists $time_in_seconds{$units};
    return $count * $time_in_seconds{$units};
}

# Validate the configuration of timeouts:
# * all timeout values must be present and non-negative
# * a non-zero abandon timeout must be strictly larger than the warning timeout
# * a non-zero sigterm timeout must be strictly larger than the warning timeout
# * a non-zero sigkill timeout must be strictly larger than both the warning timeout and the sigterm timeout
# * a non-zero sigkill timeout is not allowed without a non-zero sigterm timeout

sub timeouts_are_bad {
    my $object     = shift;
    my $obj_config = shift;
    my $name       = shift;
    my $is_bad     = 0;

    foreach my $action_timeouts ('obtain-timeouts') {
	if (!defined $obj_config->{$action_timeouts}) {
	    log_timed_message "FATAL:  $action_timeouts for $object \"$name\" are not defined." if $DEBUG_FATAL;
	    $is_bad = 1;
	}
	elsif (ref $obj_config->{$action_timeouts} ne 'ARRAY') {
	    log_timed_message "FATAL:  $action_timeouts for $object \"$name\" is not an array." if $DEBUG_FATAL;
	    $is_bad = 1;
	}
	elsif (@{ $obj_config->{$action_timeouts} } != 2) {
	    log_timed_message "FATAL:  $action_timeouts for $object \"$name\" does not contain 2 elements." if $DEBUG_FATAL;
	    $is_bad = 1;
	}
	elsif ($obj_config->{$action_timeouts}[WARNING_TIMEOUT] < 0 ||
	       $obj_config->{$action_timeouts}[ABANDON_TIMEOUT] < 0) {
	    log_timed_message "FATAL:  $action_timeouts for $object \"$name\" cannot be negative." if $DEBUG_FATAL;
	    $is_bad = 1;
	}
	elsif ($obj_config->{$action_timeouts}[ABANDON_TIMEOUT] > 0 &&
	       $obj_config->{$action_timeouts}[ABANDON_TIMEOUT] <= $obj_config->{$action_timeouts}[WARNING_TIMEOUT]) {
	    log_timed_message "FATAL:  $action_timeouts for $object \"$name\" non-zero ABANDON timeout must be larger than WARNING timeout."
	      if $DEBUG_FATAL;
	    $is_bad = 1;
	}
    }
    foreach my $action_timeouts ('capture-timeouts', 'stop-timeouts', 'deploy-timeouts', 'start-timeouts', 'cleanup-timeouts') {
	if (!defined $obj_config->{$action_timeouts}) {
	    log_timed_message "FATAL:  $action_timeouts for $object \"$name\" are not defined." if $DEBUG_FATAL;
	    $is_bad = 1;
	}
	elsif (ref $obj_config->{$action_timeouts} ne 'ARRAY') {
	    log_timed_message "FATAL:  $action_timeouts for $object \"$name\" is not an array." if $DEBUG_FATAL;
	    $is_bad = 1;
	}
	elsif (@{ $obj_config->{$action_timeouts} } != 3) {
	    log_timed_message "FATAL:  $action_timeouts for $object \"$name\" does not contain 3 elements." if $DEBUG_FATAL;
	    $is_bad = 1;
	}
	elsif ($obj_config->{$action_timeouts}[WARNING_TIMEOUT] < 0 ||
	       $obj_config->{$action_timeouts}[SIGTERM_TIMEOUT] < 0 ||
	       $obj_config->{$action_timeouts}[SIGKILL_TIMEOUT] < 0) {
	    log_timed_message "FATAL:  $action_timeouts for $object \"$name\" cannot be negative." if $DEBUG_FATAL;
	    $is_bad = 1;
	}
	elsif ($obj_config->{$action_timeouts}[SIGTERM_TIMEOUT] > 0 &&
	       $obj_config->{$action_timeouts}[SIGTERM_TIMEOUT] <= $obj_config->{$action_timeouts}[WARNING_TIMEOUT]) {
	    log_timed_message "FATAL:  $action_timeouts for $object \"$name\" non-zero SIGTERM timeout must be larger than WARNING timeout."
	      if $DEBUG_FATAL;
	    $is_bad = 1;
	}
	elsif ($obj_config->{$action_timeouts}[SIGKILL_TIMEOUT] > 0 &&
	      ($obj_config->{$action_timeouts}[SIGKILL_TIMEOUT] <= $obj_config->{$action_timeouts}[WARNING_TIMEOUT] ||
	       $obj_config->{$action_timeouts}[SIGKILL_TIMEOUT] <= $obj_config->{$action_timeouts}[SIGTERM_TIMEOUT])) {
	    log_timed_message "FATAL:  $action_timeouts for $object \"$name\" non-zero SIGKILL timeout must be larger than both WARNING and SIGTERM timeouts."
	      if $DEBUG_FATAL;
	    $is_bad = 1;
	}
	elsif ($obj_config->{$action_timeouts}[SIGKILL_TIMEOUT] >  0 &&
	       $obj_config->{$action_timeouts}[SIGTERM_TIMEOUT] == 0) {
	    log_timed_message "FATAL:  $action_timeouts for $object \"$name\" non-zero SIGKILL timeout must have non-zero SIGTERM timeout."
	      if $DEBUG_FATAL;
	    $is_bad = 1;
	}
    }
    return $is_bad;
}

# Tickets are dispensed essentially to provide security for remote operations, to abort carrying forward
# should we receive a response to a remote action request we made a long time ago and wanted to cancel.
# What we want is a globally unique ID for each ticket.  That might include an IP address, the process ID,
# a sequence number, and a timestamp.  Using just a sequence number and the source/target machine names
# for the time being is a poor-man's simplified mechanism.
sub next_ticket {
    while (++$last_ticket_number > 2_000_000_000) {
	# We just wrap around, figuring it will be an extremely long time before we see the same
	# ticket numbers again, so we don't worry about it.  The alternative is essentially to
	# die and let whatever daemon keeper is operating restart us, or to restart ourself.
	# Probably the larger danger, if there is one, is that restarting the entire process
	# begins the counting sequence at the same starting point used by an earlier run.  Which
	# is equivalent to wrapping around as we do here.
	$last_ticket_number = 0;
    }
    return "request $last_ticket_number from $unqualified_local_hostname to $unqualified_remote_hostname";
}

sub log_all_kernels {
    if ($DEBUG_DEBUG) {
	my @kernels = known_kernels();
	log_timed_message 'DEBUG:  '.(scalar @kernels).' kernels are known:';
	foreach my $kernel (@kernels) {
	    log_timed_message "DEBUG:  kernel $kernel";
	}
    }
}

sub log_all_sessions {
    if ($DEBUG_DEBUG) {
	# Spill out a list of all current sessions in this application.

	# my $session = $poe_kernel->get_active_session();
	# log_timed_message "DEBUG:  current active session is ", $poe_api_peek->session_id_loggable($session);

	my @sessions = $poe_api_peek->session_list();
	my %sessions = map { $_->ID => $poe_api_peek->session_id_loggable($_) } @sessions;
	log_timed_message 'DEBUG:  '.(scalar @sessions).' sessions are running:';
	foreach my $session_id (sort {$a <=> $b} keys %sessions) {
	    log_timed_message "DEBUG:  $sessions{$session_id}";
	}
    }
}

sub connect_to_heartbeat_server {
    log_timed_message "NOTICE:  spawning client $unqualified_local_hostname-heartbeat-client" if $DEBUG_NOTICE;

    # Note that in this POE::Component::IKC::Client->spawn() call, neither the "name" nor the "aliases" parameter
    # has anything to do with the particular Session that is internally created by the spawn().  Rather, these
    # are simply additional aliases for the local kernel that will become known to the remote kernel for use in
    # communicating between them.  Once a connection is established, the two systems essentially become peers,
    # so the labeling of the kernels as "-client" or "-server" is essentially misleading.  For simplicity,
    # we could have used "$unqualified_local_hostname-heartbeat-server" (as established by the "name" parameter
    # in an earlier call to POE::Component::IKC::Server->spawn() when the server listener was established) as
    # the "name" parameter here, without establishing any additional aliases.  However, the present construction
    # allows us to track which side has initiated the connection, which is occasionally helpful in debugging.
    # (When a connection is made, both the "-client" value in the "name" and the "-server" value in the "aliases"
    # specified here are sent to the remote system.  It responds with all of the local kernel aliases it currently
    # has in play on what we see locally here as the {$remote_heartbeat_host, $remote_heartbeat_port}, which will
    # include the "-server" given as the "name" parameter in the remote side's POE::Component::IKC::Server->spawn()
    # call on that interface and port.  Since the remote side won't have a "-client" alias in play if it is not
    # initiating a connection as a client, only the target server will know of that name as a remote kernel alias
    # [for this, our local system], enabling us to distinguish which end initiated the connection.)
    #
    # Note that if we had wanted a unidirectional connection, we could have specified only the "name" here (as a
    # "-client").  That would make the local server alias ("-server") unknown to the remote system (at least, on
    # this connection), so the remote system could not post arbitrary messages back here to our server on the
    # local system just because we made the initial client (here) to server (there) connection.  However, if
    # we try that (trying to make separate unidirectional cross-connections on different channels), we find it
    # impossible to usefully make the reverse connection (initiated as a client from the remote side) on a
    # separate channel.  That's because the local kernel's original globally unique name ($poe_kernel->ID) is
    # shared with the remote system when the first connection is established, and stored on the remote system
    # in a place which is independent of the channel.  The storage of that original name then changes the
    # Responder->register() logic on the remote client (now initiating) side when the reverse connection is
    # processed, as it presumes that once a connection is established, that same channel will be used for all
    # future communication between the two kernels.  The upshot is that the kernel aliases implicitly provided
    # in the second connection (i.e., associated with the server host/port the client has connected to) are not
    # processed.  In particular, the "-server" alias which is provided by the local side at this time is not
    # recorded on that remote system (see the Responder->register() logic, and how it aborts early), so the
    # remote (client) system will be unable to subscribe to it, and will be unable to post requests to it.  In
    # addition, monitoring of the aliases now provided by the second connection is never called on the client
    # system, so the remote application never becomes directly aware that the second connection is available
    # through that mechanism.  (Monitoring must work, because it is the only way we will later be able to find
    # out if the connection drops.)  All of this unexpected behavior forces us back to the model of using a
    # single bidirectional channel instead, and it means that we must test out any possible issues with the
    # race conditions and other complications that might arise when we do make cross-connections.

    # The POE::Component::IKC::Client->spawn() call below creates a Session, which we wish to not interfere
    # with future attempts to create a client (say, if a successful connection was lost and we need to try to
    # re-establish a connection).  The simplest way to make that happen and avoid a possible session leak is
    # to shut down any previous client session before trying to create a new one.  Among other benefits, this
    # will free up the session alias for use by the new session, so the purpose of each session is clear from
    # a listing of all the session aliases.  Possibly we should take other action as well, such as unsubscribe,
    # but that will be left to some future evolution of the code, once we test the behavior of our subscribe
    # flags much more thoroughly under failure conditions.
    if (defined $heartbeat_client_session_id) {
	if ($poe_kernel->ID_id_to_session($heartbeat_client_session_id)) {
	    # Shutting down an existing client session will force its associated channel to be shut down.
	    # And we want that, to ensure that any existing kernel-level notion that a connection exists
	    # will be destroyed, before we go creating a new channel for the new connection.  This will
	    # also effectively implement a timeout on the old session, and prevent any unexpected calls
	    # later on to the old session's on_connect or on_error handler from interfering with operation
	    # of the new session.  Likely this imposed timeout will be quicker than that which occurs if
	    # the session is left alone and is unable to connect due to a network failure; experience shows
	    # that timeout to be 189 seconds (I'm not sure where that value is set, or what combination of
	    # timeouts accumulate to get there).
	    #
	    # As a result of this call, the previous named instance of the client session will be shut down.
	    # Its name/alias will disappear immediately, but its full session object won't be deleted until
	    # after we exit back to the kernel and it gets a chance to run garbage collection.  Still, this
	    # is enough to free up the session alias so we can use it again for the new session.
	    log_timed_message "NOTICE:  destroying $unqualified_local_hostname-heartbeat-client (session $heartbeat_client_session_id)"
	      if $DEBUG_NOTICE;
	    $poe_kernel->call($heartbeat_client_session_id, 'shutdown');
	}
	$heartbeat_client_session_id = undef;
    }
    $heartbeat_client_session_id = POE::Component::IKC::Client->spawn(
	ip   => $remote_heartbeat_host,
	port => $remote_heartbeat_port,
	name => "$unqualified_local_hostname-heartbeat-client",
	# We need to establish the alias of the heartbeat server along with that of the heartbeat client, else
	# the alias of the local heartbeat server will never become known to the remote system.  (It seems that
	# an exchange of kernel aliases is made at the time of connection, in addition to the client sending the
	# "name" parameter here [as another local kernel alias] to the remote system.  I don't know if there
	# is some way to subsequently make a local kernel alias known to a connected remote system.)
	aliases => [ "$unqualified_local_hostname-heartbeat-server" ],
	# We cannot subscribe here directly in the client spawn, because if it fails to subscribe, it will die().
	# Instead we need to implement the subscription in a manner that allows us to catch and process errors,
	# so we will subscribe only after the connection is established.
	# subscribe => [qw(poe://$unqualified_remote_hostname-heartbeat-server/$unqualified_remote_hostname-heartbeat)],
	on_connect => \&create_heartbeat_sessions,
	on_error   => \&heartbeat_connection_error
    );
    # We want to know why we have sometimes seen the spawn() get executed but neither the on_connect handler
    # nor the on_error handler ever got called.  Perhaps spilling out the Session ID might be a first step
    # in debugging that condition, if it ever arises again.
    # FIX LATER:  Perhaps dump the queue of events for various sessions, including the time at which
    #             each event will be delivered to the session, to see if that might give us some clues
    #             about what particular sessions are waiting for?
    # Did the session really get created?
    if (defined $heartbeat_client_session_id) {
	log_timed_message "DEBUG:  $unqualified_local_hostname-heartbeat-client has been created as session $heartbeat_client_session_id."
	  if $DEBUG_DEBUG;
    }
    else {
	log_timed_message "ERROR:  $unqualified_local_hostname-heartbeat-client has not been created." if $DEBUG_ERROR;
    }
    log_all_sessions();
}

sub record_foreign_client_connect {
    my ($name, $real) = @_[ARG1, ARG2];
    log_timed_message "NOTICE:  Remote client kernel ", ($real ? '' : 'alias '), $name, ' has connected.' if $DEBUG_NOTICE;
    if ($name eq "$unqualified_remote_hostname-heartbeat-client") {
	log_timed_message "FERVID:  setting connected_from_remote_heartbeat_client = 1" if $DEBUG_FERVID;
	$connected_from_remote_heartbeat_client = 1;
    }
}

sub record_foreign_client_disconnect {
    my ($name, $real) = @_[ARG1, ARG2];
    log_timed_message "NOTICE:  Remote client kernel ", ($real ? '' : 'alias '), $name, ' has disconnected.' if $DEBUG_NOTICE;
    if ($name eq "$unqualified_remote_hostname-heartbeat-client") {
	if ($connected_from_remote_heartbeat_client) {
	    log_timed_message "FERVID:  setting connected_from_remote_heartbeat_client = 0" if $DEBUG_FERVID;
	    $connected_from_remote_heartbeat_client = 0;
	}
    }
}

sub record_foreign_client_subscribe {
    my ($name, $real) = @_[ARG1, ARG2];
    my $target = specifier_name($_[ARG4]);
    log_timed_message 'NOTICE:  Subscribed to ', $target, ' on remote client kernel ', ($real ? '' : 'alias '), $name if $DEBUG_NOTICE;
    if ($name eq "$unqualified_remote_hostname-heartbeat-client") {
	log_timed_message "FERVID:  setting subscribed_from_remote_heartbeat_client = 1" if $DEBUG_FERVID;
	$subscribed_from_remote_heartbeat_client = 1;
    }
}

sub record_foreign_client_unsubscribe {
    my ($name, $real) = @_[ARG1, ARG2];
    my $target = specifier_name($_[ARG4]);  # ARG4 is not documented as such for unsubscribe; we're assuming parallelism with subscribe here
    log_timed_message 'NOTICE:  Unsubscribed from ', $target, ' on remote client kernel ', ($real ? '' : 'alias '), $name if $DEBUG_NOTICE;
    if ($name eq "$unqualified_remote_hostname-heartbeat-client") {
	if ($subscribed_from_remote_heartbeat_client) {
	    log_timed_message "FERVID:  setting subscribed_from_remote_heartbeat_client = 0" if $DEBUG_FERVID;
	    $subscribed_from_remote_heartbeat_client = 0;
	}
    }
}

sub record_foreign_server_connect {
    my ($name, $real) = @_[ARG1, ARG2];
    log_timed_message "NOTICE:  Remote server kernel ", ($real ? '' : 'alias '), $name, ' has connected.' if $DEBUG_NOTICE;
    if ($name eq "$unqualified_remote_hostname-heartbeat-server") {
	log_timed_message "FERVID:  setting connected_to_remote_heartbeat_server = 1" if $DEBUG_FERVID;
	$connected_to_remote_heartbeat_server = 1;
	$last_remote_heartbeat_server_connect_time = time();
    }
}

sub record_foreign_server_disconnect {
    my ($name, $real) = @_[ARG1, ARG2];
    log_timed_message "NOTICE:  Remote server kernel ", ($real ? '' : 'alias '), $name, ' has disconnected.' if $DEBUG_NOTICE;
    if ($name eq "$unqualified_remote_hostname-heartbeat-server") {
	# As a possible alternative condition here, we might try testing
	#     (defined $_[KERNEL]->alias_resolve("$unqualified_remote_hostname-heartbeat-server"))
	# to see if it returns a session reference or undef.
	# Also look at $kernel->call('IKC') (see the POE::Component::IKC documentation), as a means of telling
	# whether you are still connected to a foreign kernel.  However, that may require first sending a message.
	if ($connected_to_remote_heartbeat_server) {
	    log_timed_message "FERVID:  setting connected_to_remote_heartbeat_server = 0" if $DEBUG_FERVID;
	    $connected_to_remote_heartbeat_server = 0;
	}
    }
}

sub record_foreign_server_subscribe {
    my ($name, $real) = @_[ARG1, ARG2];
    my $target = specifier_name($_[ARG4]);
    log_timed_message 'NOTICE:  Subscribed to ', $target, ' on remote server kernel ', ($real ? '' : 'alias '), $name if $DEBUG_NOTICE;
    if ($target eq "poe://$unqualified_remote_hostname-heartbeat-server/$unqualified_remote_hostname-heartbeat") {
	log_timed_message "FERVID:  setting subscribed_to_remote_heartbeat_server = 1" if $DEBUG_FERVID;
	$subscribed_to_remote_heartbeat_server = 1;
    }
}

sub record_foreign_server_unsubscribe {
    my ($name, $real) = @_[ARG1, ARG2];
    my $target = specifier_name($_[ARG4]);  # ARG4 is not documented as such for unsubscribe; we're assuming parallelism with subscribe here
    log_timed_message 'NOTICE:  Unsubscribed from ', $target, ' on remote server kernel ', ($real ? '' : 'alias '), $name if $DEBUG_NOTICE;
    if ($target eq "poe://$unqualified_remote_hostname-heartbeat-server/$unqualified_remote_hostname-heartbeat") {
	log_timed_message "FERVID:  setting subscribed_to_remote_heartbeat_server = 0" if $DEBUG_FERVID;
	$subscribed_to_remote_heartbeat_server = 0;
    }
}

# It's impolite to look inside the internals of a POE::Component::IKC::Responder::Object; object,
# but that package is not already providing a direct view, so we have to do what we have to do.
# This code works with POE-Component-IKC-0.2200, and probably ought to be folded directly into
# the standard distribution so it is maintained along with that code.
sub known_kernels {
    my @kernels = (
	keys %{ $ikc->{channel} },  # kernel names (IDs)
	keys %{ $ikc->{kernel} }    # kernel aliases
    );
    return @kernels;
}

# Does a particular kernel have its name (ID) or alias known by the Responder?
sub kernel_is_known {
    my $name = $_[0];
    my @kernels = known_kernels();
    my %kernels = map { $_ => 1 } @kernels;
    return $kernels{$name};
}

# Does the Responder not only know about a particular kernel, but also have a channel open to it?
sub kernel_is_connected {
    my $name = $_[0];
    my @channels = undef;
    eval {
	@channels = $ikc->channel_list($name);
    };
    return 0 if $@;
    return scalar @channels;
}

sub create_heartbeat_sessions {
    # Create sessions that depend on the foreign kernel, perhaps something that monitors such connections.
    # We're currently using monitor to find out when the subscription succeeds, rather than the callback given here.
    # However, the monitor callback for a subscription will never be called if the subscription never succeeds, so
    # we still need this callback to tell us when a subscription request has timed out, so we can disconnect and
    # retry the connection.  (We've seen situations in previous versions of this code where a connection got made
    # but then the timeout of a long-running, overlapping previous connection attempt finally got recognized [and
    # its connection on_error callback got called, to let us know that], and this destroyed the useful connection
    # before the subscription attempt completed.  That should no longer be possible in the present code, now that
    # before we try to connect as a client, we shut down a previous client session that was still trying to connect.
    # Still, we're trying to be completely robust here, against race conditions we haven't yet encountered.)
    log_timed_message "NOTICE:  Connected to remote system \"$unqualified_remote_hostname\"; subscribing to remote heartbeat server."
      if $DEBUG_NOTICE;
    $poe_kernel->post('IKC', 'subscribe',
	[ "poe://$unqualified_remote_hostname-heartbeat-server/$unqualified_remote_hostname-heartbeat" ],
	# When this is called, the subscription request has either been locally denied (e.g., no kernel of that name is known),
	# or remotely denied (via response from the remote kernel), or subscribed, or timed out.  We're using an embedded coderef
	# here rather than a state, because a state is interpreted relative to the calling session, and it's not clear what that
	# would be in the present context.
	sub {
	    my $specifiers = $_[0];

	    # A remote kernel connection could be originated and established starting from the other side, with
	    # the connection completing after we found there was no connection and started our own attempt to
	    # connect as a client, while we were then otherwise busy and before our subscription attempt completed.
	    # Or it could be disrupted from the other side in that same time frame.  So there are lots of possible
	    # race conditions here.

	    # Not having the remote kernel connected is by itself a sufficient condition for concluding that the
	    # connection is down and we must re-try the connection, without regard to whether the subscription
	    # attempt from our side completed or timed out.
	    my $not_connected = not kernel_is_connected( "$unqualified_remote_hostname-heartbeat-server" );

	    # Did we fail to subscribe?
	    my $timed_out = !defined($specifiers) || @$specifiers != 1;

	    # It might be actually be okay if we got a timeout on the subscription request originating here as a client,
	    # as long as the remote heartbeat kernel name is now known here (presumably via a connection originating from
	    # the other side).  But that would be very hard to test for, as it involves a race condition.  So for safety,
	    # we will record a failure, and later initiate a re-try, no matter which bad condition arises.  Hopefully,
	    # this strategy won't result in fibrillation as both sides continually recognize a failure condition and
	    # continually attempt to reconnect.  What we're expecting instead is that the situation should stabilize,
	    # as a successful connection from the other side should effectively disable the conditions for failure here.

	    if ( $not_connected || $timed_out ) {
		log_timed_message
		    "NOTICE:  subscription to poe://$unqualified_remote_hostname-heartbeat-server/$unqualified_remote_hostname-heartbeat failed",
		    ($not_connected ? ' (not connected)' : ''), ($timed_out ? ' (timed out)' : '') if $DEBUG_NOTICE;
		# When the connection was made, we thought the subscription would go through as well, so we set a flag to
		# indicate the connection was up.  We needed to do that to prevent other connection attempts before the
		# subscription attempt is done.  But if the subscription is incomplete, that connection is useless to us.
		if ($connected_to_remote_heartbeat_server) {
		    log_timed_message "FERVID:  subscription failed; setting connected_to_remote_heartbeat_server = 0" if $DEBUG_FERVID;
		    $connected_to_remote_heartbeat_server = 0;
		}
	    }
	    log_all_kernels();
	});
}

sub heartbeat_connection_error {
    # could not connect; tell the user why not
    my ($operation, $errnum, $errstr) = @_;
    if ($operation eq 'connect' && $errnum == ECONNREFUSED) {
	log_timed_message "ERROR:  Cannot contact the replication heartbeat server at \"$remote_heartbeat_host\" port $remote_heartbeat_port."
	  if $DEBUG_ERROR;
	# For all practical purposes, if we cannot connect, that's as good (or as bad, actually) as having connected but getting no response.
	# FIX MAJOR:  cancel any outstanding heartbeat ticket here?
	$got_remote_state_timeout = 1;
    }
    else {
	# Whatever shows up here is valuable for diagnosing problems in the field.
	# We have seen ($errstr eq "Connection timed out") errors here, at least.
	# FIX MINOR:  Should we set $got_remote_state_timeout if we have ($errstr eq "Connection timed out") here?
	log_timed_message "WARNING:  Got $operation error ($errstr) trying to contact the replication heartbeat server at \"$remote_heartbeat_host\" port $remote_heartbeat_port."
	  if $DEBUG_WARNING;
    }
}

# Boolean substates:
#   engine_running
#   heartbeat_running
#   notify_allowed
#   notify_grabbed
#   notify_locked
#   config_grabbed
#   sync_all_blocked
#   commit_running
#   rollback_running

sub execute_command {
    # The command is in $_[ARG0] and should be parsed and executed.

    # See earlier in this script for why we need to prepend a leading space to the command.
    # Hopefully, we'll be able to work around that in some future version.
    my $lines = $command_parser->Command(' ' . $_[ARG0]);
    if (defined $lines) {
	return join("\n",@$lines);
    }
    else {
	my $explanation = $is_valid_command{$_[ARG0]} ?
	    "\n\"$_[ARG0]\" is not valid command syntax;\ntry \"help $_[ARG0]\" for details of command syntax,\nor" :
	    "\n\"$_[ARG0]\" is not a recognized command;\ntry";
	return "$explanation \"help\" for a list of valid commands.";
    }
}

#   help Word(s?)
sub provide_help {
    my $args = $_[0];
    if (scalar(@$args) == 0) {
	return $help->help();
    }
    my $text = $help->lookup()->{$args->[0]};
    if ($text) {
	return [ $text ];
    }
    else {
	return [ '', "\"$args->[0]\" is not a recognized command;\ntry \"help\" for a list of valid commands." ];
    }
}

#   about
sub display_metadata {

    open BUILD, '<', "$Bin/../info/build_info";
    my $build_info = join('', <BUILD>);
    close BUILD;
    chomp $build_info;
    $build_info = '(build info is missing)' if !$build_info;

    return [ <<EOF ];

Replication State Engine for Disaster Recovery control.
Version:  $VERSION

Copyright (c) 2010 GroundWork Open Source (www.groundworkopensource.com).
All rights reserved.

Build info:
$build_info

EOF
}

#   login [<username> [<password>]]
sub log_in {
    my $username = shift;  # might be undefined
    my $password = shift;  # might be undefined
    my @response = ();
    # FIX THIS:  implement this command
    push @response, $blank_line;
    push @response, '"login" is not yet implemented.';
    return \@response;
}

#   engine start
sub start_engine {
    my @response = ();
    # FIX THIS:  implement this command
    push @response, $blank_line;
    push @response, '"engine start" is not yet implemented.';
    return \@response;
}

#   engine stop
sub stop_engine {
    my @response = ();
    # FIX THIS:  implement this command
    push @response, $blank_line;
    push @response, '"engine stop" is not yet implemented.';
    return \@response;
}

# display_object_status ('local',  'application', 'app', \@app_names, \%app_name, 'all')
# display_object_status ('local',  'application', 'app', \@app_names, \%app_name, [ alias, ... ])
# display_object_status ('local',  'database',    'db',  \@db_names,  \%db_name,  'all')
# display_object_status ('local',  'database',    'db',  \@db_names,  \%db_name,  [ alias, ... ])
# display_object_status ('remote', 'application', 'app', \@app_names, \%app_name, 'all')
# display_object_status ('remote', 'application', 'app', \@app_names, \%app_name, [ alias, ... ])
# display_object_status ('remote', 'database',    'db',  \@db_names,  \%db_name,  'all')
# display_object_status ('remote', 'database',    'db',  \@db_names,  \%db_name,  [ alias, ... ])

sub display_object_status {
    my $site      = shift;  # 'local', 'remote', or undefined (both)
    my $object    = shift;  # 'application' or 'database'
    my $abbrev    = shift;  # 'app' or 'db'
    my $names     = shift;  # arrayref for configured object names
    my $name      = shift;  # hashref for alias-to-name lookups
    my $aliases   = shift;  # either string ('all') or arrayref

    if (!@$names) {
	return [ '', "ERROR:  No ${object}s are configured." ];
    }
    if (not ref $aliases) {
	$aliases = $names;
    }

    my @response = ();
    foreach my $alias (@$aliases) {
	push @response, "ERROR:  \"$alias\" is not a known alias for any $object." if not exists $name->{$alias};
    }
    if (@response) {
	unshift @response, $blank_line;
	return \@response;
    }

    my $local_state  = fetch_local_state();
    my $remote_state = fetch_remote_state();

    my $object_width = length $object;
    foreach my $alias (@$aliases) {
	my $alias_width = length $alias;
	$object_width = $alias_width if $alias_width > $object_width;
    }

    # FIX THIS:  we will want to know not only whether sync operations are blocked, but also whether
    # they are currently ongoing, and whether particular sets of files are ready for a commit operation
    my $title_line   = "\u$object Replication Status";
    my $major_line   = '=======================================================================================';
    my $minor_line   = '---------------------------------------------------------------------------------------';
    my $format       = "%-24s  %-6s  %-${object_width}s  %9s  %8s  %s";

    push @response, $blank_line;
    push @response, $title_line;
    push @response, $major_line;
    push @response, sprintf ($format, 'Last Replication Time', 'Site', "\u${object}", 'Blocked?', 'Active?', 'Hostname');
    push @response, $minor_line;

    my $have_old_local_sync_time  = 0;
    my $have_old_remote_sync_time = 0;

    if (!defined($site) || $site eq 'local') {
	foreach my $alias (@$aliases) {
	    my $sync_time = $local_state->{$abbrev}{ $name->{$alias} }{last_sync_time};
	    my $blocked   = $local_state->{$abbrev}{ $name->{$alias} }{blocked} ? 'blocked' : 'unblocked';
	    my $active    = $local_state->{$abbrev}{ $name->{$alias} }{active} || 'inactive';
	    my $timestamp = $sync_time ? (scalar localtime $sync_time) : '(not yet attempted)';
	    if ($sync_time && ($sync_time <= $local_state->{start_time})) {
		$have_old_local_sync_time = 1;
		$timestamp =~ s/....$/(*)/;
	    }
	    push @response, sprintf ($format, $timestamp, 'Local', $alias, $blocked, $active, $unqualified_local_hostname);
	}
    }
    if (!defined($site) || $site eq 'remote') {
	if ($remote_state->{state_time}) {
	    foreach my $alias (@$aliases) {
		my $sync_time = $remote_state->{$abbrev}{ $name->{$alias} }{last_sync_time};
		my $blocked   = $remote_state->{$abbrev}{ $name->{$alias} }{blocked} ? 'blocked' : 'unblocked';
		my $active    = $remote_state->{$abbrev}{ $name->{$alias} }{active} || 'inactive';
		my $timestamp = $sync_time ? (scalar localtime $sync_time) : '(not yet attempted)';
		if ($sync_time && ($sync_time <= $remote_state->{start_time})) {
		    $have_old_remote_sync_time = 1;
		    $timestamp =~ s/....$/(*)/;
		}
		push @response, sprintf ($format, $timestamp, 'Remote', $alias, $blocked, $active, $unqualified_remote_hostname);
	    }
	    if ((time() - $remote_state->{state_time}) > ($config->{'contact-heartbeat-period'} * 3)) {
		push @response, $minor_line;
		push @response, 'NOTICE:  The remote state reported here is old and may now be wrong.';
	    }
	}
	else {
	    my $blocked   = 'unknown';
	    my $active    = 'unknown';
	    my $timestamp = '(not yet contacted)';
	    foreach my $alias (@$aliases) {
		push @response, sprintf ($format, $timestamp, 'Remote', $alias, $blocked, $active, $unqualified_remote_hostname);
	    }
	    my @message = fill_text (
		"Remote \"$unqualified_remote_hostname\" has not yet been contacted,",
		"so we don't know the state of its ${object}s."
	    );
	    $message[0] = 'NOTICE:  ' . $message[0];
	    push @response, $minor_line;
	    push @response, @message;
	}
    }
    if ($have_old_local_sync_time || $have_old_remote_sync_time) {
	# If a Last Replication Time predates our start time, we may have wiped out the replication-is-active
	# state that truly applied to that replication time, by setting it to "inactive" when we started up.
	my @message = fill_text (
	    'Last Replication Time values marked with "(*)" predate the start time of ',
	    ( ($have_old_local_sync_time && $have_old_remote_sync_time) ? 'the respective ' : 'the ' ),
	    'Replication Engine ',
	    '('
		.
		($have_old_local_sync_time ? ((scalar localtime $local_state->{start_time}) . ' for the Local system') : '')
		.
		(($have_old_local_sync_time && $have_old_remote_sync_time) ? ', or ' : '')
		.
		($have_old_remote_sync_time ? ((scalar localtime $remote_state->{start_time}) . ' for the Remote system') : '')
		.
	    '), ',
	    'and the respective objects may have an unreliable "Active?" status displayed.',
	);
	$message[0] = 'NOTICE:  ' . $message[0];
	push @response, $minor_line;
	push @response, @message;
    }

    return \@response;
}

#   status all [local|remote] [app|db]
sub display_all_status {
    my $site   = shift;  # 'local', 'remote', or undefined (both)
    my $abbrev = shift;  # 'app', 'db', or undefined (both)

    # FIX THIS:  fix the double blank lines between sections

    my @response = ();
    if (!defined($abbrev) || $abbrev eq 'app') {
	push @response, @{display_application_status ($site, 'all')};
    }
    if (!defined($abbrev) || $abbrev eq 'db') {
	push @response, @{display_database_status ($site, 'all')};
    }
    if (!defined($abbrev)) {
	# FIX LATER:  if and when we ever implement the "status engine" command, call it here
	# push @response, @{display_engine_status($site)};
	push @response, @{display_notification_status($site)};
	push @response, @{display_configuration_status($site)};
	push @response, @{display_heartbeat_status()};
    }
    return \@response;
}

#   status [local|remote] app <application-name> ...
sub display_application_status {
    my $site    = shift;  # 'local', 'remote', or undefined (both)
    my $aliases = shift;  # either string ('all') or arrayref
    return display_object_status ($site, 'application', 'app', \@app_names, \%app_name, $aliases);
}

#   status [local|remote] db <database-name> ...
sub display_database_status {
    my $site    = shift;  # 'local', 'remote', or undefined (both)
    my $aliases = shift;  # either string ('all') or arrayref
    return display_object_status ($site, 'database', 'db', \@db_names, \%db_name, $aliases);
}

#   status [local|remote] engine
sub display_engine_status {
    my $site = shift;  # 'local', 'remote', or undefined (both)
    my @response = ();
    # FIX THIS:  fully implement this command
    # FIX THIS:  the displayed engine state should include such things as:
    # whether we are currently in failure mode
    # has Notification Authority
    # state of Notification Authority Control
    # has Master Configuration Authority
    # the state of the last remote heartbeat, and the consecutive number of matching states
    # the configured mode-transition thresholds
    # perhaps various other numbers that get computed on each heartbeat
    # whether replication is globally enabled (from the config file; we don't have a command to dynamically control this)
    # summary of whether any sync operations are currently in progress
    if (!defined($site) || $site eq 'local') {
	push @response, $blank_line;
	push @response, "Local host: $local_heartbeat_host";
	push @response, @{ Replication::State->print_replication_state() };
    }
    if (!defined($site) || $site eq 'remote') {
	push @response, $blank_line;
	push @response, "Remote host:  $remote_heartbeat_host";
	push @response, '"status remote engine" is not yet implemented.';
    }
    push @response, $blank_line;
    push @response, '"status engine" is not yet implemented.';
    return \@response;
}

#   status [local|remote] notify
sub display_notification_status {
    my $site = shift;  # 'local', 'remote', or undefined (both)
    my @response = ();

    my $local_state  = fetch_local_state();
    my $remote_state = fetch_remote_state();
    my $title_line   = 'Notification Status';
    my $major_line   = '===========================================================================';
    my $minor_line   = '---------------------------------------------------------------------------';
    my $format       = '%-24s  %-6s  %-9s  %-8s  %s';

    push @response, $blank_line;
    push @response, $title_line;
    push @response, $major_line;
    push @response, sprintf ($format, 'Last Contact Time', 'Site', 'Authority', 'Control', 'Hostname');
    push @response, $minor_line;

    if (!defined($site) || $site eq 'local') {
	my $timestamp = scalar localtime;
	my $authority = $local_state->{has_notification_authority} ? 'yes' : 'no';
	my $control   = $local_state->{notification_authority_control} || 'unknown';
	push @response, sprintf ($format, $timestamp, 'Local', $authority, $control, $unqualified_local_hostname);
    }
    if (!defined($site) || $site eq 'remote') {
	if ($remote_state->{state_time}) {
	    my $timestamp = scalar localtime $remote_state->{state_time};
	    my $authority = $remote_state->{has_notification_authority} ? 'yes' : 'no';
	    my $control   = $remote_state->{notification_authority_control} || 'unknown';
	    push @response, sprintf ($format, $timestamp, 'Remote', $authority, $control, $unqualified_remote_hostname);
	    if ((time() - $remote_state->{state_time}) > ($config->{'contact-heartbeat-period'} * 3)) {
		push @response, $minor_line;
		push @response, 'NOTICE:  The remote state reported here is old and may now be wrong.';
	    }
	}
	else {
	    my $timestamp = '(not yet contacted)';
	    my $authority = 'unknown';
	    my $control   = 'unknown ';
	    push @response, sprintf ($format, $timestamp, 'Remote', $authority, $control, $unqualified_remote_hostname);
	    push @response, $minor_line;
	    push @response,
		"NOTICE:  Remote \"$unqualified_remote_hostname\" has not yet been contacted, so we don't",
		"know the state of its Notification Authority and Control settings.";
	}
    }
    # FIX THIS:  we need to be analyzing notification_authority_control rather than has_notification_authority here;
    # has_notification_authority is just a derivative field
    if ($remote_state->{state_time}) {
	my $comment = '';
	if ((time() - $remote_state->{state_time}) > ($config->{'contact-heartbeat-period'} * 3)) {
	    $comment = " (though the remote state shown here\nis stale and may be incorrect)";
	}
	if ($local_state->{has_notification_authority} && $remote_state->{has_notification_authority}) {
	    push @response, $minor_line;
	    push @response,
	      "NOTICE:  Apparently, both local \"$unqualified_local_hostname\" and remote \"$unqualified_remote_hostname\" have",
	      "Notification Authority$comment.  This setup does not violate any",
	      'rules, but may not be how you intend to run the systems.';
	}
	elsif (!$local_state->{has_notification_authority} && !$remote_state->{has_notification_authority}) {
	    push @response, $minor_line;
	    push @response,
	      "ERROR:  Apparently, neither local \"$unqualified_local_hostname\" nor remote \"$unqualified_remote_hostname\"",
	      "has Notification Authority$comment.  This is a potentially misleading",
	      'setup, which should be corrected as soon as possible.';
	}
    }

    return \@response;
}

#   status [local|remote] config
sub display_configuration_status {
    my $site = shift;  # 'local', 'remote', or undefined (both)
    my @response = ();

    my $local_state  = fetch_local_state();
    my $remote_state = fetch_remote_state();
    my $title_line   = 'Master Configuration Status';
    my $major_line   = '===========================================================================';
    my $minor_line   = '---------------------------------------------------------------------------';
    my $format       = '%-24s  %-6s  %-9s  %s';

    push @response, $blank_line;
    push @response, $title_line;
    push @response, $major_line;
    push @response, sprintf ($format, 'Last Contact Time', 'Site', 'Authority', 'Hostname');
    push @response, $minor_line;

    if (!defined($site) || $site eq 'local') {
	my $timestamp = scalar localtime;
	my $authority = $local_state->{has_master_configuration_authority} ? 'yes' : 'no';
	push @response, sprintf ($format, $timestamp, 'Local', $authority, $unqualified_local_hostname);
    }
    if (!defined($site) || $site eq 'remote') {
	if ($remote_state->{state_time}) {
	    my $timestamp = scalar localtime $remote_state->{state_time};
	    my $authority = $remote_state->{has_master_configuration_authority} ? 'yes' : 'no';
	    push @response, sprintf ($format, $timestamp, 'Remote', $authority, $unqualified_remote_hostname);
	    if ((time() - $remote_state->{state_time}) > ($config->{'contact-heartbeat-period'} * 3)) {
		push @response, $minor_line;
		push @response, 'NOTICE:  The remote state reported here is old and may now be wrong.';
	    }
	}
	else {
	    my $timestamp = '(not yet contacted)';
	    my $authority = 'unknown';
	    push @response, sprintf ($format, $timestamp, 'Remote', $authority, $unqualified_remote_hostname);
	    push @response, $minor_line;
	    push @response,
		"NOTICE:  Remote \"$unqualified_remote_hostname\" has not yet been contacted, so we don't",
		"know the state of its Master Configuration Authority setting.";
	}
    }
    if ($remote_state->{state_time}) {
	my $comment = '';
	if ((time() - $remote_state->{state_time}) > ($config->{'contact-heartbeat-period'} * 3)) {
	    $comment = " (though the remote state shown here\nis stale and may be incorrect)";
	}
	if ($local_state->{has_master_configuration_authority} && $remote_state->{has_master_configuration_authority}) {
	    push @response, $minor_line;
	    push @response,
	      "WARNING:  Apparently, both local \"$unqualified_local_hostname\" and remote \"$unqualified_remote_hostname\" have",
	      "Master Configuration Authority$comment.  This is a potentially dangerous",
	      'setup, which should be corrected as soon as possible.';
	}
	elsif (!$local_state->{has_master_configuration_authority} && !$remote_state->{has_master_configuration_authority}) {
	    push @response, $minor_line;
	    push @response,
	      "NOTICE:  Apparently, neither local \"$unqualified_local_hostname\" nor remote \"$unqualified_remote_hostname\"",
	      "has Master Configuration Authority$comment.  This setup does not violate",
	      'any rules, but may not be how you intend to run the systems.';
	}
    }

    return \@response;
}

#   status heartbeat
sub display_heartbeat_status {
    my @response = ();
    my $now = time();

    my $local_state  = fetch_local_state();
    my $remote_state = fetch_remote_state();
    my $last_remote_server_contact_time = $remote_state->{state_time} ? scalar( localtime $remote_state->{state_time} ) : '(not yet contacted)';
    my $last_remote_client_contact_time = $local_state->{server_time} ? scalar( localtime $local_state->{server_time} ) : '(not yet contacted)';

    my $local_to_remote = "$unqualified_local_hostname (client) -> $unqualified_remote_hostname (server)";
    my $remote_to_local = "$unqualified_remote_hostname (client) -> $unqualified_local_hostname (server)";

    my $length_local_to_remote = length $local_to_remote;
    my $length_remote_to_local = length $remote_to_local;

    my $direction_width = $length_local_to_remote;
    $direction_width = $length_remote_to_local if $length_remote_to_local > $direction_width;

    my $title_line   = 'Heartbeat Status';
    my $major_line   = '===========================================================================';
    my $minor_line   = '---------------------------------------------------------------------------';
    my $format       = "%-24s  %-${direction_width}s  %-6s";

    push @response, $blank_line;
    push @response, $title_line;
    push @response, $major_line;
    push @response, sprintf ($format, 'Last Contact Time', 'Direction', 'Status');
    push @response, $minor_line;

    # FIX MINOR:  print an appropriate last contact time for each direction (but I think we're doing that now)

    # We once saw our $connected_to_remote_heartbeat_server flag contain stale data that was never updated, while
    # showing that the connection was up long after no contact was being made in either direction.  That happened
    # when the remote side died badly, reporting an internal POE error.  Presumably it didn't disconnect cleanly.
    # But regardless of the cause, we need to validate application of the flag with additional checks on the
    # respective contact times.  Literally speaking, the $got_remote_state_timeout flag should only apply to one
    # direction (remote server status), as that is where it is set, but if we cannot contact the remote server,
    # then we cannot fetch its up-to-date client status either.  The calculation here depends on the system times
    # on the two machines being synchronized, as we are comparing local and remote timestamps.
    #
    # There is an asymmetry in how our cross-connections between mirrored clients and servers are handled by POE.  Only one
    # connection from a client to a server is needed for full bidirectional communication.  Once a channel is established
    # from one initiator to the other target, the two sides essentially become peers, and messages can be sent in either
    # direction.  In that situation, there is no reason for the other side to attempt to connect as a client to the other
    # side as a server.  If such a second connection is attempted, some part of it will be ignored because the remote kernel
    # name is already known (this fact will be discovered in the POE::Component::IKC::Responder->register() routine).
    #
    # Given that, in order to achieve reliable operation, each side must attempt to establish a connection, there is a race
    # condition in establishing an A-to-B and a B-to-A connection.  Ordinarily, if an A-to-B connection is established, B
    # won't have any motivation to attempt a B-to-A connection.  But if both sides initiate their attempts at the same time,
    # these operations may overlap and both connections might be in play.  Whatever error handling we have in play must be
    # able to recognize and deal with the presence of such multiple connections when it needs to understand that a failure
    # has occurred and it needs to back off and re-establish a connection.  We don't deal with that right here.
    #
    # The main point here is that the $connected_to_remote_heartbeat_server represents the fact that the remote kernel name
    # is known and that at least one connection is established.  It is the universal proxy for any type of active connection.
    # It says nothing about the original directionality of that connection.  But it is the proper flag to use for computing
    # both $remote_server_status and $remote_client_status.  However, the notion of pretending there is a directionality to
    # the connections, and displaying such a construct to the user, is now suspect.  It might be better to just display the
    # notion that a connection exists, and independently note whether local and/or remote status is stale.
    #
    # If the $connected_to_remote_heartbeat_server flag is true, the $connected_from_remote_heartbeat_client flag indicates
    # who initiated the connection (though this is not terribly critical or even interesting information).  If the "from"
    # flag is true, the remote side initiated the connection.  If the "from" flag is false, the local side initiated the
    # connection.  In theory, the "from" flag could be true on both ends in the cross-connection scenario outlined above,
    # though we have not tested to see if that condition can actually arise.  (It may be prohibited by the issue of the
    # Responder ignoring parts of the second connection, as noted above.)
    #
    my $server_status_is_stale = !defined($remote_state->{state_time}) ||
      ($now - $remote_state->{state_time}) > ($config->{'contact-heartbeat-period'} * 3);
    my $client_status_is_stale = !defined($local_state->{server_time}) ||
      ($now - $local_state->{server_time}) > ($config->{'contact-heartbeat-period'} * 3);
    my $remote_server_status = ($connected_to_remote_heartbeat_server && !$got_remote_state_timeout && !$server_status_is_stale) ? 'up' : 'down';
    my $remote_client_status = ($connected_to_remote_heartbeat_server && !$got_remote_state_timeout && !$client_status_is_stale) ? 'up' : 'down';
    push @response, sprintf ($format, $last_remote_server_contact_time, $local_to_remote, $remote_server_status);
    push @response, sprintf ($format, $last_remote_client_contact_time, $remote_to_local, $remote_client_status);

    # We purposely only test for stale timestamps, and not for the other conditions,
    # because we want to allow transient phenomena not to trigger this extra message.
    if ($server_status_is_stale || $client_status_is_stale) {
	push @response, $minor_line;
	push @response, 'NOTICE:  Heartbeats are not currently operating in both directions.';
    }

    return \@response;
}

#   notify grab
sub grab_notifications {
    my @response = ();
    push @response, $blank_line;

    my $local_state  = fetch_local_state();
    my $remote_state = fetch_remote_state();

    my $remote_state_age = defined($remote_state->{'state_time'}) ? (time() - $remote_state->{'state_time'}) : undef;
    my $remote_state_is_current = !$got_remote_state_timeout
	&& defined($remote_state_age)
	&& $remote_state_age < $config->{'contact-heartbeat-period'} * 3;

    my  $local_system_has_MCA =  $local_state->{has_master_configuration_authority};
    my $remote_system_has_MCA = $remote_state->{has_master_configuration_authority};

    # What we care about here are potential conflicts of control (notification_authority_control), not
    # of state (has_notification_authority).  See the documentation for a listing of the combinations
    # of local and remote control settings and their desirability and effects.
    if ($remote_state_is_current) {
	if ($remote_state->{notification_authority_control} eq 'grabbed') {
	    my @message = fill_text (
		"Local \"$unqualified_local_hostname\" is grabbing Notification ",
		"Authority Control while remote \"$unqualified_remote_hostname\" still has it; ",
		"this will result in duplicate notifications being sent out."
	    );
	    my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
	    push @response, @$response if @$response;
	}
	elsif ($remote_state->{notification_authority_control} eq 'dynamic') {
	    if ($local_system_has_MCA && $remote_system_has_MCA) {
		# Evidently the two sides each believe they are in charge.  This is ripe for problems.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now have locked-down Notification Authority. ",
		    "Because both this system and remote \"$unqualified_remote_hostname\" are currently ",
		    "assigned Master Configuration Authority (a serious misconfiguration!), ",
		    "this will result in duplicate notifications being sent out."
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
	    }
	    elsif ($local_system_has_MCA && !$remote_system_has_MCA) {
		# Here we are evidently the Primary system.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now have locked-down Notification Authority."
		);
		my $response = log_dispatch (LOG_LEVEL_INFO, SEVERITY_OK, @message);
		push @response, @$response if @$response;
	    }
	    elsif (!$local_system_has_MCA && $remote_system_has_MCA) {
		# Here we are evidently the DR system.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now have locked-down Notification Authority. ",
		    "This is workable while operating in failure mode, but will result in ",
		    "duplicate notifications being sent out when operating in normal mode."
		);
		my $response = log_dispatch (LOG_LEVEL_INFO, SEVERITY_OK, @message);
		push @response, @$response if @$response;
	    }
	    elsif (!$local_system_has_MCA && !$remote_system_has_MCA) {
		# Evidently nobody is in charge at this point.  This is not necessarily an error, because the site
		# could have disabled Master Configuration Authority on one system in preparation for enabling it
		# on the other system.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now have locked-down Notification ",
		    "Authority.  That will work for handling notifications, but neither ",
		    "system currently has Master Configuration Authority, and that might ",
		    "violate your current expectations."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    else {
		# If the chain of tests above is written correctly, this is
		# a logical impossibility and we should never get here.
	    }
	}
	elsif ($remote_state->{notification_authority_control} eq 'released') {
	    if ($local_system_has_MCA && $remote_system_has_MCA) {
		# Evidently the two sides each believe they are in charge.  This is ripe for problems.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now have locked-down Notification Authority. ",
		    "That will work for notifications, but be aware that both this system ",
		    "and remote \"$unqualified_remote_hostname\" are currently assigned Master ",
		    "Configuration Authority, and that is a serious misconfiguration!"
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
	    }
	    elsif ($local_system_has_MCA && !$remote_system_has_MCA) {
		# Here we are evidently the Primary system.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now have Notification ",
		    "Authority Control while remote \"$unqualified_remote_hostname\" has disavowed it. ",
		    "This is workable, but failover cannot occur with this setup."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    elsif (!$local_system_has_MCA && $remote_system_has_MCA) {
		# Here we are evidently the DR system.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now have Notification ",
		    "Authority Control while remote \"$unqualified_remote_hostname\" has disavowed it. ",
		    "This is workable, but failback cannot occur with this setup."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    elsif (!$local_system_has_MCA && !$remote_system_has_MCA) {
		# Evidently nobody is in charge at this point.  This is not necessarily an error, because the site
		# could have disabled Master Configuration Authority on one system in preparation for enabling it
		# on the other system.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now have locked-down Notification ",
		    "Authority.  That will work for handling notifications, but neither ",
		    "system currently has Master Configuration Authority, and that might ",
		    "violate your current expectations."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    else {
		# If the chain of tests above is written correctly, this is
		# a logical impossibility and we should never get here.
	    }
	}
	else {
	    # error condition; should never occur
	}
    }
    else {
	# We cannot compare this requested setting with the remote side, so we just accept it blindly.
	my @message = fill_text (
	    "Local \"$unqualified_local_hostname\" will now have locked-down Notification Authority."
	);
	my $response = log_dispatch (LOG_LEVEL_INFO, SEVERITY_OK, @message);
	push @response, @$response if @$response;
    }

    $local_state->{has_notification_authority} = true;
    $local_state->{notification_authority_control} = 'grabbed';
    store_local_state ($local_state);
    # FIX THIS:  implement proper error detection on the save
    save_replication_state ();

    my $error = $nagios->enable_notifications();
    push @response, $error if $error;

    return \@response;
}

#   notify dynamic
sub dynamic_notifications {
    my @response = ();
    push @response, $blank_line;

    my $local_state  = fetch_local_state();
    my $remote_state = fetch_remote_state();

    my $remote_state_age = defined($remote_state->{'state_time'}) ? (time() - $remote_state->{'state_time'}) : undef;
    my $remote_state_is_current = !$got_remote_state_timeout
	&& defined($remote_state_age)
	&& $remote_state_age < $config->{'contact-heartbeat-period'} * 3;

    my  $local_system_has_MCA =  $local_state->{has_master_configuration_authority};
    my $remote_system_has_MCA = $remote_state->{has_master_configuration_authority};

    # FIX LATER:  Drop this analysis and all but the final "else" message (since we just use whatever messages
    # analyze() emits, and don't try to provide any further advice).  If we actually had something extra
    # to say, we would fill in the branches here.

    # What we care about here are potential conflicts of control (notification_authority_control), not
    # of state (has_notification_authority).  See the documentation for a listing of the combinations
    # of local and remote control settings and their desirability and effects.
    # if ($remote_state_is_current) ...
    if (0) {
	if ($remote_state->{notification_authority_control} eq 'grabbed') {
	    if ($local_system_has_MCA && $remote_system_has_MCA) {
		# Evidently the two sides each believe they are in charge.  This is ripe for problems.
	    }
	    elsif ($local_system_has_MCA && !$remote_system_has_MCA) {
		# Here we are evidently the Primary system.
	    }
	    elsif (!$local_system_has_MCA && $remote_system_has_MCA) { 
		# Here we are evidently the DR system.
	    }
	    elsif (!$local_system_has_MCA && !$remote_system_has_MCA) {
		# Evidently nobody is in charge at this point.  This is not necessarily an error, because the site
		# could have disabled Master Configuration Authority on one system in preparation for enabling it
		# on the other system.
	    }
	    else {
		# If the chain of tests above is written correctly, this is
		# a logical impossibility and we should never get here.
	    }
	}
	elsif ($remote_state->{notification_authority_control} eq 'dynamic') {
	    if ($local_system_has_MCA && $remote_system_has_MCA) {
		# Evidently the two sides each believe they are in charge.  This is ripe for problems.
	    }
	    elsif ($local_system_has_MCA && !$remote_system_has_MCA) {
		# Here we are evidently the Primary system.
	    }
	    elsif (!$local_system_has_MCA && $remote_system_has_MCA) { 
		# Here we are evidently the DR system.
	    }
	    elsif (!$local_system_has_MCA && !$remote_system_has_MCA) {
		# Evidently nobody is in charge at this point.  This is not necessarily an error, because the site
		# could have disabled Master Configuration Authority on one system in preparation for enabling it
		# on the other system.
	    }
	    else {
		# If the chain of tests above is written correctly, this is
		# a logical impossibility and we should never get here.
	    }
	}
	elsif ($remote_state->{notification_authority_control} eq 'released') {
	    if ($local_system_has_MCA && $remote_system_has_MCA) {
		# Evidently the two sides each believe they are in charge.  This is ripe for problems.
	    }
	    elsif ($local_system_has_MCA && !$remote_system_has_MCA) {
		# Here we are evidently the Primary system.
	    }
	    elsif (!$local_system_has_MCA && $remote_system_has_MCA) { 
		# Here we are evidently the DR system.
	    }
	    elsif (!$local_system_has_MCA && !$remote_system_has_MCA) {
		# Evidently nobody is in charge at this point.  This is not necessarily an error, because the site
		# could have disabled Master Configuration Authority on one system in preparation for enabling it
		# on the other system.
	    }
	    else {
		# If the chain of tests above is written correctly, this is
		# a logical impossibility and we should never get here.
	    }
	}
	else {
	    # error condition; should never occur
	}
    }
    else {
	# We cannot compare this requested setting with the remote side, so we just accept it blindly.
	my @message = fill_text (
	    "Local \"$unqualified_local_hostname\" will now have dynamic Notification Authority."
	);
	my $response = log_dispatch (LOG_LEVEL_INFO, SEVERITY_OK, @message);
	push @response, @$response if @$response;
    }

    # We don't try to calculate and set the state of notification authority here, deferring that instead
    # to the complicated analyze() calculations invoked below.  If we were going to do that here, we would
    # take advantage of the fact that we decided whether we should be in failure mode all along, even if
    # we were not allowed to change the local system's Notification Authority setting, and do this:
    #     $local_state->{has_notification_authority} = $local_state->{in_failure_mode};
    # But the situation is hugely more complicated than that.  Rather than repeat all the branching logic
    # for that decision here, we just invoke the existing routine below, after we have saved the dynamic
    # control setting to make it available to that routine, to analyze the current local and remote states
    # and make any necessary changes to the current state.

    $local_state->{notification_authority_control} = 'dynamic';
    store_local_state ($local_state);
    # FIX THIS:  implement proper error detection on the save
    save_replication_state ();

    # This call gives us the full-bore analysis experience.
    my $response = $poe_kernel->call ("$unqualified_local_hostname-analyzer", 'analyze', true);
    push @response, @$response if @$response;

    return \@response;
}

#   notify release
sub release_notifications {
    my @response = ();
    push @response, $blank_line;

    my $local_state  = fetch_local_state();
    my $remote_state = fetch_remote_state();

    my $remote_state_age = defined($remote_state->{'state_time'}) ? (time() - $remote_state->{'state_time'}) : undef;
    my $remote_state_is_current = !$got_remote_state_timeout
	&& defined($remote_state_age)
	&& $remote_state_age < $config->{'contact-heartbeat-period'} * 3;

    my  $local_system_has_MCA =  $local_state->{has_master_configuration_authority};
    my $remote_system_has_MCA = $remote_state->{has_master_configuration_authority};

    # What we care about here are potential conflicts of control (notification_authority_control), not
    # of state (has_notification_authority).  See the documentation for a listing of the combinations
    # of local and remote control settings and their desirability and effects.
    if ($remote_state_is_current) {
	if ($remote_state->{notification_authority_control} eq 'grabbed') {
	    if ($local_system_has_MCA && $remote_system_has_MCA) { 
		# Evidently the two sides each believe they are in charge.  This is ripe for problems.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now disavow Notification ",
		    "Authority Control while remote \"$unqualified_remote_hostname\" has grabbed it. ",
		    "That will work for handling notifications, but be aware that both this ",
		    "system and remote \"$unqualified_remote_hostname\" are currently assigned ", 
		    "Master Configuration Authority, and that is a serious misconfiguration!"
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
	    }
	    elsif ($local_system_has_MCA && !$remote_system_has_MCA) {
		# Here we are evidently the Primary system.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now disavow Notification ",
		    "Authority Control while remote \"$unqualified_remote_hostname\" has grabbed it. ",
		    "This is workable, but failback cannot occur with this setup."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    elsif (!$local_system_has_MCA && $remote_system_has_MCA) {
		# Here we are evidently the DR system.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now disavow Notification ",
		    "Authority Control while remote \"$unqualified_remote_hostname\" has grabbed it. ",
		    "This is workable, but failover cannot occur with this setup."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    elsif (!$local_system_has_MCA && !$remote_system_has_MCA) {
		# Evidently nobody is in charge at this point.  This is not necessarily an error, because the site
		# could have disabled Master Configuration Authority on one system in preparation for enabling it
		# on the other system.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now disavow Notification ",
		    "Authority Control while remote \"$unqualified_remote_hostname\" has grabbed it. ",
		    "That will work for handling notifications, but neither system currently ",
		    "has Master Configuration Authority, and that might violate your current ",
		    "expectations."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    else {
		# If the chain of tests above is written correctly, this is
		# a logical impossibility and we should never get here.
	    }
	}
	elsif ($remote_state->{notification_authority_control} eq 'dynamic') {
	    if ($local_system_has_MCA && $remote_system_has_MCA) { 
		# Evidently the two sides each believe they are in charge.  This is ripe for problems.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now disavow Notification ",
		    "Authority Control while remote \"$unqualified_remote_hostname\" allows ",
		    "dynamic control of Notification Authority.  That will work for ",
		    "handling notifications, but be aware that both this system and ",
		    "remote \"$unqualified_remote_hostname\" are currently assigned Master ", 
		    "Configuration Authority, and that is a serious misconfiguration!"
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
	    }
	    elsif ($local_system_has_MCA && !$remote_system_has_MCA) {
		# Here we are evidently the Primary system.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now disavow Notification ",
		    "Authority Control while remote \"$unqualified_remote_hostname\" allows ",
		    "dynamic control of Notification Authority.  This is workable but is a ",
		    "strange configuration, having the DR system handle all notifications ",
		    "while the Primary system still has Master Configuration Authority."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    elsif (!$local_system_has_MCA && $remote_system_has_MCA) {
		# Here we are evidently the DR system.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now disavow Notification ",
		    "Authority Control while remote \"$unqualified_remote_hostname\" allows ",
		    "dynamic control of Notification Authority.  This is workable, but failover ",
		    "cannot occur with this setup."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    elsif (!$local_system_has_MCA && !$remote_system_has_MCA) {
		# Evidently nobody is in charge at this point.  This is not necessarily an error, because the site
		# could have disabled Master Configuration Authority on one system in preparation for enabling it
		# on the other system.
		my @message = fill_text (
		    "Local \"$unqualified_local_hostname\" will now disavow Notification ",
		    "Authority Control while remote \"$unqualified_remote_hostname\" allows ",
		    "dynamic control of Notification Authority.  That will work for handling ",
		    "notifications, but neither system currently has Master Configuration ",
		    "Authority, and that might violate your current expectations."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    else {
		# If the chain of tests above is written correctly, this is
		# a logical impossibility and we should never get here.
	    }
	}
	elsif ($remote_state->{notification_authority_control} eq 'released') {
	    my @message = fill_text (
		"Local \"$unqualified_local_hostname\" will now disavow taking on Notification ",
		"Authority.  Remote \"$unqualified_remote_hostname\" has previously disavowed taking on ",
		"Notification Authority, so no notifications will ever be generated."
	    );
	    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
	    push @response, @$response if @$response;
	}
	else {
	    # error condition; should never occur
	}
    }
    else {
	# We cannot compare this requested setting with the remote side, so we just accept it blindly.
	my @message = fill_text (
	    "Local \"$unqualified_local_hostname\" will now disavow taking on Notification ",
	    "Authority.  If remote \"$unqualified_remote_hostname\" is not configured to allow it to ",
	    "assume Notification Authority, no notifications will ever be generated."
	);
	my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
	push @response, @$response if @$response;
    }

    $local_state->{has_notification_authority} = false;
    $local_state->{notification_authority_control} = 'released';
    store_local_state ($local_state);
    # FIX THIS:  implement proper error detection on the save
    save_replication_state ();

    my $error = $nagios->disable_notifications();
    push @response, $error if $error;

    return \@response;
}

# Note:  Grab and release operations on Master Configuration Authority will proceed
# whether or not the desired state is supposedly already in effect, simply so the
# external state file is forcibly updated to ensure our internal and external notions
# of the state are synchronized.  This is in contrast to an ideology which might have
# us skip the operation if the desired state is already true.  Also, we need to check
# both local and remote states, insofar as we can know the remote state, to ensure we
# are not improperly setting up a conflict of authority.

#   config grab
sub grab_configuration {
    my $forced = shift;  # 'forced' or undefined
    my @response = ();
    push @response, $blank_line;

    my $will_grab_MCA = false;
    my $remote_state  = fetch_remote_state();
    if ($remote_state->{has_master_configuration_authority}) {
	if ($forced) {
	    push @response,
		"NOTICE:  Local \"$unqualified_local_hostname\" is grabbing Master Configuration ",
		"Authority while remote \"$unqualified_remote_hostname\" still has it.";
	    log_timed_message
		"NOTICE:  Local \"$unqualified_local_hostname\" is grabbing Master Configuration ",
		"Authority while remote \"$unqualified_remote_hostname\" still has it.";
	    my $errors = $foundation->send_message(SEVERITY_CRITICAL, REPLICATION_ENGINE,
		"Local \"$unqualified_local_hostname\" is grabbing Master Configuration Authority while remote \"$unqualified_remote_hostname\" still has it."
	    );
	    push @response,   @$errors if $errors;
	    log_timed_message @$errors if $errors;
	    $will_grab_MCA = true;
	}
	else {
	    push @response,
		"ERROR:  Request denied for local \"$unqualified_local_hostname\" to grab Master ",
		"Configuration Authority while remote \"$unqualified_remote_hostname\" still has it.";
	    log_timed_message
		"ERROR:  Request denied for local \"$unqualified_local_hostname\" to grab Master ",
		"Configuration Authority while remote \"$unqualified_remote_hostname\" still has it.";
	    my $errors = $foundation->send_message(SEVERITY_WARNING, REPLICATION_ENGINE,
		"Request denied for local \"$unqualified_local_hostname\" to grab Master Configuration Authority while remote \"$unqualified_remote_hostname\" still has it."
	    );
	    push @response,   @$errors if $errors;
	    log_timed_message @$errors if $errors;
	}
    }
    else {
	push @response,   "INFO:  Local \"$unqualified_local_hostname\" will now have Master Configuration Authority.";
	log_timed_message "INFO:  Local \"$unqualified_local_hostname\" will now have Master Configuration Authority.";
	my $errors = $foundation->send_message(SEVERITY_OK, REPLICATION_ENGINE,
	    "Local \"$unqualified_local_hostname\" will now have Master Configuration Authority."
	);
	push @response,   @$errors if $errors;
	log_timed_message @$errors if $errors;
	$will_grab_MCA = true;
    }

    if ($will_grab_MCA) {
	my $local_state = fetch_local_state();
	$local_state->{has_master_configuration_authority} = true;
	store_local_state ($local_state);
	# FIX THIS:  implement proper error detection on the save
	save_replication_state ();
    }

    # Note:  We don't notify Foundation to go re-read our state file if may
    # have changed, as the current design just calls for it to look at the
    # local configuration when a user logs in to the GroundWork Monitor system.

    return \@response;
}

#   config release
sub release_configuration {
    my @response = ();
    push @response, $blank_line;

    my $remote_state = fetch_remote_state();
    if ($remote_state->{has_master_configuration_authority}) {
	push @response,
	    "INFO:  Local \"$unqualified_local_hostname\" is releasing Master Configuration Authority; ",
	    "remote \"$unqualified_remote_hostname\" still has it.";
	log_timed_message
	    "INFO:  Local \"$unqualified_local_hostname\" is releasing Master Configuration Authority; ",
	    "remote \"$unqualified_remote_hostname\" still has it.";
	my $errors = $foundation->send_message(SEVERITY_OK, REPLICATION_ENGINE,
	    "Local \"$unqualified_local_hostname\" is releasing Master Configuration Authority; remote \"$unqualified_remote_hostname\" still has it."
	);
	push @response,   @$errors if $errors;
	log_timed_message @$errors if $errors;
    }
    else {
	push @response,
	    "NOTICE:  Neither local \"$unqualified_local_hostname\" nor remote \"$unqualified_remote_hostname\" ",
	    "has Master Configuration Authority.";
	log_timed_message
	    "NOTICE:  Neither local \"$unqualified_local_hostname\" nor remote \"$unqualified_remote_hostname\" ",
	    "has Master Configuration Authority.";
	my $errors = $foundation->send_message(SEVERITY_WARNING, REPLICATION_ENGINE,
	    "Neither local \"$unqualified_local_hostname\" nor remote \"$unqualified_remote_hostname\" has Master Configuration Authority."
	);
	push @response,   @$errors if $errors;
	log_timed_message @$errors if $errors;
    }

    my $local_state  = fetch_local_state();
    $local_state->{has_master_configuration_authority} = false;
    store_local_state ($local_state);
    # FIX THIS:  implement proper error detection on the save
    save_replication_state ();

    return \@response;
}

# FIX THIS:  It's not clear whether we will want to expose the pulse command to the
# user.  If we do so, we will need to collect whatever errors otherwise just get posted
# to the log file by the internal heartbeat routines which are not ordinarily expecting
# to run in an interactive context, and ensure that they also get returned to the user.
#   pulse    # forcibly run a full heartbeat cycle if one is not in progress
sub pulse_heartbeat {
    my @response = ();
    # FIX THIS:  implement this command
    push @response, $blank_line;
    push @response, '"pulse" is not yet implemented.';
    return \@response;
}

# control_blocking ('block',   'application', 'app', \@app_names, \%app_name, 'all')
# control_blocking ('block',   'application', 'app', \@app_names, \%app_name, [ alias, ... ])
# control_blocking ('block',   'database',    'db',  \@db_names,  \%db_name,  'all')
# control_blocking ('block',   'database',    'db',  \@db_names,  \%db_name,  [ alias, ... ])
# control_blocking ('unblock', 'application', 'app', \@app_names, \%app_name, 'all')
# control_blocking ('unblock', 'application', 'app', \@app_names, \%app_name, [ alias, ... ])
# control_blocking ('unblock', 'database',    'db',  \@db_names,  \%db_name,  'all')
# control_blocking ('unblock', 'database',    'db',  \@db_names,  \%db_name,  [ alias, ... ])

sub control_blocking {
    my $operation = shift;  # 'block' or 'unblock'
    my $object    = shift;  # 'application' or 'database'
    my $abbrev    = shift;  # 'app' or 'db'
    my $names     = shift;  # arrayref for configured object names
    my $name      = shift;  # hashref for alias-to-name lookups
    my $aliases   = shift;  # either string ('all') or arrayref

    if (!@$names) {
	return [ '', "ERROR:  No ${object}s are configured." ];
    }
    if (not ref $aliases) {
	$aliases = $names;
    }

    my @response = ();
    foreach my $alias (@$aliases) {
	push @response, "ERROR:  \"$alias\" is not a known alias for any $object." if not exists $name->{$alias};
    }
    if (@response) {
	unshift @response, $blank_line;
	return \@response;
    }

    my @objects = ();
    my $local_state = fetch_local_state();
    foreach my $alias (@$aliases) {
	$local_state->{$abbrev}{ $name->{$alias} }{blocked} = ($operation eq 'block') ? true : false;
	if ($operation eq 'unblock' && $local_state->{$abbrev}{ $name->{$alias} }{active} eq 'stalled') {
	    ## We used to drop the stalled state and make it inactive here.  But that just loses perhaps
	    ## important information about the last replication cycle, without any real benefit I can see.
	    ## Having a stalled state doesn't prevent any future action, so there's no reason to leave that
	    ## state around until the next replication cycle begins.
	    # delete $local_state->{$abbrev}{ $name->{$alias} }{active};
	}
	push @objects, $name->{$alias};
    }
    store_local_state ($local_state);
    # FIX THIS:  implement proper error detection on the save
    save_replication_state ();
    push @response, $blank_line;
    foreach my $alias (@$aliases) {
	push @response,   "\u${operation}ed ${object}:  $alias";
	log_timed_message "\u${operation}ed ${object}:  $alias";
    }
    my $errors = $foundation->send_message(SEVERITY_OK, REPLICATION_ENGINE,
	"Replication has been ${operation}ed for ${object}s:  " . join(', ', @objects)
    );
    push @response,   @$errors if $errors;
    log_timed_message @$errors if $errors;
    return \@response;
}

#   block all [app|db]
sub block_all_syncs {
    my $abbrev   = shift;  # 'app', 'db', or undefined (both)
    my @response = ();
    push @response, @{ control_blocking ('block', 'application', 'app', \@app_names, \%app_name, 'all') } if (!defined($abbrev) || $abbrev eq 'app');
    push @response, @{ control_blocking ('block', 'database',    'db',  \@db_names,  \%db_name,  'all') } if (!defined($abbrev) || $abbrev eq 'db' );
    return \@response;
}

#   block app <application-name> ...
sub block_application_syncs {
    my $aliases = shift;  # arrayref
    return control_blocking ('block', 'application', 'app', \@app_names, \%app_name, $aliases);
}

#   block db <database-name> ...
sub block_database_syncs {
    my $aliases = shift;  # arrayref
    return control_blocking ('block', 'database', 'db', \@db_names, \%db_name, $aliases);
}

#   unblock all [app|db]
sub unblock_all_syncs {
    my $abbrev   = shift;  # 'app', 'db', or undefined (both)
    my @response = ();
    push @response, @{ control_blocking ('unblock', 'application', 'app', \@app_names, \%app_name, 'all') } if (!defined($abbrev) || $abbrev eq 'app');
    push @response, @{ control_blocking ('unblock', 'database',    'db',  \@db_names,  \%db_name,  'all') } if (!defined($abbrev) || $abbrev eq 'db' );
    return \@response;
}

#   unblock app <application-name> ...
sub unblock_application_syncs {
    my $aliases = shift;  # arrayref
    return control_blocking ('unblock', 'application', 'app', \@app_names, \%app_name, $aliases);
}

#   unblock db <database-name> ...
sub unblock_database_syncs {
    my $aliases = shift;  # arrayref
    return control_blocking ('unblock', 'database', 'db', \@db_names, \%db_name, $aliases);
}

#   diff all [app|db]
sub display_all_diffs {
    my @response = ();
    push @response, @{display_application_diffs('all')};
    push @response, @{display_database_diffs('all')};
    return \@response;
}

#   diff app <application-name> ...
sub display_application_diffs {
    my $applications = shift;  # either string ('all') or arrayref
    if (not ref $applications) {
	$applications = \@app_names;
    }
    my @response = ();
    push @response, $blank_line;
    foreach my $app (@$applications) {
	# FIX THIS:  implement this command
	push @response, "\"diff app $app\" is not yet implemented.";
    }
    return \@response;
}

#   diff db <database-name> ...
sub display_database_diffs {
    my $databases = shift;  # either string ('all') or arrayref
    if (not ref $databases) {
	$databases = \@db_names;
    }
    my @response = ();
    push @response, $blank_line;
    foreach my $db (@$databases) {
	# FIX THIS:  implement this command
	push @response, "\"diff db $db\" is not yet implemented.";
    }
    return \@response;
}

# schedule_syncs ('application', 'app', \@app_names, \%app_name, 'all')
# schedule_syncs ('application', 'app', \@app_names, \%app_name, [ alias, ... ])
# schedule_syncs ('database',    'db',  \@db_names,  \%db_name,  'all')
# schedule_syncs ('database',    'db',  \@db_names,  \%db_name,  [ alias, ... ])

sub schedule_syncs {
    my $object    = shift;  # 'application' or 'database'
    my $abbrev    = shift;  # 'app' or 'db'
    my $names     = shift;  # arrayref for configured object names
    my $name      = shift;  # hashref for alias-to-name lookups
    my $aliases   = shift;  # either string ('all') or arrayref

    if (not $initialized{$object}) {
	return [
	    '',
	    "ERROR:  \u${object}s are not yet initialized to allow replication cycles.",
	    "        Wait a full $config->{'contact-heartbeat-period'} seconds and try again."
	];
    }

    if (!@$names) {
	return [ '', "ERROR:  No ${object}s are configured." ];
    }
    if (not ref $aliases) {
	$aliases = $names;
    }

    my @response = ();
    foreach my $alias (@$aliases) {
	push @response, "ERROR:  \"$alias\" is not a known alias for any $object." if not exists $name->{$alias};
    }
    if (@response) {
	unshift @response, $blank_line;
	return \@response;
    }

    my @objects = ();
    my $action  = 'obtain';
    push @response, $blank_line;
    foreach my $alias (@$aliases) {
	my $obj_state_name = object_state_name($object, $name->{$alias});
	log_timed_message "STATS:  Scheduling $obj_state_name for immediate execution at ", scalar localtime if $DEBUG_STATS;
	my $response = $poe_kernel->call ("$unqualified_local_hostname-${object}s", $obj_state_name,
	  { object => $object, abbrev => $abbrev, name => $name->{$alias}, action => $action, });
	push @response, @$response if @$response;
	# FIX MAJOR:  log the response as well?
	# FIX MAJOR:  only do this push if the response was successful?
	push @objects, $name->{$alias};
    }

    my $errors = $foundation->send_message(SEVERITY_OK, REPLICATION_ENGINE,
	"Replication has been scheduled for ${object}s:  " . join(', ', @objects)
    );
    push @response,   @$errors if $errors;
    log_timed_message @$errors if $errors;
    return \@response;
}

#   sync all [app|db]
sub synchronize_all {
    my $abbrev   = shift;  # 'app', 'db', or undefined (both)
    my @response = ();
    push @response, @{ schedule_syncs ('application', 'app', \@app_names, \%app_name, 'all') } if (!defined($abbrev) || $abbrev eq 'app');
    push @response, @{ schedule_syncs ('database',    'db',  \@db_names,  \%db_name,  'all') } if (!defined($abbrev) || $abbrev eq 'db' );
    return \@response;
}

#   sync app <application-name> ...
sub synchronize_applications {
    my $aliases = shift;  # arrayref
    return schedule_syncs ('application', 'app', \@app_names, \%app_name, $aliases);
}

#   sync db <database-name> ...
sub synchronize_databases {
    my $aliases = shift;  # arrayref
    return schedule_syncs ('database', 'db', \@db_names, \%db_name, $aliases);
}

#   commit app <application-name>
sub commit_application {
    my $alias = shift;
    if (not exists $app_name{$alias}) {
	return [ '', "ERROR:  \"$alias\" is not a known application alias." ];
    }
    my $application = $app_name{$alias};
    my @response = ();
    # FIX THIS:  implement this command
    push @response, $blank_line;
    push @response, "\"commit app $application\" is not yet implemented.";
    return \@response;
}

#   commit db <database-name>
sub commit_database {
    my $alias = shift;
    if (not exists $db_name{$alias}) {
	return [ '', "ERROR:  \"$alias\" is not a known database alias." ];
    }
    my $database = $db_name{$alias};
    my @response = ();
    # FIX THIS:  implement this command
    push @response, $blank_line;
    push @response, "\"commit db $database\" is not yet implemented.";
    return \@response;
}

#   alias all [app|db]
sub display_all_aliases {
    my $abbrev   = shift;  # 'app', 'db', or undefined (both)
    my @response = ();

    my $max_object_length = 0;
    my $max_name_length   = 0;
    my $max_alias_length  = 0;
    if (!defined($abbrev) || $abbrev eq 'app') {
	if ($max_object_length < length 'application') {
	    $max_object_length = length 'application';
	}
	while (my ($alias, $name) = each %app_name) {
	    if ($max_alias_length < length $alias) {
		$max_alias_length = length $alias;
	    }
	    if ($max_name_length < length $name) {
		$max_name_length = length $name;
	    }
	}
    }
    if (!defined($abbrev) || $abbrev eq 'db' ) {
	if ($max_object_length < length 'database') {
	    $max_object_length = length 'database';
	}
	while (my ($alias, $name) = each %db_name) {
	    if ($max_alias_length < length $alias) {
		$max_alias_length = length $alias;
	    }
	    if ($max_name_length < length $name) {
		$max_name_length = length $name;
	    }
	}
    }

    my $title_line   = "Available Aliases";
    my $major_line   = '===================================================================';
    my $minor_line   = '-------------------------------------------------------------------';
    my $format       = "%-${max_object_length}s  %-${max_name_length}s  %-${max_alias_length}s";

    push @response, $blank_line;
    push @response, $title_line;
    push @response, $major_line;
    push @response, sprintf ($format, 'Object', 'Name', 'Alias');
    push @response, $minor_line;

    my @aliases = ();

    if (!defined($abbrev) || $abbrev eq 'app') {
	while (my ($alias, $name) = each %app_name) {
	    push @aliases, sprintf ($format, 'application', $name, $alias);
	}
    }
    if (!defined($abbrev) || $abbrev eq 'db' ) {
	while (my ($alias, $name) = each %db_name) {
	    push @aliases, sprintf ($format, 'database', $name, $alias);
	}
    }
    push @response, sort @aliases;
    return \@response;
}

sub display_object_aliases {
    my $object   = shift;
    my $obj_name = shift;
    my $alias    = shift;
    if (not exists $obj_name->{$alias}) {
	return [ '', "ERROR:  \"$alias\" is not a known $object alias." ];
    }
    my $name = $obj_name->{$alias};
    my @response = ();
    my @aliases = ();
    if (keys %$obj_name) {
	my $major_line = '================================================';
	push @response, $blank_line;
	push @response, "Aliases for $object $name:";
	push @response, $major_line;
	foreach my $alias (sort keys %$obj_name) {
	    push @aliases, $alias if $obj_name->{$alias} eq $name;
	}
	push @response, sort @aliases;
    }
    return \@response;
}

#   alias app <application-name>       # lists available aliases for this app
sub display_application_aliases {
    my $alias = shift;
    return display_object_aliases('application', \%app_name, $alias);
}

#   alias db <database-name>   # lists available aliases for this db
sub display_database_aliases {
    my $alias = shift;
    return display_object_aliases('database', \%db_name, $alias);
}

sub list_object {
    my $object = shift;
    my $abbrev = shift;
    my $name   = shift;
    my @response = ();

    my $title_line   = "Available Backups for \u$object $name";
    my $major_line   = '===================================================================';
    my $minor_line   = '-------------------------------------------------------------------';
    my $format       = "%-24s";

    # find available backups
    my $backup_directory = "$backups_config_base_dir/$abbrev/$name";
    if (! -d $backup_directory) {
	return [ '', "NOTICE:  No backups are available for $object \"$name\"." ];
    }
    if (not opendir(DIRECTORY, $backup_directory)) {
	return [ '', "ERROR:  Cannot read $backup_directory", "        ($!)" ];
    }

    my @directories = grep { /^2[[:digit:]][[:digit:]][[:digit:]]-[[:digit:]][[:digit:]]-[[:digit:]][[:digit:]]\.[[:digit:]][[:digit:]]_[[:digit:]][[:digit:]]_[[:digit:]][[:digit:]]\z/ } readdir DIRECTORY;
    closedir DIRECTORY;
    if (@directories) {
	push @response, $blank_line;
	push @response, $title_line;
	push @response, $major_line;
	push @response, sprintf ($format, 'Backup Time');
	push @response, $minor_line;

	foreach my $directory (sort @directories) {
	    push @response, sprintf ($format, $directory);
	}

	my $count = @directories;
	push @response, $minor_line;
	push @response, ($count == 1 ? 'This 1 backup is' : "These $count backups are") . ' to be found in:';
	push @response, $backup_directory . '/';
    }
    else {
	return [ '', "NOTICE:  No backups are available for $object \"$name\"." ];
    }

    return \@response;
}

#   list app <application-name>       # lists available backup timestamps for this app
sub list_application {
    my $alias = shift;
    if (not exists $app_name{$alias}) {
	return [ '', "ERROR:  \"$alias\" is not a known application alias." ];
    }
    my $application = $app_name{$alias};
    return list_object('application', 'app', $application);
}

#   list db <database-name>   # lists available backup timestamps for this db
sub list_database {
    my $alias = shift;
    if (not exists $db_name{$alias}) {
	return [ '', "ERROR:  \"$alias\" is not a known database alias." ];
    }
    my $database = $db_name{$alias};
    return list_object('database', 'db', $database);
}

#   rollback app <application-name> [timestamp]
sub rollback_application {
    my $alias     = shift;
    my $timestamp = shift;  # might be undefined
    if (not exists $app_name{$alias}) {
	return [ '', "ERROR:  \"$alias\" is not a known application alias." ];
    }
    my $application = $app_name{$alias};
    my @response = ();
    # FIX THIS:  implement this command
    push @response, $blank_line;
    push @response, "\"rollback app $application\" is not yet implemented.";
    return \@response;
}

#   rollback db <database-name> [timestamp]
sub rollback_database {
    my $alias     = shift;
    my $timestamp = shift;  # might be undefined
    if (not exists $db_name{$alias}) {
	return [ '', "ERROR:  \"$alias\" is not a known database alias." ];
    }
    my $database = $db_name{$alias};
    my @response = ();
    # FIX THIS:  implement this command
    push @response, $blank_line;
    push @response, "\"rollback db $database\" is not yet implemented.";
    return \@response;
}

sub execute_heartbeat {
    # FIX THIS
    # The command is in $_[ARG0] and should be parsed and executed.
    print "got heartbeat: $_[ARG0]\n";

    # See earlier in this script for why we need to prepend a leading space to the command.
    # Hopefully, we'll be able to work around that in some future version.
    my $lines = $heartbeat_parser->Heartbeat(' ' . $_[ARG0]);
    if (defined $lines) {
	return join("\n",@$lines);
    }
    else {
	return "\"$_[ARG0]\" is not a recognized heartbeat.";
    }
}

#   thub Word(s?)
sub run_heartbeat {
    my $args = $_[0];
    return [ '', "\"$args->[0]\" is not a recognized heartbeat." ];
}

# Run one pulse of the heartbeat.
sub heart_throb {
    log_timed_message "STATS:  Heart throb." if $DEBUG_STATS;

    # If we don't get heartbeats in a very long time, perhaps the connection is bad.  We could put up with that
    # by essentially ignoring it, because generally our recovery mechanisms should kick in and bring everything
    # back into a working state when the remote system is once again accessible.  But during that period, a lot
    # of heartbeat attempts, and potentially other remote-action requests, will all be queued up waiting for
    # communication to go smoothly again.  Rather than let requests pile up indefinitely if the connection is
    # obviously down for a very long time, we turn off our flag that says the connection is available.  That
    # will force a reconnection attempt, thereby cleaning up whatever broken coherence might exist, and stop
    # such queueing until the connection is re-established.  Getting the connection re-established might be
    # somewhat of a messy process, as it might be connected and disconnected a few times due to race conditions,
    # but it should finally stabilize after only a few cycles.
    if ($connected_to_remote_heartbeat_server && ($max_antiquated_heartbeats || $max_archaic_heartbeats || $max_ancient_heartbeats)) {
	my $now = time();
	my $remote_state = fetch_remote_state();

	# $last_connect_is_recent is used to prevent dropping a new connection again, right away,
	# before we've had a chance to use it to obtain updated remote state.
	my $last_connect_is_recent = ($now - $last_remote_heartbeat_server_connect_time) <= ($config->{'contact-heartbeat-period'} * 3);
	my $remote_state_age       = defined($remote_state->{'state_time'}) ? ($now - $remote_state->{'state_time'}) : undef;

	my $server_status_is_antiquated = $max_antiquated_heartbeats && !$last_connect_is_recent &&
	  defined($remote_state_age) && $remote_state_age > ($config->{'contact-heartbeat-period'} * $max_antiquated_heartbeats);
	my $server_status_is_archaic = $max_archaic_heartbeats && !$last_connect_is_recent &&
	  defined($remote_state_age) && $remote_state_age > ($config->{'contact-heartbeat-period'} * $max_archaic_heartbeats);
	my $server_status_is_ancient = $max_ancient_heartbeats && !$last_connect_is_recent &&
	  defined($remote_state_age) && $remote_state_age > ($config->{'contact-heartbeat-period'} * $max_ancient_heartbeats);

	# First-level sanity check.
	if ($server_status_is_antiquated && not kernel_is_known("$unqualified_remote_hostname-heartbeat-server")) {
	    log_timed_message "NOTICE:  Remote kernel $unqualified_remote_hostname-heartbeat-server is not known; will reconnect."
	      if $DEBUG_NOTICE;
	    # Force retrying the connection.
	    log_timed_message "FERVID:  found antiquated remote state; setting connected_to_remote_heartbeat_server = 0"
	      if $DEBUG_FERVID;
	    $connected_to_remote_heartbeat_server = 0;
	}
	# Second-level sanity check.
	elsif ($server_status_is_archaic && not kernel_is_connected("$unqualified_remote_hostname-heartbeat-server")) {
	    log_timed_message "NOTICE:  Remote kernel $unqualified_remote_hostname-heartbeat-server is not connected; will reconnect."
	      if $DEBUG_NOTICE;
	    # Force retrying the connection.
	    log_timed_message "FERVID:  found archaic remote state; setting connected_to_remote_heartbeat_server = 0"
	      if $DEBUG_FERVID;
	    $connected_to_remote_heartbeat_server = 0;
	}
	# Third-level sanity check.
	elsif ($server_status_is_ancient) {
	    log_timed_message "NOTICE:  Remote state is ancient; will reconnect."
	      if $DEBUG_NOTICE;
	    # Force retrying the connection.
	    log_timed_message "FERVID:  found ancient remote state; setting connected_to_remote_heartbeat_server = 0"
	      if $DEBUG_FERVID;
	    $connected_to_remote_heartbeat_server = 0;
	}

	if (not $connected_to_remote_heartbeat_server) {
	    log_all_kernels();
	}
    }

    # FIX MINOR:  should we test $subscribed_to_remote_heartbeat_server here instead?
    if ($connected_to_remote_heartbeat_server) {
	# We impose a deadline on the remote request, because we are about to queue up the request to be sent to the
	# remote system using a mechanism that will not allow us to cancel it even if we can see here on the local
	# side that the request has timed out and the response has not been received in a timely fashion.  Note that,
	# if the link to the remote system is temporarily down, that fact might not be noted in the value of the
	# $connected_to_remote_heartbeat_server flag.  In that case, the request will still be generated and will
	# remain queued for some time, and if the link comes back up before the virtual connection is dropped, then
	# all the queued messages will be sent at that time.  This could result in a burst of redundant, stale
	# requests being seen on the remote side.  Passing a deadline along with the request allows the remote
	# system to immediately know which requests are stale and should be dropped on the floor without any
	# additional processing.  The deadline is set here on the local system but will be interpreted on the
	# remote system, which is part of why the clocks on the local and remote systems must be synchronized.
	my $deadline = time() + $config->{'contact-heartbeat-period'} / 2;
	$last_ticket{heartbeat} = next_ticket();
	# Run a message exchange with the remote server.  For simplicity, just ask it its own full status,
	# so we can record that here.
	$_[KERNEL]->post('IKC', 'post',
	  "poe://$unqualified_remote_hostname-heartbeat-server/$unqualified_remote_hostname-heartbeat/report_local_state",
	  {
	  deadline => $deadline,
	  ticket   => $last_ticket{heartbeat},
	  callback => "poe://$unqualified_local_hostname-heartbeat-server/$unqualified_local_hostname-heartbeat/receive_remote_state"
	  });
	# Set a timeout for receipt of the remote state, so we will know when we failed to get what we asked for.
	$_[KERNEL]->delay ('remote_state_receipt_timeout', $config->{'contact-heartbeat-period'} / 2);
    }
    else {
	delete $last_ticket{heartbeat};
	log_timed_message "NOTICE:  Attempting to connect to \"$unqualified_remote_hostname\" heartbeat." if $DEBUG_NOTICE;
	# Instead of instigating a heartbeat, we must reconnect to the opposing server, since we're not currently connected.
	connect_to_heartbeat_server();
    }
    $_[KERNEL]->delay ('throb', $config->{'contact-heartbeat-period'});
}

sub return_local_state {
    my $args          = $_[ARG0];
    my $nagios_status = $_[ARG1];

    my $object   = $args->{object};   # 'application' or 'database' ('heartbeat')
    my $abbrev   = $args->{abbrev};   # 'app' or 'db' ('pulse')
    my $name     = $args->{name};     # app or db name ('nagios')
    my $action   = $args->{action};   # action ('check')
    my $ticket   = $args->{ticket};   # ticket for current-request verification
    my $callback = $args->{payload};  # callback event, to be posted to after all state has been gathered

    my $local_state = fetch_local_state();

    # FIX THIS:  put this kind of stuff as well into the local state?
    # $local_state->{'local_server'}  = $unqualified_local_hostname;
    # $local_state->{'remote_server'} = $unqualified_remote_hostname;

    # ps will return an exit status of 0 if it finds at least one matching process, 1 if not.
    $local_state->{monitoring_is_up} = ($nagios_status == 0) ? true : false;

    $local_state->{server_time} = $local_state->{'state_time'} = time;
    $local_state->{remote_heartbeat_ticket} = $ticket;
    # Save the reported state so the data we just updated matches on both sides of the connection.
    store_local_state ($local_state);
    # FIX THIS:  do we need to save the updated local state as well?

    if ($callback) {
	# Send the final results back to the guy that called us.
	$_[KERNEL]->post('IKC', 'post', $callback, $local_state);
    }
    else {
	log_timed_message "NOTICE:  Final result from incoming remote call for $object \"$name\" $action is being discarded." if $DEBUG_NOTICE;
    }
}

sub nagios_check_callback {
    my $args     = $_[ARG0];
    my $status   = $_[ARG1];
    my $messages = $_[ARG2];

    my $object  = $args->{object};   # 'application' or 'database' ('heartbeat')
    my $abbrev  = $args->{abbrev};   # 'app' or 'db' ('pulse')
    my $name    = $args->{name};     # app or db name ('nagios')
    my $action  = $args->{action};   # action ('check')
    my $ticket  = $args->{ticket};   # ticket for current-request verification
    my $payload = $args->{payload};  # opaque payload, in this case the external callback

#    my $function = (caller(0))[3];
#    log_timed_message "FERVID:  $function() has been called for \"$name\" $object $action." if $DEBUG_FERVID;

#    log_timed_message
#      "INFO:  Program for \"$name\" $object $action returned wait status $status [", wait_status_message($status), "] and ",
#      @$messages ? ((scalar @$messages).((@$messages == 1) ? ' message' : ' messages').' (#):') : 'no messages.' if $DEBUG_INFO;
#    log_message "# ", join ("\n# ", @$messages) if @$messages;

    # ps will return an exit status of 0 if it finds at least one matching process, 1 if not.
    if ($status) {
	# The child process exited badly (in this case, it did not find a nagios process running).
	log_timed_message "NOTICE:  \"$name\" $object $action found status $status (nagios is down)" if $DEBUG_NOTICE;
    }
    else {
#	log_timed_message "INFO:  Found good status for \"$name\" $object $action." if $DEBUG_INFO;
    }
    $_[KERNEL]->yield('return_local_state', $args, $status);
}

sub report_local_state {
    my $args     = $_[ARG0];
    my $deadline = $args->{deadline};  # deadline for running this request
    my $ticket   = $args->{ticket};    # ticket for current-request verification
    my $callback = $args->{callback};  # callback event, to be posted to after all state has been gathered

    if (time() >= $deadline) {
	# Stale heartbeat requests are not significant; another request should come shortly.
	# Mostly, we just don't want to execute a bunch of them on top of one another.
	log_timed_message "INFO:  Received stale request to check heartbeat; will ignore." if $DEBUG_INFO;
	return;
    }

    # Here we just queue up a part of the state gathering that must be carried out asynchronously
    # because it runs in a child process.  Once we get that result, we'll pack it in with the rest
    # of the local state and return the whole bundle to the caller.

    # FIX THIS:  I wish there were a more direct way of figuring out whether monitoring is up.
    # A ps command is a relatively expensive operation, given its internal implementation.
    #
    # FIX THIS:  A simple ps command will also pick up <defunct> processes.  To be strict
    # about whether the monitoring is up, we ought to filter those out.  Then again, if the
    # parent nagios dies, its defunct children will be inherited by the init process, and
    # quickly disappear.  So what we're really talking about here is a race condition just
    # as the parent nagios is dying -- which means it's not worth worrying about, as we
    # will catch it on the next heartbeat if the nagios state is persistent.
    #
    # Be aware that this will spill the nagios PIDs on the standard output stream.  The simplest
    # way to deal with this is just to ignore the messages returned from the child process.
    #
    # ps will return an exit status of 0 if it finds at least one matching process, 1 if not.
    my $script = '/bin/ps';
    my @params = ();
    push @params, '--no-headers';
    push @params, '-C';
    push @params, '.nagios.bin';
    push @params, '-o';
    push @params, 'pid';
    $_[KERNEL]->yield('run_child_process',
      {
      object   => 'heartbeat',
      abbrev   => 'pulse',
      name     => 'nagios',
      action   => 'check',
      ticket   => $ticket,
      payload  => $callback,
      script   => $script,
      timeouts => [ 0, 20, 30 ],
      callback => 'nagios_check_callback'
      },
      \@params);
}

# FIX MINOR:  check a return ticket against what we should have stored in $ticket{$abbrev}{$name}{$action}
# FIX MINOR:  do we now store heartbeat tickets in $ticket{heartbeat} ?
sub receive_remote_state {
    my $remote_state = $_[ARG0];
    # log_timed_message "On $unqualified_remote_hostname,  local_server is $remote_state->{'local_server'}";
    # log_timed_message "On $unqualified_remote_hostname, remote_server is $remote_state->{'remote_server'}";
    # log_timed_message "On $unqualified_remote_hostname,    state_time is ", (scalar localtime($remote_state->{'state_time'}));

    # FIX MAJOR:  in analyze()?
    # FIX THIS:  keep track of the last $config->{'flapping-window-heartbeats'} or so attempts
    # to probe for remote state, and if we see flapping within that period (either inability to
    # fetch said state, or a down state, per $config->{'max-bad-heartbeats-before-flapping'}),
    # note it as such in the remote state; perhaps the easiest way to do this is to save the last
    # so-many timestamps of heartbeats when the remote system was seen to be up, up to a maximum
    # of $config->{'flapping-window-heartbeats'} timestamps

    # FIX THIS:  keep track of the time of the last remote outage (either when we
    # failed to obtain remote state, or the remote system was down)

    if ( defined($last_ticket{heartbeat}) && $remote_state->{remote_heartbeat_ticket} eq $last_ticket{heartbeat} ) {
	my $local_state = fetch_local_state();
	if (not defined $local_state->{last_remote_up_state_times}) {
	    $local_state->{last_remote_up_state_times} = [];
	}
	if ($remote_state->{monitoring_is_up}) {
	    my @last_remote_up_state_times = @{ $local_state->{last_remote_up_state_times} };
	    push @last_remote_up_state_times, $remote_state->{state_time};
	    # We only keep everything within the last configured flapping-detection period,
	    # except that we also keep the last up time even if it is older than that.
	    my $flapping_window_start_time = time() - ($config->{'contact-heartbeat-period'} * $config->{'flapping-window-heartbeats'});
	    while (@last_remote_up_state_times > 1 && $last_remote_up_state_times[0] <= $flapping_window_start_time) {
		shift @last_remote_up_state_times;
	    }
	    $local_state->{last_remote_up_state_times} = \@last_remote_up_state_times;
	}

	store_local_state  ($local_state);
	store_remote_state ($remote_state);
	# FIX THIS:  implement proper error detection on the save
	save_replication_state ();

	# FIX THIS:  check to see if the local and remote machines conflict in their
	# Master Configuration Authority states, and if so, send a message to Foundation
	# (actually, defer that conflict detection to analyze())
    }
    else {
	# We don't ever expect to see this condition, but just in case, let's let ourselves know if it does occur.
	# The detail in the secondary log message may tell us a bit about how far out-of-sync the two sides are.
	my $message  = "Disaster Recovery replication received mismatched heartbeat ticket.";
	my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, $message) if $DEBUG_ERROR;
	log_timed_message 'NOTICE:  Remote heartbeat ticket "', $remote_state->{remote_heartbeat_ticket},
	  '" does not match last local ticket "', ($last_ticket{heartbeat} || ''), '".' if $DEBUG_NOTICE;
    }

    # FIX THIS:  should we set this to 0 even if the ticket did not match?  perhaps then this should be set to s
    $got_remote_state_timeout = 0;
    delete $last_ticket{heartbeat};
    # Cancel any outstanding timeout.
    $_[KERNEL]->delay ('remote_state_receipt_timeout');
}

sub remote_state_receipt_timeout {
    $got_remote_state_timeout = 1;
    # We cancel the outstanding ticket to invalidate any response we might get for it later on.
    delete $last_ticket{heartbeat};
    my $message  = "Receipt of remote replication state timed out.";
    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_WARNING, $message) if $DEBUG_ERROR;
    # FIX THIS:  do whatever else is needed here
}

sub log_rotation {
    if (!rotate_logfile()) {
	my $message  = "Problem with rotating the replication logfile; replication (process $$) will stop.";
	my $errors = $foundation->send_message(SEVERITY_CRITICAL, REPLICATION_ENGINE, $message);
	print STDERR join("\n", @$errors) if $errors;
	die $message;
    }

    $_[KERNEL]->delay ('log_rotation', $log_rotation_period);
}

# This might move to the Replication::Logger package.
sub log_dispatch {
    my $log_level           = shift;
    my $foundation_severity = shift;
    my @message             = @_;

    my @response = ();
    my $errors = $foundation->send_message($foundation_severity, REPLICATION_ENGINE, @message);
    $message[0] = "$log_level:  $message[0]" if $log_level;
    log_timed_message @message;
    push @response,   @message;
    push @response,   @$errors if $errors;
    log_timed_message @$errors if $errors;
    return \@response;
}

# Return an epoch-based timestamp representing the next time this event
# should occur, based on the specified event-period and event-phase parameters.
# If the parameters indicate that no further events will occur, returns 0.
# If the parameters are invalid, returns an undefined value.
sub next_event_time {
    my $event_type   = shift;
    my $event_period = shift;
    my $event_phase  = shift;

    # We often schedule successive invocations of an event by using the result of this routine.  But sometimes when
    # such an event is invoked, it actually runs when the system clock is still reading a second before the nominal
    # event time.  That appears to be an issue within POE::Kernel or within the underlying kernel implementation of
    # alarm() that we cannot immediately work around.  Such an event may well call next_event_time() to schedule the
    # next occurrence of the event, but if such an early invocation happens, we might end up concluding that the
    # next time the event should run is when it was originally expected to have run in the next second instead of
    # at some later time according to the $event_period and $event_phase specifications.  This can cause undesirable
    # stuttering (apparent duplicate event invocations).  To prevent that, we don't allow scheduling of events
    # within a couple of seconds of the present time.
    my $inhibition_period = 2;

    # The implementation of this code takes advantage of the fact that the C-level mktime() routine which
    # underlies timelocal_nocheck() is defined to normalize values outside of their usual ranges.
    if ($event_period eq 'daily') {
	if (ref($event_phase) eq 'ARRAY') {
	    my @event_phase = sort @$event_phase;
	    if (@event_phase == 0) {
		log_timed_message "ERROR:  daily $event_type-phase array contains no hh:mm specifications." if $DEBUG_ERROR;
		return undef;
	    }
	    else {
		foreach my $phase (@event_phase) {
		    if ($phase !~ /^(\d\d):(\d\d)$/) {
			log_timed_message "ERROR:  daily $event_type-phase array contains incomprehensible \"$phase\"." if $DEBUG_ERROR;
			return undef;
		    }
		}
	    }
	    my $now = time() + $inhibition_period;
	    my ($sec, $min, $hour, $mday, $mon, $year, $wday, $yday, $isdst) = localtime $now;
	    my $event_time = 0;
	    do  {
		foreach my $phase (@event_phase) {
		    $phase =~ /^(\d\d):(\d\d)$/;
		    my $hh = $1;
		    my $mm = $2;
		    $hour = $hh;
		    $min  = $mm;
		    $sec  = 0;
		    $event_time = timelocal_nocheck ($sec, $min, $hour, $mday, $mon, $year);
		    last if $event_time > $now;
		}
		++$mday;
	    } while ($event_time <= $now);
	    return $event_time;
	}
	elsif ($event_phase =~ /^(\d\d):(\d\d)$/) {
	    my $hh = $1;
	    my $mm = $2;
	    my $now = time() + $inhibition_period;
	    # We use localtime to take into account local Daylight Savings Time
	    # adjustments.  Hopefully we won't run into strange results around
	    # the time of transition, though we haven't thought that out thoroughly.
	    my ($sec, $min, $hour, $mday, $mon, $year, $wday, $yday, $isdst) = localtime $now;
	    $hour = $hh;
	    $min  = $mm;
	    $sec  = 0;
	    my $event_time = timelocal_nocheck ($sec, $min, $hour, $mday, $mon, $year);
	    # If the current time is at or after the specified time of day, then
	    # bump up the day (the next event will instead be tomorrow).  This
	    # correction should handle as well any unexpected situation with DST
	    # adjustments.
	    while ($event_time <= $now) {
		++$mday;
		$event_time = timelocal_nocheck ($sec, $min, $hour, $mday, $mon, $year);
	    }
	    return $event_time;
	}
	else {
	    log_timed_message "ERROR:  daily $event_type-phase \"$event_phase\" is incomprehensible." if $DEBUG_ERROR;
	    return undef;
	}
    }
    elsif ($event_period eq 'minicron') {
	# Only certain formats are supported.  This is really just a transitional implementation
	# until the full "cron" format is supported, and it may be deprecated then (except that
	# my phase-offset "*/#+#" extension might be seen as useful).
	if ($event_phase =~ m{^(\d{1,2})$}) {
	    # "25":  a certain number of minutes past the hour, once every hour
	    my $minute = $1;
	    my $now = time() + $inhibition_period;
	    my ($sec, $min, $hour, $mday, $mon, $year, $wday, $yday, $isdst) = localtime $now;
	    $min = $minute;
	    $sec = 0;
	    my $event_time = timelocal_nocheck ($sec, $min, $hour, $mday, $mon, $year);
	    while ($event_time <= $now) {
		++$hour;
		$event_time = timelocal_nocheck ($sec, $min, $hour, $mday, $mon, $year);
	    }
	    return $event_time;
	}
	elsif ($event_phase =~ m{^\*/(\d{1,2})$}) {
	    # "*/30":  minutes within every hour which are evenly divisible by the specified step size
	    my $step_size = $1;
	    my $now = time() + $inhibition_period;
	    my ($sec, $min, $hour, $mday, $mon, $year, $wday, $yday, $isdst) = localtime $now;
	    $min = 0;
	    $sec = 0;
	    my $event_time = timelocal_nocheck ($sec, $min, $hour, $mday, $mon, $year);
	    while ($event_time <= $now) {
		$min += $step_size;
		if ($min >= 60) {
		    $min = 0;
		    ++$hour;
		}
		$event_time = timelocal_nocheck ($sec, $min, $hour, $mday, $mon, $year);
	    }
	    return $event_time;
	}
	elsif ($event_phase =~ m{^\*/(\d{1,2})\+(\d{1,2})$}) {
	    # "*/30+10":  as above, except with an offset
	    my $step_size = $1;
	    my $offset    = $2;
	    my $now = time() + $inhibition_period;
	    my ($sec, $min, $hour, $mday, $mon, $year, $wday, $yday, $isdst) = localtime $now;
	    $min = $offset;
	    $sec = 0;
	    my $event_time = timelocal_nocheck ($sec, $min, $hour, $mday, $mon, $year);
	    while ($event_time <= $now) {
		$min += $step_size;
		if ($min >= 60) {
		    $min = $offset;
		    ++$hour;
		}
		$event_time = timelocal_nocheck ($sec, $min, $hour, $mday, $mon, $year);
	    }
	    return $event_time;
	}
	else {
	    log_timed_message "ERROR:  minicron $event_type-phase \"$event_phase\" is incomprehensible." if $DEBUG_ERROR;
	    return undef;
	}
	return undef;
    }
    elsif ($event_period eq 'cron') {
	# FIX LATER:  Use one of the following CPAN packages to support crontab formats:
	#     Schedule::Cron
	#     Schedule::Cron::Events
	#     Set::Crontab
	# But look closely at the bug reports for whatever package you choose, and patch
	# the package or edit the supplied user input as needed to work around them.
	# Also be sure to apply the $inhibition_period to these calculations, as above.
	log_timed_message "ERROR:  The \"cron\" $event_type-period is not yet supported." if $DEBUG_ERROR;
	return undef;
    }

    log_timed_message "ERROR:  Found unknown $event_type-period:  $event_period" if $DEBUG_ERROR;
    return undef;
}

# FIX THIS:  this comment may be out of date now
# We want to test if a given process is both a child of ourself and still alive.
# There are a few possible implementations here:
# (1) use waitpid() if the WNOWAIT option is supported (yes on Solaris, apparently not on Linux);
#     this is probably the best choice if it is available
# (2) use waitid() if it is available, perhaps through the POSIX package (haven't checked on
#     this yet); this may be difficult to do portably because this function prototype includes
#     a pointer to a read/write structure
# (3) use getpgrp() if it supports a non-zero PID (fortunately, it is implemented by calling
#     getpgid() on POSIX-compliant platforms such as Solaris and Linux where the C-level getpgrp()
#     does not accept a PID argument); this is a kind of loose parentage test that relies on
#     application conventions about process group management
# (4) use getsid() if we can find it in the POSIX package, as another mechansim for loose
#     parentage determination
# (5) kill(0,$pid) to first test if the process is alive and we have permission to signal it,
#     then test parentage somehow (though most parentage tests will probably inherently include
#     some kind of liveness test, so doing the soft-kill first is probably redundant)
# Ultimately, I think we should implement a XOpen::XSI package or somesuch, to provide a
# bunch of the useful system calls that are not in POSIX, such as waitid() and getsid().
# Or get the Perl community to add these calls to the POSIX package, since it already
# includes setsid().  Note that we can use syscall to call these two routines if they
# are available on the platform, so perhaps that's what we'll need to do for now.
# But if we do that, the syscall.ph header will be needed, and for that, we will need to get
# Bitrock to include it in the distribution.  A question is, is it available in a portable
# form in the Bitrock environment?  See GWMON-8508, where this issue is being tracked.

sub child_is_alive {
    my $pid = shift;

    # This is perhaps a redundant test, but if the child was that intent on running away, we'll let it.
    # (This will also catch situations where the child has already exited, whether or not our child
    # wanted that independence, and whether or not it's been replaced by a doppleganger.)
    return 0 if getsid($pid) != $parent_sid;

    my $result = waitid_pid_nowait($pid);
    if ($result < 0) {
	return 0 if $! == ECHILD;  # Process does not exist or is not our child.

	# This should never happen.  There are no other documented error codes.
	log_timed_message "ERROR:  waitid_pid_nowait() returned error ($!); assuming child process is gone" if $DEBUG_ERROR;

	# For now, let's make this error very visible.  We can restructure
	# the code if this ever happens, once we know what might happen.
	die "FATAL:  Received impossible error code ($!) from waitid_pid_nowait(),";

	return 0;
    }
    # ($result == 0) => kid is kicking; ($result != 0) => kid is a zombie
    return !$result;
}

sub on_child_start {
    my ($object, $abbrev, $name, $action, $ticket, $payload, $script, $timeouts, $callback, $params) = @_[ARG0..ARG9];

    # FIX LATER:  Ought we to check for an alias-assignment failure due to an alias-name collision
    # during a subsequent session's attempt to establish exactly the same alias, if this session is
    # somehow still alive when another comes along and wants to use exactly the same alias?  If that
    # should occur, should that signal to us that we have a serious concurrency/synchronization
    # problem happening that we need to correct, perhaps by either abandoning the new session or
    # forcibly killing the old one, or both?
    $_[KERNEL]->alias_set("$action-$object-$name");

    # We need an arrayref in any case, even if there are no parameters for the script we will run.
    if (not defined $params) {
	$params = [];
    }

    # This is here to help debug why these sessions seem to hang around long after
    # their _stop event has been automatically triggered.
    # FIX THIS:  Might we have a circular reference somewhere?
    # See POE::Session for details and for clues as to how to verify and debug.
    log_timed_message "DEBUG:  Session '$action-$object-$name' is being started." if $DEBUG_DEBUG;

    # We block SIGCHLD for the duration of this routine, using sigprocmask, to guarantee
    # that we get a consistent view of the child process until we can call segpgid() on
    # the child process (the process ID of the child process won't be able to be reassigned
    # during this period, since the zombie process won't be reaped during this period).
    # Note that the implementation here presumes we are running a single-threaded process,
    # so sigprocmask() can be called.  Otherwise, only pthread_sigmask() would necessarily
    # be valid, but then calling it for a single thread probably would not be sufficient
    # to provide the protection we're looking for.
    my $oldblockset = POSIX::SigSet->new;
    my $newblockset = POSIX::SigSet->new(SIGCHLD);
    sigprocmask(SIG_BLOCK, $newblockset, $oldblockset) or die "FATAL:  Could not block SIGCHLD ($!),";

    my $child = undef;
    eval {
	if ($DEBUG_DEBUG) {
	    log_timed_message "DEBUG:  About to run the script to $action $object \"$name\":";
	    log_timed_message "DEBUG:  $script ", defined($params) ? join(' ', @$params) : '';
	}
	$child = POE::Wheel::Run->new (
	    Program     => [ $script ],
	    ProgramArgs => $params,
	    StdoutEvent => 'got_child_stdout',
	    StderrEvent => 'got_child_stderr',
	    CloseEvent  => 'got_child_close',
	    ErrorEvent  => 'got_child_error',
	    # We don't want the child process to be its own session leader, as later on we may
	    # want to use its session ID as a means of proving it is one of our descendants.
	    NoSetSid  => 1,
	    # We do want the child process to be its own process group leader, as later on we might
	    # want to kill the entire tree of processes descended from it, should it time out.
	    # setpgrp() will be called in the child.  Compare to the POSIX::setpgid() call below.
	    NoSetPgrp => 0,
	);
    };
    if ($@) {
	chomp $@;
	log_timed_message "ERROR:  Failed to spawn a child process to $action $object \"$name\": $@" if $DEBUG_ERROR;
	if ($$ != $parent_pid) {
	    # We're in the child process.  We'd better die right away so as not to
	    # confuse anyone by continuing to run as though we were the parent process.
	    die "Child process $$ is dying after error,";
	}
    }
    else {
	# Avoid potential race conditions in the parent process (this script) by setting the process group
	# of the child here to make it a process group leader, even though it will also be set in the child.
	# One of these two calls (the setpgrp() in the child, due to the NoSetPgrp => 0 setting, or this
	# setpgid() in the parent) will fail, but that won't matter.  See APITUE2/e, page 270 for the rationale.
	# Having the child be a process group leader means that we can easily kill the entire process group
	# should the child script time out.  The only possible danger here comes from Perl's safe signals, which
	# might cause a SIGCHLD to be recognized and dealt with early (and thus a dead child process to be waited
	# for and completely gone before we make this setpgid() call).  That raises the possibility that we might
	# be setting the process group on some other process which has subsequently taken the same process ID
	# that was used by our direct child.  The sigprocmask() call above is an attempt to avoid that issue,
	# by preventing any unexpected under-the covers child reaping by Perl while we execute in this routine.
	# Not waiting for the child until we make this call should at least guarantee that the direct child
	# still occupies the slot in the process table, and thus no other process can be mistaken for it.  In
	# the possible absence of that guarantee, all bets are off.
	#
	# Two things are confusing about POSIX::setpgid().  First, it will return the string '0 but true'
	# as its result upon success, and undef upon failure.  So this call behaves more like a standard
	# Perl routine (returns a true value upon success, and a false value upon failure) than as a C call
	# (returns a 0 [i.e., false] upon success, and -1 [i.e., true] upon failure).  Of course, if you
	# capture the '0 but true' return value and try to use it in a numeric context, it will behave as
	# though it were 0, which can be rather confusing and contradictory to what you would naively expect
	# considering that the value is true.  Second, as with any system call, POSIX::setpgid() does not
	# clear $! before running, so you cannot depend on the value of $! if you do not first check for an
	# error return from the call and you have not explicitly cleared $! yourself before the call.
	#
	# A POSIX::EACCES failure is okay (the child did the dirty work first). 
	if (! POSIX::setpgid($child->PID, 0) && $! != POSIX::EACCES) {
	    # ESRCH (pid is not the current process and not a child of the current process) is the bad error
	    # code we dread here.  We might also see EACCES (an attempt was made to change the process group ID
	    # of one of the children of the calling process and the child had already performed an execve()),
	    # but that should not be considered an error in the present context.  (The child's own setpgid()
	    # call should have occurred before such an execve() call, so it ought to be safe.)
	    log_timed_message "ERROR:  setpgid() in parent of child for $action of $object \"$name\" failed ($!)" if $DEBUG_ERROR;
	}

	# I don't know how to fetch the parent session ID directly later on,
	# so we'd better save it now while it's still in hand.
	$_[HEAP]{parent} = $_[SENDER];

	$_[KERNEL]->sig_child($child->PID, 'got_child_signal');

	# Wheel events include the wheel's ID.
	$_[HEAP]{children_by_wid}{$child->ID} = $child;

	# Signal events include the process ID.
	$_[HEAP]{children_by_pid}{$child->PID} = $child;

	# FIX THIS:  when the process truly dies (probably in the sig_child handler), this must be deleted.
	# Put the child object in the heap where our timeout handler can find it and call its
	# $child->kill() method.  We don't make this storage specific to the $action because we
	# want only one child process to be outstanding for any given object at any one time.
	# FIX MAJOR:  is this even the right heap to be stuffing this on?  If this is on the heap
	# of the particular session that is managing the child process, it probably isn't the heap
	# for the session that may eventually run the timeout.
	$_[HEAP]{children_by_object}{$object}{$name} = $child;

	# FIX THIS:  When the process truly dies, this must be deleted.  But (and this is critical) we must
	# only do that in the later of the sig_child handler and the close handler, since we always want to
	# capture all the child message output, and we always want to capture the child exit status.  So we
	# will need this info to be available in both places, while we don't know which event will occur first.
	# Put the relevant application args in the heap where the various on_child_xxx() handlers can find them.
	# I really hate putting two copies into the heap, but that seems to be forced by our inability to know
	# whether on_child_close or on_child_signal will be called last.
	my $args = {
	    object   => $object,
	    abbrev   => $abbrev,
	    name     => $name,
	    action   => $action,
	    ticket   => $ticket,
	    payload  => $payload,
	    timeouts => $timeouts,
	    callback => $callback
	};
	$_[HEAP]{args_by_wid}{$child->ID}  = $args;
	$_[HEAP]{args_by_pid}{$child->PID} = $args;

	# Create a place to store accumulating messages from the child process (clear any previous data here).
	$_[HEAP]{messages}{$object}{$name} = [];

	# Create a place to store the final exit status from the child process (clear any previous data here).
	$_[HEAP]{status}{$object}{$name} = undef;

	log_timed_message 'FERVID:  Child PID ', $child->PID, " to $action $object \"$name\" started as wheel ", $child->ID, '.' if $DEBUG_FERVID;

	# Set a timeout for running the script to completion, so we will know when we failed to get what we asked for.
	# Stuff the $alarm_id in the heap in association with this object, so we can cancel it if we get a response when we hope to.
	if ($timeouts->[WARNING_TIMEOUT] > 0) {
	    my $alarm_id = $_[KERNEL]->delay_set ('got_child_timeout', $timeouts->[WARNING_TIMEOUT], $args, WARNING_TIMEOUT);
	    $_[HEAP]{alarms}{$abbrev}{$name} = $alarm_id;
	}
	elsif ($timeouts->[SIGTERM_TIMEOUT] > 0) {
	    my $alarm_id = $_[KERNEL]->delay_set ('got_child_timeout', $timeouts->[SIGTERM_TIMEOUT], $args, SIGTERM_TIMEOUT);
	    $_[HEAP]{alarms}{$abbrev}{$name} = $alarm_id;
	}
	elsif ($timeouts->[SIGKILL_TIMEOUT] > 0) {
	    my $alarm_id = $_[KERNEL]->delay_set ('got_child_timeout', $timeouts->[SIGKILL_TIMEOUT], $args, SIGKILL_TIMEOUT);
	    $_[HEAP]{alarms}{$abbrev}{$name} = $alarm_id;
	}
    }

    # Restore the old signal mask so we can reap zombies once again.
    sigprocmask(SIG_SETMASK, $oldblockset) or die "FATAL:  Could not restore SIGCHLD signal ($!),";
}

# Wheel event, including the wheel's ID.
sub on_child_stdout {
    my ($stdout_line, $wheel_id) = @_[ARG0, ARG1];
    # FIX THIS:  is there a race condition here, whereby these $child and $args references may have already been deleted by on_child_signal?
    my $child  = $_[HEAP]{children_by_wid}{$wheel_id};
    my $args   = $_[HEAP]{args_by_wid}{$wheel_id};
    my $object = $args->{object};    # 'application' or 'database'
    my $name   = $args->{name};      # app or db name
    push @{ $_[HEAP]{messages}{$object}{$name} }, $stdout_line;

    # This is only for early development debugging.
    # log_timed_message 'FERVID:  ', $child->PID, " STDOUT: $stdout_line" if $DEBUG_FERVID;
}

# Wheel event, including the wheel's ID.
sub on_child_stderr {
    my ($stderr_line, $wheel_id) = @_[ARG0, ARG1];
    # FIX THIS:  is there a race condition here, whereby these $child and $args references may have already been deleted by on_child_signal?
    my $child  = $_[HEAP]{children_by_wid}{$wheel_id};
    my $args   = $_[HEAP]{args_by_wid}{$wheel_id};
    my $object = $args->{object};    # 'application' or 'database'
    my $name   = $args->{name};      # app or db name

    # FIX THIS:  For now, we just intermix the stdout and stderr lines in our accumulated output.
    # This has all the usual problems of possibly out-of-order presentation.  While the Wheel
    # setup will probably avoid interleaving in the middle of lines, we still have no guarantee
    # that on_child_stdout and on_child_stderr will be called in the same order as the script
    # wrote data on these streams.  In fact, we have seen the opposite to occur sometimes.
    # Possibly we might want to separate these i/o streams, though that has its own issues of
    # understanding where the errors actually occurred in the sequence of actions by the child.
    push @{ $_[HEAP]{messages}{$object}{$name} }, $stderr_line;

    # This is only for early development debugging.
    # log_timed_message 'FERVID:  ', $child->PID, " STDERR: $stderr_line" if $DEBUG_FERVID;
}

sub on_child_error {
    my ($operation, $error_number, $error_string, $wheel_id, $bad_fh_name) = @_[ARG0..ARG4];
    if ($operation eq 'read' and !$error_number) {
	# Here's what we would report in this case, if it were worth reporting.
	# But this will always happen when the child script dies, for both STDOUT
	# and STDERR, so it's not worth reporting.
	$error_string = "remote end of child $bad_fh_name closed";
    }
    else {
	# Here's what we need to report, in all other cases.
	log_timed_message "FATAL:  Child process $$ for wheel $wheel_id:  $operation failed with error $error_number ($error_string)" if $DEBUG_FATAL;
    }

    # This might be called in the child process (it's not clear from the documentation whether that might be the case).
    if ($$ != $parent_pid) {
	# We're in the child process.  We'd better die right away so as not to
	# confuse anyone by continuing to run as though we were the parent process.
	# Let's experiment, though, and see if we can get a message out to the outside world,
	# in case we have child processes mysteriously disappear.
	log_timed_message "FATAL:  (from child) Child process $$ is dying after error." if $DEBUG_FATAL;
	die "Child process $$ is dying after error,";
    }
}

# Wheel event, including the wheel's ID.
sub on_child_close {
    my $wheel_id = $_[ARG0];

    my $args     = $_[HEAP]{args_by_wid}{$wheel_id};
    my $object   = $args->{object};    # 'application' or 'database'
    my $abbrev   = $args->{abbrev};    # 'app' or 'db'
    my $name     = $args->{name};      # app or db name
    my $action   = $args->{action};    # action
    my $ticket   = $args->{ticket};    # ticket for current-request verification
    my $payload  = $args->{payload};   # opaque payload
    my $callback = $args->{callback};  # callback event, to be triggered when the child process is finally done
    delete $_[HEAP]{args_by_wid}{$wheel_id};

    # FIX THIS:
    # Unless we want to transfer the accumulated messages to $args, we don't
    # need to clean up anything here directly related to the i/o data.  We'll
    # leave it to the callback to spill out the accumulated messages to the
    # log along with the child process exit status.

    # FIX MAJOR:  There is a possible race condition here.  If we get here, the
    # process might already be dead, but we won't recognize that here until the
    # on_child_signal event happens, so the timeout might still be called before
    # the on_child_signal event happens.  That could cause it to try to kill a
    # PID which is already in use by some other process.  Figure out some
    # reliable way to solve that race condition.

    # May have been reaped already by on_child_signal().
    my $child = delete $_[HEAP]{children_by_wid}{$wheel_id};
    if (defined $_[HEAP]{children_by_pid}{$child->PID}) {
	# We are the first termination handler [between on_child_close and on_child_signal] to run for this child.
	# We will wait for the other termination handler to fire before moving on.
	log_timed_message 'FERVID:  PID ', $child->PID, " for $action of $object \"$name\" closed all pipes." if $DEBUG_FERVID;
    }
    else {
	# We are the last termination handler [between on_child_close and on_child_signal] to run for this child.
	log_timed_message "FERVID:  wid $wheel_id for $action of $object \"$name\" closed all pipes." if $DEBUG_FERVID;
	$_[KERNEL]->yield('got_child_done', $args);
    }
}

sub on_child_signal {
    my $pid    = $_[ARG1];
    my $status = $_[ARG2];

    my $args     = $_[HEAP]{args_by_pid}{$pid};
    my $object   = $args->{object};    # 'application' or 'database' or 'heartbeat'
    my $abbrev   = $args->{abbrev};    # 'app' or 'db' or 'pulse'
    my $name     = $args->{name};      # app or db name
    my $action   = $args->{action};    # action
    my $ticket   = $args->{ticket};    # ticket for current-request verification
    my $payload  = $args->{payload};   # opaque payload
    my $callback = $args->{callback};  # callback event, to be triggered when the child process is finally done
    delete $_[HEAP]{args_by_pid}{$pid};

    log_timed_message "FERVID:  PID $pid script to $action $object \"$name\" has ", wait_status_message($status), '.' if $DEBUG_FERVID;
    $_[HEAP]{status}{$object}{$name} = $status;

    # Disable the appropriate object action timeout alarm, so we don't get the timeout later on.
    # FIX THIS:  Note that this is no guarantee that the timeout hasn't already been called.
    delete $_[HEAP]{children_by_object}{$object}{$name};
    if (! $_[HEAP]{alarms}{$abbrev}{$name}) {
	# FIX THIS:  this is just to prove to myself that this alarm is actually set here when it should be
	log_timed_message "DEBUG:  There is no alarm to limit the $action of $object \"$name\"." if $DEBUG_DEBUG;
    }
    elsif (! $_[KERNEL]->alarm_remove ($_[HEAP]{alarms}{$abbrev}{$name}) ) {
	# FIX THIS:  We have seen this happen when the timeout was called before we got here,
	# but also at other times long before the timeout is called.  What is happening here?
	log_timed_message "ERROR:  Failed to remove the $action callback timeout alarm for $object \"$name\"." if $DEBUG_ERROR;
    }

    # May have been reaped already by on_child_close().
    my $child = delete $_[HEAP]{children_by_pid}{$pid};
    if (defined $_[HEAP]{children_by_wid}{$child->ID}) {
	# We are the first termination handler [between on_child_close and on_child_signal] to run for this child.
	# We will wait for the other termination handler to fire before moving on.
    }
    else {
	# We are the last termination handler [between on_child_close and on_child_signal] to run for this child.
	$_[KERNEL]->yield('got_child_done', $args);
    }
}

# FIX THIS:  It's no longer clear why we need this level of indirection.  In earlier development,
# there was more to the body of the on_child_done routine.  Perhaps we'd be better off just yielding
# to the callback directly, from the places where on_child_done is currently being yielded to.
sub on_child_done {
    my $args     = $_[ARG0];
    my $object   = $args->{object};    # 'application' or 'database'
    my $abbrev   = $args->{abbrev};    # 'app' or 'db'
    my $name     = $args->{name};      # app or db name
    my $action   = $args->{action};    # action
    my $ticket   = $args->{ticket};    # ticket for current-request verification
    my $payload  = $args->{payload};   # opaque payload
    my $callback = $args->{callback};  # callback event, to be triggered when the child process is finally done

    if ($args->{callback}) {
	$_[KERNEL]->post($_[HEAP]{parent}, $args->{callback}, $args, $_[HEAP]{status}{$object}{$name}, $_[HEAP]{messages}{$object}{$name});
    }
    else {
	log_timed_message "ERROR:  Child to $action $object \"$name\" is finished, but there is no callback to carry on." if $DEBUG_ERROR;
    }
    # The job of this session is done.  We remove its alias so it can be subjected to garbage collection.
    $_[KERNEL]->alias_remove("$action-$object-$name");
}

sub on_child_timeout {
    my $args         = $_[ARG0];
    my $timeout_type = $_[ARG1];

    my $object   = $args->{object};    # 'application' or 'database'
    my $abbrev   = $args->{abbrev};    # 'app' or 'db'
    my $name     = $args->{name};      # app or db name
    my $action   = $args->{action};    # action
    my $timeouts = $args->{timeouts};  # arrayref to [ warning, sigterm, sigkill ] timeouts

    # We block SIGCHLD for the duration of this routine, using sigprocmask, to guarantee
    # that we get a consistent view of the child processes and whether they exist or not
    # (whether alive or dead or in transition, at least the process ID of the child
    # process won't be able to be reassigned during this period, since the zombie process
    # won't be asynchronously reaped while we probe here).
    #
    # Note that the implementation here presumes we are running a single-threaded process,
    # so sigprocmask() can be called.  Otherwise, only pthread_sigmask() would necessarily
    # be valid, but then calling it for a single thread probably would not be sufficient
    # to provide the protection we're looking for.
    my $oldblockset = POSIX::SigSet->new;
    my $newblockset = POSIX::SigSet->new(SIGCHLD);
    sigprocmask(SIG_BLOCK, $newblockset, $oldblockset) or die "FATAL:  Could not block SIGCHLD ($!),";

    my $child = $_[HEAP]{children_by_object}{$object}{$name};

    # I really, really hate sending signals to other processes in an environment where processes may
    # come and go continuously from this or other applications.  Trouble is, if you only have the Process
    # ID to go by, you can never be 100% sure that the process you're signaling hasn't been replaced by
    # some other process that is now re-using the same PID.  We do our best here to close that window
    # of uncertainty, by insisting that we only send a signal if we believe the other process is our
    # direct child.  There is still a window of vulnerability (race condition) between the parentage test
    # and the kill; unfortunately, UNIX doesn't seem to provide any kind of atomic operation that would
    # include both actions.  Still, we will at least take advantage of whatever facilities are available,
    # and hope that the kernel doesn't re-use PIDs for a long time after they are freed up.

    # The child process should still be there if it has not been reaped yet.  And the call to
    # sigprocmask() above is an attempt to freeze the system sufficiently so as to block reaping,
    # though whether that comes too late is open to question.

    # The child had better be alive now, or we shouldn't have been called here (presuming it won't
    # have died and been somehow have been reaped before this timeout was cancelled).

    # Possible multiprocessor concurrency means there can be no guarantee unless we can block reaping,
    # know that it is not being carried out under any auspices other than the ones we directly control
    # (such as within a system() call or within the implementation of backticks, which is why their
    # use must be disallowed in this program), and thereby prevent the child PID from being re-used.

    # We don't call $child->kill(); because that would only kill the direct child script, not also
    # whatever processes it may have spawned.  In the presence of a timeout, we want instead to kill the
    # entire process group.  Note that the perlfunc documentation for kill suggests that negating the
    # PID is non-portable, but then the perlipc documentation ignores that and recommends the practice.
    # The underlying kill(2) call in both Linux and Solaris supports it, so we use that here rather
    # then going through a complicated dance with %Config to map a signal name to a signal number so
    # we can negate it.

    if ($child && child_is_alive($child->PID)) {
	my $timeout = $timeouts->[$timeout_type];
	if ($timeout_type == WARNING_TIMEOUT) {
	    # A warning timeout has occurred.
	    my $message  = "Replication:  $action of $object \"$name\" is taking more than $timeout seconds.";
	    my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, $message);
	    # If the child is still alive, schedule a follow-on sigterm or sigkill timeout.
	    if ($timeouts->[SIGTERM_TIMEOUT] > 0) {
		# schedule a future SIGTERM timeout
		my $alarm_id = $_[KERNEL]->delay_set ('got_child_timeout', ($timeouts->[SIGTERM_TIMEOUT] - $timeout), $args, SIGTERM_TIMEOUT);
		# Stuff this $alarm_id in the heap in association with this object,
		# so we can cancel it if we get a response when we hope to.
		$_[HEAP]{alarms}{$abbrev}{$name} = $alarm_id;
	    }
	    elsif ($timeouts->[SIGKILL_TIMEOUT] > 0) {
		# schedule a future SIGKILL timeout
		my $alarm_id = $_[KERNEL]->delay_set ('got_child_timeout', ($timeouts->[SIGKILL_TIMEOUT] - $timeout), $args, SIGKILL_TIMEOUT);
		# Stuff this $alarm_id in the heap in association with this object,
		# so we can cancel it if we get a response when we hope to.
		$_[HEAP]{alarms}{$abbrev}{$name} = $alarm_id;
	    }
	}
	elsif ($timeout_type == SIGTERM_TIMEOUT) {
	    # A sigterm timeout has occurred.
	    my $message  = "Replication:  $action of $object \"$name\" is taking more than $timeout seconds, and will be terminated.";
	    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_WARNING, $message);
	    # If the child is still alive, send SIGTERM to the child's process group and schedule a follow-on sigkill timeout.
	    if ($child && child_is_alive($child->PID)) {
		# send SIGTERM to the child's process group
		log_timed_message "NOTICE:  Sending SIGTERM to child process group that is trying to $action $object \"$name\":  ", $child->PID
		  if $DEBUG_NOTICE;
		my $count = kill 'TERM', 0 - $child->PID;
		log_timed_message "DEBUG:  $count processes were sent a SIGTERM signal." if $DEBUG_DEBUG;
		if ($timeouts->[SIGKILL_TIMEOUT] > 0) {
		    # schedule a future SIGKILL timeout
		    my $alarm_id = $_[KERNEL]->delay_set ('got_child_timeout', ($timeouts->[SIGKILL_TIMEOUT] - $timeout), $args, SIGKILL_TIMEOUT);
		    # Stuff this $alarm_id in the heap in association with this object,
		    # so we can cancel it if we get a response when we hope to.
		    $_[HEAP]{alarms}{$abbrev}{$name} = $alarm_id;
		}
	    }
	}
	elsif ($timeout_type == SIGKILL_TIMEOUT) {
	    # A sigkill timeout has occurred.
	    my $message  = "Replication:  $action of $object \"$name\" is taking more than $timeout seconds, and will be killed.";
	    my $response = log_dispatch (LOG_LEVEL_FATAL, SEVERITY_CRITICAL, $message);
	    # If the child is still alive, send SIGKILL to the child's process group.
	    if ($child && child_is_alive($child->PID)) {
		# send SIGKILL to the child's process group
		log_timed_message "NOTICE:  Sending SIGKILL to child process group that is trying to $action $object \"$name\":  ", $child->PID
		  if $DEBUG_NOTICE;
		my $count = kill 'KILL', 0 - $child->PID;
		log_timed_message "DEBUG:  $count processes were sent a SIGKILL signal." if $DEBUG_DEBUG;
	    }
	}
	else {
	    log_timed_message "ERROR:  During $action of $object \"$name\", found unknown timeout type $timeout_type." if $DEBUG_ERROR;
	    # We consider this to be critical not because it will necessarily cause a run-time hiccup,
	    # but because it reveals some implementation problem.
	    my $errors = $foundation->send_message(SEVERITY_CRITICAL, REPLICATION_ENGINE,
		"Replication internal error:  $action of $object \"$name\" has encountered an unknown timeout type."
	    );
	    log_timed_message @$errors if $errors;
	}
    }
    else {
	# Let's log this unexpected occurrence, just to let ourselves know it can sometimes happen.
	# We've seen this happen sometimes due to an apparent race condition just as the child is finishing.
	# That's a clue that perhaps the relevant timeout should be slightly longer.
	log_timed_message "ERROR:  Timeout for $action of $object \"$name\" was triggered after child is gone." if $DEBUG_ERROR;
    }

    # Restore the old signal mask so we can reap zombies once again.
    sigprocmask(SIG_SETMASK, $oldblockset) or die "FATAL:  Could not restore SIGCHLD signal ($!),";

    # We don't delete the $child object from the heap now, leaving that cleanup
    # for the child termination processing.
}

sub run_child_process {
    my $args     = $_[ARG0];
    my $params   = $_[ARG1];
    my $object   = $args->{object};    # 'application' or 'database'
    my $abbrev   = $args->{abbrev};    # 'app' or 'db'
    my $name     = $args->{name};      # app or db name
    my $action   = $args->{action};    # action
    my $ticket   = $args->{ticket};    # ticket for current-request verification
    my $payload  = $args->{payload};   # opaque payload
    my $script   = $args->{script};    # script or program pathname
    my $timeouts = $args->{timeouts};  # arrayref to [ warning, sigterm, sigkill ] timeouts
    my $callback = $args->{callback};  # callback event, to be triggered when the child process is finally done

    log_timed_message "DEBUG:  Running child process to $action $object \"$name\"." if $DEBUG_DEBUG;

    # FIX THIS:  What if some action script is already operating here for this object?
    # In that case, we don't want to have possibly conflicting actions running at the
    # same time.  So before we go running a new session to manage an action script child,
    # we need to verify that we don't already have any actions running for this object,
    # either here or remotely on behalf of us here.  If we do find such a conflict, we
    # need to log this situation and not run the child process now.

    POE::Session->create(
	inline_states => {
	    _start            => \&on_child_start,
	    got_child_stdout  => \&on_child_stdout,
	    got_child_stderr  => \&on_child_stderr,
	    got_child_close   => \&on_child_close,
	    got_child_error   => \&on_child_error,
	    got_child_signal  => \&on_child_signal,
	    # FIX LATER:  on_child_done may go away in the future, in favor of direct callbacks
	    got_child_done    => \&on_child_done,
	    got_child_timeout => \&on_child_timeout,
	    _stop => sub {
		log_timed_message "DEBUG:  Session '$action-$object-$name' is being stopped." if $DEBUG_DEBUG;
		# Removing the alias here should be just a stop-gap fallback, as it will generally
		# have already been removed within the on_child_done() routine when it ran.  So we
		# are mainly covering the case where that routine has not yet run, and the session
		# was not really ready to die when the _stop event appeared.
		$_[KERNEL]->alias_remove("$action-$object-$name");

		# FIX THIS:  If the child process is still running, kill its process group here with
		# SIGTERM and then SIGKILL a very short time later (like perhaps 10 or 15 seconds).
		# But if we do that, then make sure to erase the record of the child process afterward,
		# because this _stop method might get called a second time as the entire script dies.
		# (That might be an artifact of earlier code, when we did not remove the alias, so it
		# might no longer be occurring.  Or this session might stay alive anyway until its
		# child dies, that being the hook that saves it temporarily from garbage collection.)
	    },
	},
	args => [ $object, $abbrev, $name, $action, $ticket, $payload, $script, $timeouts, $callback, $params ],
    );
}

# FIX THIS:  add possible timeouts to this picture
# Sequence of actions for a complete replication cycle:
#
# Local site				Remote site
# ========================	========================================================
# sync_object
# obtain_object
#				remote_capture_object  (receives request from remote)
#				capture_object
#				local_capture_is_done  [FIX THIS:  or timeout?]
#				return_capture_result  (returns status to remote)
# remote_capture_callback
# object_obtain_callback
# stop_object
# object_stop_callback
# deploy_object
# object_deploy_callback
# start_object
# object_start_callback

sub remote_capture_object {
    my $args     = $_[ARG0];
    my $object   = $args->{object};    # 'application' or 'database'
    my $abbrev   = $args->{abbrev};    # 'app' or 'db'
    my $name     = $args->{name};      # app or db name
    my $action   = $args->{action};    # action ('capture')
    my $deadline = $args->{deadline};  # deadline for running this request
    my $ticket   = $args->{ticket};    # ticket for current-request verification
    my $callback = $args->{callback};  # callback event, to be saved and triggered when all this remote processing is finished

    if (time() >= $deadline) {
	# Stale capture requests need to be visible externally, not just in the log file,
	# because they represent a failure of expected replication actions to run smoothly.
	my $message  = "Replication:  Received stale request to $action \"$name\" $object; will ignore.";
	my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, $message);
	return;
    }

    # FIX LATER:  Should we store the ticket somewhere, and refuse to run the capture if we already have a ticket outstanding?
    # That would only work if there is some way to forcibly clear old tickets from time to time.

    # First, save the remote callback so later on we will know where to return results,
    # inasmuch as that won't happen from within this routine.
    $_[HEAP]->{callback}{$abbrev}{$name} = $callback;

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\"." if $DEBUG_FERVID;

    my $local_state = fetch_local_state();

    # FIX MINOR:  extend this test to check for ongoing local stop/deploy/start actions (maybe just the object being "active"),
    # though it is unlikely that we would see interference because this side should not be in Slave mode if the other side is
    # (which it must be, to make a capture request)
    #
    # if (blocked, or we have interference from any ongoing local stop/deploy/start actions) ...
    #
    # We don't test here for remote blocking as well (to generate a console message, at least),
    # because if replication were blocked there, this routine wouldn't have been called.
    if ( $local_state->{$abbrev}{$name}{blocked} ) {
	my $message  = "Not running $action action for $object \"$name\" because replication for this object is blocked.";
	my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, $message);
	# FIX THIS:  mark as inactive or stalled (if blocked)?
    }
    else {
	my $script = realpath ("$actions_base/$abbrev/$name/$name.$action");
	if ($script) {
	    if (-x $script) {
		log_timed_message "FERVID:  Ready to run the $action script for $object \"$name\"." if $DEBUG_FERVID;
		# Run the $action script as the next step in locally installing the new configuration.
		# FIX MAJOR:  pass in the $args here, adding/overriding the $action action, $action scripts,
		# $action timeouts, and $action callback, rather than constructing a new anonymous hash?
		# Most importantly, add the $action callback here?
		$_[KERNEL]->yield("${action}_object", {
		    object   => $object,
		    abbrev   => $abbrev,
		    name     => $name,
		    action   => $action,
		    ticket   => $ticket,
		    script   => $script,
		    # FIX THIS:  make sure the timeouts are populated in the heap of this session at its startup time
		    # FIX THIS:  what is the timeout event here?
		    timeouts => $_[HEAP]{$abbrev}{$name}{"${action}-timeouts"},
		    callback => "local_${action}_is_done"
		});
	    }
	    else {
		my $message  = "Replication:  The $object \"$name\" $action script is not executable.";
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
	    }
	}
	else {
	    my $message  = "Replication:  The path to the $object \"$name\" $action script does not exist.";
	    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
	}
    }
}

# FIX THIS:  where is this invoked?  is there actually any need for this now?
sub local_capture_timed_out {
    my $args   = $_[ARG0];
    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'
    my $name   = $args->{name};    # app or db name
    my $action = $args->{action};  # action ('capture')

    # FIX MAJOR:  deal with a ticket here as well?

    # FIX THIS:  make sure that $args contains all the right stuff
    $args->{status}   = -1;
    $args->{messages} = [ "ERROR:  Remote $action of $object \"$name\" timed out." ];
    $_[KERNEL]->yield('return_capture_result', $args);
}

sub local_capture_is_done {
    my $args     = $_[ARG0];
    my $status   = $_[ARG1];
    my $messages = $_[ARG2];

    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'
    my $name   = $args->{name};    # app or db name
    my $action = $args->{action};  # action ('capture')
    my $ticket = $args->{ticket};  # ticket for current-request verification

    log_timed_message
      "INFO:  Script to $action $object \"$name\" returned wait status $status [", wait_status_message($status), "] and ",
      @$messages ? ((scalar @$messages).((@$messages == 1) ? ' message' : ' messages').' (#):') : 'no messages.' if $DEBUG_INFO;
    log_message "# ", join ("\n# ", @$messages) if @$messages;

    # FIX THIS:  make sure that $args contains all the right stuff
    $args->{status}   = $status;
    $args->{messages} = $messages;
    $_[KERNEL]->yield('return_capture_result', $args);
}

sub return_capture_result {
    my $args     = $_[ARG0];
    my $object   = $args->{object};  # 'application' or 'database'
    my $abbrev   = $args->{abbrev};  # 'app' or 'db'
    my $name     = $args->{name};    # app or db name
    my $action   = $args->{action};  # action ('capture')
    my $ticket   = $args->{ticket};  # ticket for current-request verification
    my $status   = $args->{status};
    my $messages = $args->{messages};

    # Whoever gets here first (timeout or done) will have the opportunity to get their results
    # returned to the original caller.  Anyone else will have their results discarded.
    # FIX THIS:  why is this callback stored in the heap and not in the args?
    my $callback = delete $_[HEAP]->{callback}{$abbrev}{$name};

    if ($callback) {
	# Send the final results back to the guy that called us.
	$_[KERNEL]->post('IKC', 'post', $callback, $args);
    }
    else {
	log_timed_message "NOTICE:  Final result from incoming remote call to $action $object \"$name\" is being discarded." if $DEBUG_NOTICE;
    }
}

sub remote_capture_callback {
    my $args     = $_[ARG0];
    my $object   = $args->{object};  # 'application' or 'database'
    my $abbrev   = $args->{abbrev};  # 'app' or 'db'
    my $name     = $args->{name};    # app or db name
    my $action   = $args->{action};  # action ('capture')
    my $ticket   = $args->{ticket};  # ticket for current-request verification
    my $status   = $args->{status};
    my $messages = $args->{messages};

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\"." if $DEBUG_FERVID;

    # Pass the remote result back to the session that initiated the remote call.
    # But first, restore the action to how we view it locally.
    $action = $args->{action} = 'obtain';
    $_[KERNEL]->post("$unqualified_local_hostname-${object}s", "object_${action}_callback", $args);
}

sub object_obtain_callback {
    my $args     = $_[ARG0];
    my $object   = $args->{object};  # 'application' or 'database'
    my $abbrev   = $args->{abbrev};  # 'app' or 'db'
    my $name     = $args->{name};    # app or db name
    my $action   = $args->{action};  # action ('obtain')
    my $ticket   = $args->{ticket};  # ticket for current-request verification
    my $status   = $args->{status};
    my $messages = $args->{messages};

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\"." if $DEBUG_FERVID;

    log_timed_message
      "INFO:  Script to $action $object \"$name\" returned wait status $status [", wait_status_message($status), "] and ",
      @$messages ? ((scalar @$messages).((@$messages == 1) ? ' message' : ' messages').' (#):') : 'no messages.' if $DEBUG_INFO;
    log_message "# ", join ("\n# ", @$messages) if @$messages;

    if ( !defined($ticket) || !defined($last_ticket{$abbrev}{$name}{$action}) || $ticket ne $last_ticket{$abbrev}{$name}{$action} ) {
	# We don't ever expect to see this condition, but just in case, let's let ourselves know if it does occur.
	# The detail in the secondary log message may tell us a bit about how far out-of-sync the two sides are.
	my $message  = "Disaster Recovery replication received mismatched \"$action\" ticket for $object \"$name\".";
	my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, $message) if $DEBUG_ERROR;
	log_timed_message "NOTICE:  $object \"$name\" returned \"$action\" ticket \"", ($ticket || ''),
	  "\" does not match last local ticket \"", ($last_ticket{$abbrev}{$name}{$action} || ''), '".' if $DEBUG_NOTICE;
	# We don't delete the current ticket, turn off the alarm, or mark the object as stalled, because that would
	# be doing those things for the wrong ticket.  If we previously got a timeout for this incoming ticket, the
	# object state was set to "stalled" then.  We still hope to get a response (or timeout) for the right ticket.
	return;
    }
    log_timed_message "DEBUG:  object_obtain_callback() is deleting ticket \"", ($last_ticket{$abbrev}{$name}{$action} || ''),
      "\" for $abbrev $name $action" if $DEBUG_DEBUG;
    delete $last_ticket{$abbrev}{$name}{$action};

    # FIX THIS:  should this logic still be a part of this callback?
    # Or are we looking in the wrong session's heap for the alarm ID?
    #
    # First, disable the appropriate object $action timeout alarm, so we don't get the timeout later on.
    if (! $_[HEAP]{alarms}{$abbrev}{$name}) {
	# FIX THIS:  this is just to prove to myself that this alarm is actually set here when it should be
	log_timed_message "DEBUG:  There is no alarm to limit the $action of $object \"$name\"." if $DEBUG_DEBUG;
    }
    else {
	# FIX MAJOR:  How is it that we are not seeing the alarm that should have been set before we launched the remote capture operation?
	# FIX THIS:  drop this message once we understand what is going on
	log_timed_message "DEBUG:  Canceling a $action callback timeout alarm for $object \"$name\"." if $DEBUG_DEBUG;
	if (! $_[KERNEL]->alarm_remove ($_[HEAP]{alarms}{$abbrev}{$name}) ) {
	    log_timed_message "ERROR:  Failed to remove a $action callback timeout alarm for $object \"$name\"." if $DEBUG_ERROR;
	}
	# FIX THIS:  in theory, we ought to delete the heap object as well
    }

    if ($status) {
	# The remote operation exited badly.

	my $local_state = fetch_local_state();
	$local_state->{$abbrev}{$name}{active} = 'stalled';
	store_local_state ($local_state);
	# FIX THIS:  implement proper error detection on the save
	save_replication_state ();

	my $message  = "Replication:  Found bad $action status $status for $object \"$name\"; abandoning this sync cycle.";
	my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
    }
    else {
	log_timed_message "INFO:  Found good $action status for $object \"$name\"." if $DEBUG_INFO;
	# FIX MINOR:  also perhaps check here to see if we have interference from any ongoing local stop/deploy/start
	# actions, that might prevent us from executing the next step in this replication cycle
	my $action = $args->{action} = 'stop';
	if (may_not_run_replicate_action($args)) {
	    my $local_state = fetch_local_state();
	    $local_state->{$abbrev}{$name}{active} = 'stalled';
	    store_local_state ($local_state);
	    # FIX THIS:  implement proper error detection on the save
	    save_replication_state ();
	}
	else {
	    # good obtain status, and free to proceed
	    my $script = realpath ("$actions_base/$abbrev/$name/$name.$action");
	    if ($script) {
		if (-x $script) {
		    log_timed_message "FERVID:  Ready to run the $action script for $object \"$name\"." if $DEBUG_FERVID;
		    # Run the $action script as the next step in locally installing the new configuration.
		    # FIX MAJOR:  pass in the $args here, adding/overriding the $action action, $action scripts,
		    # $action timeouts, and $action callback, rather than constructing a new anonymous hash?
		    # Most importantly, add the $action callback here?
		    $_[KERNEL]->yield("${action}_object", {
			object   => $object,
			abbrev   => $abbrev,
			name     => $name,
			action   => $action,
			script   => $script,
			timeouts => $_[HEAP]{$abbrev}{$name}{"${action}-timeouts"},
			callback => "object_${action}_callback"
		    });
		}
		else {
		    my $message  = "Replication:  The $object \"$name\" $action script is not executable.";
		    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
		}
	    }
	    else {
		my $message  = "Replication:  The path to the $object \"$name\" $action script does not exist.";
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
	    }
	}
    }
}

# FIX THIS:  This routine is obsolescent, and it should probably be dropped now.
# Its name should be used instead of local_capture_is_done().
sub object_capture_callback {
    my $args     = $_[ARG0];
    my $status   = $_[ARG1];
    my $messages = $_[ARG2];

    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'
    my $name   = $args->{name};    # app or db name
    my $action = $args->{action};  # action ('capture')

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\"." if $DEBUG_FERVID;

    log_timed_message
      "INFO:  Script to $action $object \"$name\" returned wait status $status [", wait_status_message($status), "] and ",
      @$messages ? ((scalar @$messages).((@$messages == 1) ? ' message' : ' messages').' (#):') : 'no messages.' if $DEBUG_INFO;
    log_message "# ", join ("\n# ", @$messages) if @$messages;

    # FIX THIS:  should this logic still be a part of this callback?
    # First, disable the appropriate object $action timeout alarm, so we don't get the timeout later on.
    if (! $_[HEAP]{alarms}{$abbrev}{$name}) {
	# FIX THIS:  this is just to prove to myself that this alarm is actually set here when it should be
	log_timed_message "DEBUG:  There is no alarm to limit the $action of $object \"$name\"." if $DEBUG_DEBUG;
    }
    elsif (! $_[KERNEL]->alarm_remove ($_[HEAP]{alarms}{$abbrev}{$name}) ) {
	log_timed_message "ERROR:  Failed to remove a capture callback timeout alarm for $object \"$name\"." if $DEBUG_ERROR;
    }

    if ($status) {
	# FIX THIS:  what to do?  probably just set the object inactive or perhaps stalled, then return
	my $message  = "Replication:  Found bad $action status $status for $object \"$name\"; abandoning this sync cycle.";
	my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
    }
    else {
	log_timed_message "INFO:  Found good $action status for $object \"$name\"." if $DEBUG_INFO;
	# FIX THIS:  continue on
	# if (not active or blocked, or we have interference from any ongoing local stop/deploy/start actions) ...
	if (0) {
	    # FIX THIS:  replace this message
	    log_timed_message "NOTICE:  $object \"$name\" is inactive or blocked" if $DEBUG_NOTICE;
	    # FIX THIS:  mark as inactive or stalled (if blocked)?
	}
	else {
	    # good capture status, and free to proceed
	    my $action = 'stop';
	    my $script = realpath ("$actions_base/$abbrev/$name/$name.$action");
	    if ($script) {
		if (-x $script) {
		    log_timed_message "FERVID:  Ready to run the $action script for $object \"$name\"." if $DEBUG_FERVID;
		    # Run the $action script as the next step in locally installing the new configuration.
		    # FIX MAJOR:  pass in the $args here, adding/overriding the $action action, $action scripts,
		    # $action timeouts, and $action callback, rather than constructing a new anonymous hash?
		    # Most importantly, add the $action callback here?
		    $_[KERNEL]->yield("${action}_object", {
			object   => $object,
			abbrev   => $abbrev,
			name     => $name,
			action   => $action,
			script   => $script,
			timeouts => $_[HEAP]{$abbrev}{$name}{"${action}-timeouts"},
			callback => "object_${action}_callback"
		    });
		}
		else {
		    my $message  = "Replication:  The $object \"$name\" $action script is not executable.";
		    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
		}
	    }
	    else {
		my $message  = "Replication:  The path to the $object \"$name\" $action script does not exist.";
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
	    }
	}
    }
}

sub object_stop_callback {
    my $args     = $_[ARG0];
    my $status   = $_[ARG1];
    my $messages = $_[ARG2];

    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'
    my $name   = $args->{name};    # app or db name
    my $action = $args->{action};  # action ('stop')

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\"." if $DEBUG_FERVID;

    log_timed_message
      "INFO:  Script to $action $object \"$name\" returned wait status $status [", wait_status_message($status), "] and ",
      @$messages ? ((scalar @$messages).((@$messages == 1) ? ' message' : ' messages').' (#):') : 'no messages.' if $DEBUG_INFO;
    log_message "# ", join ("\n# ", @$messages) if @$messages;

    if ($status) {
	# The child process exited badly.

	my $local_state = fetch_local_state();
	$local_state->{$abbrev}{$name}{active} = 'stalled';
	store_local_state ($local_state);
	# FIX THIS:  implement proper error detection on the save
	save_replication_state ();

	my $message  = "Replication:  Found bad $action status $status for $object \"$name\"; abandoning this sync cycle.";
	my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
    }
    else {
	log_timed_message "INFO:  Found good $action status for $object \"$name\"." if $DEBUG_INFO;
	# FIX MINOR:  also perhaps check here to see if we have interference from any ongoing local stop/deploy/start
	# actions, that might prevent us from executing the next step in this replication cycle
	my $action = $args->{action} = 'deploy';
	if (may_not_run_replicate_action($args)) {
	    my $local_state = fetch_local_state();
	    $local_state->{$abbrev}{$name}{active} = 'stalled';
	    store_local_state ($local_state);
	    # FIX THIS:  implement proper error detection on the save
	    save_replication_state ();
	}
	else {
	    # good stop status, and free to proceed
	    my $script = realpath ("$actions_base/$abbrev/$name/$name.$action");
	    if ($script) {
		if (-x $script) {
		    log_timed_message "FERVID:  Ready to run the $action script for $object \"$name\"." if $DEBUG_FERVID;
		    # Run the $action script as the next step in locally installing the new configuration.
		    # FIX MAJOR:  pass in the $args here, adding/overriding the $action action, $action scripts,
		    # $action timeouts, and $action callback, rather than constructing a new anonymous hash?
		    # Most importantly, add the $action callback here?
		    $_[KERNEL]->yield("${action}_object", {
			object   => $object,
			abbrev   => $abbrev,
			name     => $name,
			action   => $action,
			script   => $script,
			timeouts => $_[HEAP]{$abbrev}{$name}{"${action}-timeouts"},
			callback => "object_${action}_callback"
		    });
		}
		else {
		    my $message  = "Replication:  The $object \"$name\" $action script is not executable.";
		    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
		}
	    }
	    else {
		my $message  = "Replication:  The path to the $object \"$name\" $action script does not exist.";
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
	    }
	}
    }
}

sub object_deploy_callback {
    my $args     = $_[ARG0];
    my $status   = $_[ARG1];
    my $messages = $_[ARG2];

    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'
    my $name   = $args->{name};    # app or db name
    my $action = $args->{action};  # action ('deploy')

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\"." if $DEBUG_FERVID;

    log_timed_message
      "INFO:  Script to $action $object \"$name\" returned wait status $status [", wait_status_message($status), "] and ",
      @$messages ? ((scalar @$messages).((@$messages == 1) ? ' message' : ' messages').' (#):') : 'no messages.' if $DEBUG_INFO;
    log_message "# ", join ("\n# ", @$messages) if @$messages;

    if ($status) {
	# The child process exited badly.

	my $local_state = fetch_local_state();
	$local_state->{$abbrev}{$name}{active} = 'stalled';
	store_local_state ($local_state);
	# FIX THIS:  implement proper error detection on the save
	save_replication_state ();

	my $message  = "Replication:  Found bad $action status $status for $object \"$name\"; abandoning this sync cycle.";
	my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
    }
    else {
	log_timed_message "INFO:  Found good $action status for $object \"$name\"." if $DEBUG_INFO;
	# FIX MINOR:  also perhaps check here to see if we have interference from any ongoing local stop/deploy/start
	# actions, that might prevent us from executing the next step in this replication cycle
	my $action = $args->{action} = 'start';
	if (may_not_run_replicate_action($args)) {
	    my $local_state = fetch_local_state();
	    $local_state->{$abbrev}{$name}{active} = 'stalled';
	    store_local_state ($local_state);
	    # FIX THIS:  implement proper error detection on the save
	    save_replication_state ();
	}
	else {
	    # good deploy status, and free to proceed
	    my $script = realpath ("$actions_base/$abbrev/$name/$name.$action");
	    if ($script) {
		if (-x $script) {
		    log_timed_message "FERVID:  Ready to run the $action script for $object \"$name\"." if $DEBUG_FERVID;
		    # Run the $action script as the next step in locally installing the new configuration.
		    # FIX MAJOR:  pass in the $args here, adding/overriding the $action action, $action scripts,
		    # $action timeouts, and $action callback, rather than constructing a new anonymous hash?
		    # Most importantly, add the $action callback here?
		    $_[KERNEL]->yield("${action}_object", {
			object   => $object,
			abbrev   => $abbrev,
			name     => $name,
			action   => $action,
			script   => $script,
			timeouts => $_[HEAP]{$abbrev}{$name}{"${action}-timeouts"},
			callback => "object_${action}_callback"
		    });
		}
		else {
		    my $message  = "Replication:  The $object \"$name\" $action script is not executable.";
		    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
		}
	    }
	    else {
		my $message  = "Replication:  The path to the $object \"$name\" $action script does not exist.";
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
	    }
	}
    }
}

sub object_start_callback {
    my $args     = $_[ARG0];
    my $status   = $_[ARG1];
    my $messages = $_[ARG2];

    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'
    my $name   = $args->{name};    # app or db name
    my $action = $args->{action};  # action ('start')

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\"." if $DEBUG_FERVID;

    log_timed_message
      "INFO:  Script to $action $object \"$name\" returned wait status $status [", wait_status_message($status), "] and ",
      @$messages ? ((scalar @$messages).((@$messages == 1) ? ' message' : ' messages').' (#):') : 'no messages.' if $DEBUG_INFO;
    log_message "# ", join ("\n# ", @$messages) if @$messages;

    # FIX THIS:  disable the appropriate object start timeout alarm;
    # either we need to stuff its ID somewhere that we can get to it,
    # or we need to be able to manufacture it from the available data
    # (this note is probably a holdover from when we handled timeouts
    # in this session)

    if ($status) {
	# The child process exited badly.

	my $local_state = fetch_local_state();
	$local_state->{$abbrev}{$name}{active} = 'stalled';
	store_local_state ($local_state);
	# FIX THIS:  implement proper error detection on the save
	save_replication_state ();

	my $message  = "Replication:  Found bad $action status $status for $object \"$name\"; abandoning this sync cycle.";
	my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
    }
    else {
	log_timed_message "INFO:  Found good $action status for $object \"$name\"." if $DEBUG_INFO;
	# Mark the object as inactive.
	my $local_state = fetch_local_state();
	delete $local_state->{$abbrev}{$name}{active};
	store_local_state ($local_state);
	# FIX THIS:  implement proper error detection on the save
	save_replication_state ();
    }
}

sub object_cleanup_callback {
    my $args     = $_[ARG0];
    my $status   = $_[ARG1];
    my $messages = $_[ARG2];

    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'
    my $name   = $args->{name};    # app or db name
    my $action = $args->{action};  # action ('erase' or 'prune')

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called at end of $action for $object \"$name\"." if $DEBUG_FERVID;

    log_timed_message
      "INFO:  Script to $action $object \"$name\" returned wait status $status [", wait_status_message($status), "] and ",
      @$messages ? ((scalar @$messages).((@$messages == 1) ? ' message' : ' messages').' (#):') : 'no messages.' if $DEBUG_INFO;
    log_message "# ", join ("\n# ", @$messages) if @$messages;

    # FIX THIS:  disable the appropriate object cleanup timeout alarm;
    # either we need to stuff its ID somewhere that we can get to it,
    # or we need to be able to manufacture it from the available data
    # (this note is probably a holdover from when we handled timeouts
    # in this session)

    if ($status) {
	# The child process exited badly.

	my $message  = "Replication:  Found bad $action status $status for $object \"$name\"; this cleanup cycle is incomplete.";
	my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
    }
    else {
	log_timed_message "INFO:  Found good $action status for $object \"$name\"." if $DEBUG_INFO;
    }
}

sub obtain_object {
    my $args   = $_[ARG0];
    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'
    my $name   = $args->{name};    # app or db name
    my $action = $args->{action};  # action ('obtain')

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\"." if $DEBUG_FERVID;

    # FIX THIS:  Should we be passing in the timeouts instead of pulling them from the heap here?
    my $timeouts = $_[HEAP]{$abbrev}{$name}{"${action}-timeouts"};

    if ($timeouts->[WARNING_TIMEOUT] > 0) {
	# First set a timeout for obtaining the remote $object data, so we will know when we failed to get what we asked for.
	my $alarm_id = $_[KERNEL]->delay_set ('object_obtain_timeout', $timeouts->[WARNING_TIMEOUT], $args, WARNING_TIMEOUT);
	# Stuff this $alarm_id in the heap in association with this $object,
	# so we can cancel it if we get a response when we hope to.
	$_[HEAP]{alarms}{$abbrev}{$name} = $alarm_id;
    }
    elsif ($timeouts->[ABANDON_TIMEOUT] > 0) {
	# First set a timeout for obtaining the remote $object data, so we will know when we failed to get what we asked for.
	my $alarm_id = $_[KERNEL]->delay_set ('object_obtain_timeout', $timeouts->[ABANDON_TIMEOUT], $args, ABANDON_TIMEOUT);
	# Stuff this $alarm_id in the heap in association with this $object,
	# so we can cancel it if we get a response when we hope to.
	$_[HEAP]{alarms}{$abbrev}{$name} = $alarm_id;
    }

    my $deadline = time();
    if ($timeouts->[ABANDON_TIMEOUT] > 0) {
	$deadline += $timeouts->[ABANDON_TIMEOUT];
    }
    else {
	# If this object is not configured with an abandon timeout, we substitute half the time until the next iteration.
	# That's crude, but we don't have a better algorithmic way to determine a sensible cutoff time.
	my $event_time = next_event_time ('sync', $_[HEAP]{$abbrev}{$name}{'sync-period'}, $_[HEAP]{$abbrev}{$name}{'sync-phase'});
	if ($event_time) {
	    my $cutoff = ($event_time - $deadline) / 2;
	    $deadline += $cutoff;
	}
	else {
	    # In the absence of any other way to determine the deadline, we default to an hour from now.
	    $deadline += $seconds_per_hour;
	}
    }

    # FIX THIS:  Should we issue a log message if $last_ticket{$abbrev}{$name}{$action} is not undefined before we assign to it?
    my $ticket = $last_ticket{$abbrev}{$name}{$action} = next_ticket();
    log_timed_message "DEBUG:  obtain_object() is assigning ticket \"", ($last_ticket{$abbrev}{$name}{$action} || ''),
      "\" for $abbrev $name $action" if $DEBUG_DEBUG;
    $action = 'capture';  # override the incoming value
    $_[KERNEL]->post('IKC', 'post',
      "poe://$unqualified_remote_hostname-heartbeat-server/$unqualified_remote_hostname-sync/remote_capture_object",
	{
	object   => $object,
	abbrev   => $abbrev,
	name     => $name,
	action   => $action,
	deadline => $deadline,
	ticket   => $ticket,
	callback => "poe://$unqualified_local_hostname-heartbeat-server/$unqualified_local_hostname-sync/remote_capture_callback"
	}
    );
}

sub capture_object {
    my $args     = $_[ARG0];
    my $object   = $args->{object};    # 'application' or 'database'
    my $abbrev   = $args->{abbrev};    # 'app' or 'db'
    my $name     = $args->{name};      # app or db name
    my $action   = $args->{action};    # action ('capture')
    my $ticket   = $args->{ticket};    # ticket for current-request verification
    my $script   = $args->{script};    # capture script pathname
    my $timeouts = $args->{timeouts};  # arrayref to [ warning, sigterm, sigkill ] timeouts
    my $callback = $args->{callback};  # callback event, to be triggered when the child process is finally done

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\", to run $script" if $DEBUG_FERVID;

    # We pass these parameters to the child process:
    #
    # application:
    # {staged_path} {replica_machine} {replica_user} {replica_path} {working_path} {copy_pattern} ...
    #
    # database:
    # {staged_path} {replica_machine} {replica_user} {replica_path} {dump_command}
    my @params = ();

    my $staged_path  = "$backups_config_base_dir/$abbrev/$name/staged";
    my $replica_path = "$pending_config_base_dir/$abbrev/$name/replica";

    if ($object eq 'application') {
	my $copy_patterns = $config->{$abbrev}{$name}{'application-replication-patterns'};
	push @params, $staged_path;
	push @params, $replica_machine;
	push @params, $replica_user;
	push @params, $replica_path;
	push @params, $working_path;
	push @params, @$copy_patterns;
    }
    elsif ($object eq 'database') {
	my $dump_command      = $config->{$abbrev}{$name}{'database-dump-command'};
	my $local_dump_prefix = "$unqualified_local_hostname.$name";
	push @params, $staged_path;
	push @params, $replica_machine;
	push @params, $replica_user;
	push @params, $replica_path;
	push @params, $dump_command;
	push @params, $local_dump_prefix;
    }

    # Fork the child process that will run the capture script.  When it's done, the callback
    # will be called with an $args hashref, plus a $status scalar and a \@messages arrayref.
    $_[KERNEL]->yield('run_child_process', $args, \@params);
}

# FIX THIS:  I'd like to collapse these action_object routines together, in some kind of FA,
# so the common code can be eliminated.  But that will await a future version.  For the time
# being, we're leaving these routines around as standalone functions rather than folding them
# into the few places they are called, mostly because we suspect that in a future version, we
# will want to have other (user-initiated) direct calls to them in order to unstick objects
# which failed during a previous sync operation.

sub stop_object {
    my $args     = $_[ARG0];
    my $object   = $args->{object};    # 'application' or 'database'
    my $abbrev   = $args->{abbrev};    # 'app' or 'db'
    my $name     = $args->{name};      # app or db name
    my $action   = $args->{action};    # action ('stop')
    my $script   = $args->{script};    # stop script pathname
    my $timeouts = $args->{timeouts};  # arrayref to [ warning, sigterm, sigkill ] timeouts
    my $callback = $args->{callback};  # callback event, to be triggered when the child process is finally done

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\", to run $script" if $DEBUG_FERVID;

    # Fork the child process that will run the stop script.  When it's done, the callback
    # will be called with an $args hashref, plus a $status scalar and a \@messages arrayref.
    $_[KERNEL]->yield('run_child_process', $args);
}

sub deploy_object {
    my $args     = $_[ARG0];
    my $object   = $args->{object};    # 'application' or 'database'
    my $abbrev   = $args->{abbrev};    # 'app' or 'db'
    my $name     = $args->{name};      # app or db name
    my $action   = $args->{action};    # action ('deploy')
    my $script   = $args->{script};    # deploy script pathname
    my $timeouts = $args->{timeouts};  # arrayref to [ warning, sigterm, sigkill ] timeouts
    my $callback = $args->{callback};  # callback event, to be triggered when the child process is finally done

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\", to run $script" if $DEBUG_FERVID;

    # We pass these parameters to the child process:
    #
    # application:
    # {replica_path} {ready_path} {erase_path} {shadow_path} {working_path} {copy_pattern} ...
    #
    # database:
    # {replica_path} {ready_path} {erase_path} {shadow_path} {dump_command} {load_command}
    my @params = ();

    my $replica_path = "$pending_config_base_dir/$abbrev/$name/replica";
    my $ready_path   = "$pending_config_base_dir/$abbrev/$name/ready";
    my $erase_path   = "$pending_config_base_dir/$abbrev/$name/erase";
    my $shadow_path  = "$backups_config_base_dir/$abbrev/$name/shadow";

    if ($object eq 'application') {
	my $copy_patterns = $config->{$abbrev}{$name}{'application-replication-patterns'};
	push @params, $replica_path;
	push @params, $ready_path;
	push @params, $erase_path;
	push @params, $shadow_path;
	push @params, $working_path;
	push @params, @$copy_patterns;
    }
    elsif ($object eq 'database') {
	my $dump_command       = $config->{$abbrev}{$name}{'database-dump-command'};
	my $local_dump_prefix  = "$unqualified_local_hostname.$name";
	my $remote_dump_prefix = "$unqualified_remote_hostname.$name";
	my $load_command       = $config->{$abbrev}{$name}{'database-load-command'};
	push @params, $replica_path;
	push @params, $ready_path;
	push @params, $erase_path;
	push @params, $shadow_path;
	push @params, $dump_command;
	push @params, $local_dump_prefix;
	push @params, $remote_dump_prefix;
	push @params, $load_command;
    }

    # Fork the child process that will run the deploy script.  When it's done, the callback
    # will be called with an $args hashref, plus a $status scalar and a \@messages arrayref.
    $_[KERNEL]->yield('run_child_process', $args, \@params);
}

sub start_object {
    my $args     = $_[ARG0];
    my $object   = $args->{object};    # 'application' or 'database'
    my $abbrev   = $args->{abbrev};    # 'app' or 'db'
    my $name     = $args->{name};      # app or db name
    my $action   = $args->{action};    # action ('start')
    my $script   = $args->{script};    # start script pathname
    my $timeouts = $args->{timeouts};  # arrayref to [ warning, sigterm, sigkill ] timeouts
    my $callback = $args->{callback};  # callback event, to be triggered when the child process is finally done

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\", to run $script" if $DEBUG_FERVID;

    # Fork the child process that will run the start script.  When it's done, the callback
    # will be called with an $args hashref, plus a $status scalar and a \@messages arrayref.
    $_[KERNEL]->yield('run_child_process', $args);
}

sub cull_object {
    my $args     = $_[ARG0];
    my $object   = $args->{object};    # 'application' or 'database'
    my $abbrev   = $args->{abbrev};    # 'app' or 'db'
    my $name     = $args->{name};      # app or db name
    my $action   = $args->{action};    # action ('erase' or 'prune')
    my $script   = $args->{script};    # cleanup script pathname
    my $timeouts = $args->{timeouts};  # arrayref to [ warning, sigterm, sigkill ] timeouts
    my $callback = $args->{callback};  # callback event, to be triggered when the child process is finally done

    my $function = (caller(0))[3];
    log_timed_message "FERVID:  $function() has been called for $action of $object \"$name\"." if $DEBUG_FERVID;

    # We pass these parameters to the child process:
    #
    # erase:
    # {erase_path}
    #
    # prune:
    # {prune_path} {min_kept_count} {min_kept_age}
    my @params = ();

    my $erase_path = "$pending_config_base_dir/$abbrev/$name/erase";
    my $prune_path = "$backups_config_base_dir/$abbrev/$name/prune";

    if ($action eq 'erase') {
	push @params, $erase_path;
    }
    elsif ($action eq 'prune') {
	my $min_kept_count = $_[HEAP]{$abbrev}{$name}{'min-kept-backups'};
	my $min_kept_age   = $_[HEAP]{$abbrev}{$name}{'min-backup-age'};
	push @params, $prune_path;
	push @params, $min_kept_count;
	push @params, $min_kept_age;
    }

    # Fork the child process that will run the cleanup script.  When it's done, the callback
    # will be called with an $args hashref, plus a $status scalar and a \@messages arrayref.
    $_[KERNEL]->yield('run_child_process', $args, \@params);
}

sub object_obtain_timeout {
    my $args         = $_[ARG0];
    my $timeout_type = $_[ARG1];  # WARNING_TIMEOUT or ABANDON_TIMEOUT

    my $object = $args->{object};    # 'application' or 'database'
    my $abbrev = $args->{abbrev};    # 'app' or 'db'
    my $name   = $args->{name};      # app or db name
    my $action = $args->{action};    # action ('obtain')

    if ($timeout_type == WARNING_TIMEOUT) {
	my $message  = "Replication:  Data capture for $object \"$name\" is taking a long time.";
	my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, $message);
	my $timeouts = $_[HEAP]{$abbrev}{$name}{"${action}-timeouts"};
	my $timeout  = $timeouts->[$timeout_type];
	if ($timeouts->[ABANDON_TIMEOUT] > 0) {
	    # First set a timeout for obtaining the remote $object data, so we will know when we failed to get what we asked for.
	    my $alarm_id = $_[KERNEL]->delay_set ('object_obtain_timeout', $timeouts->[ABANDON_TIMEOUT] - $timeout,
	      $args, ABANDON_TIMEOUT);
	    # Stuff this $alarm_id in the heap in association with this $object,
	    # so we can cancel it if we get a response when we hope to.
	    # FIX THIS:  make sure this alarm gets cancelled if we do get a timely response from the remote server
	    $_[HEAP]{alarms}{$abbrev}{$name} = $alarm_id;
	}
    }
    else {
	# Anything but a WARNING_TIMEOUT is treated as an ABANDON_TIMEOUT without explicit checking.
	my $message  = "Replication:  Remote data capture for $object \"$name\" timed out; abandoning this sync cycle.";
	my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);

	# Do whatever is necessary to abandon the obtain operation on the local machine, while allowing future obtain
	# operations to begin and not be inhibited by believing that the old operation is still active.  Arrange so that
	# if we do subsequently get a callback from the remote machine for this obtain operation, it will be ignored.

	my $local_state = fetch_local_state();
	$local_state->{$abbrev}{$name}{active} = 'stalled';
	store_local_state ($local_state);
	# FIX THIS:  implement proper error detection on the save
	save_replication_state ();

	log_timed_message "DEBUG:  object_obtain_timeout() is deleting ticket \"", ($last_ticket{$abbrev}{$name}{$action} || ''),
	  "\" for $abbrev $name $action" if $DEBUG_DEBUG;
	delete $last_ticket{$abbrev}{$name}{$action};
    }
}

# FIX MAJOR:  most or all of this routine should be eliminated now, its functionality having migrated elsewhere
# (I believe this routine is now orphaned). clean this up now, transferring the non-migrated text to where it belongs
sub object_stop_timeout {
    my $object = $_[ARG0];  # 'application' or 'database'
    my $abbrev = $_[ARG1];  # 'app' or 'db'
    my $name   = $_[ARG2];  # app or db name
    my $action = 'stop';    # FIX THIS:  make it part of the incoming arguments

    # We block SIGCHLD for the duration of this routine, using sigprocmask, to guarantee
    # that we get a consistent view of the child processes and whether they exist or not
    # (whether alive or dead or in transition, at least the process ID of the child
    # process won't be able to be reassigned during this period, since the zombie process
    # won't be reaped during this period).
    # Note that the implementation here presumes we are running a single-threaded process,
    # so sigprocmask() can be called.  Otherwise, only pthread_sigmask() would necessarily
    # be valid, but then calling it for a single thread probably would not be sufficient
    # to provide the protection we're looking for.
    my $oldblockset = POSIX::SigSet->new;
    my $newblockset = POSIX::SigSet->new(SIGCHLD);
    sigprocmask(SIG_BLOCK, $newblockset, $oldblockset) or die "FATAL:  Could not block SIGCHLD ($!),";

    # Note that the timeout processing here is not 100% accurate.  If the child process had not
    # started before the timeout routine was called (say, because the system was so busy that it
    # did not have time to spawn the child process, then this timeout will come and go without
    # doing anything of value, and then the child process will run afterward.
    # FIX THIS:  detect and deal with that situation

    # FIX THIS
    log_timed_message "FIX THIS:  OLD:  ERROR:  \u$object stopping for \"$name\" timed out." if $DEBUG_ERROR;
    # FIX THIS:  The purpose of this timeout routine is to cancel any existing
    # child process which is running the application stop script, in such a way
    # that it will return an error status that the parent process (the stop_object
    # event here) will receive that status and act accordingly.  Probably that
    # means we need to reach into the heap to find its PID, and send it a SIGTERM.

    # FIX THIS:  We're seeing an undefined value here while we know the child process is
    # still running, on the first invocation of this timeout for any {object, name}.  Why?
    # Also, on a subsequent invocation of the timeout, this value is now defined as a hashref,
    # but that probably happens during auto-vivification during the $child reference just below,
    # rather than when the child process is created.  So the value does not really represent
    # what we need it to.
    log_timed_message "FIX THIS:  OLD:  FERVID:  children_by_object value for $object \"$name\" is:  ", $_[HEAP]{children_by_object} if $DEBUG_FERVID;

    my $child = $_[HEAP]{children_by_object}{$object}{$name};
    log_timed_message "FIX THIS:  OLD:  FERVID:  object_stop_timeout child is: ", $child if $DEBUG_FERVID;
    if ($child) {
	# I really, really hate sending signals to other processes in an environment
	# where processes may come and go continuously from this or other applications.
	# Trouble is, if you only have the Process ID to go by, you can never be 100%
	# sure that the process you're signaling hasn't been replaced by some other
	# process that is now re-using the same PID.  We do our best here to close that
	# window of uncertainty, by insisting that we only send a signal if we believe
	# the other process is our direct child.  There is still a window of vulnerability
	# (race condition) between the parentage test and the kill; unfortunately, UNIX
	# doesn't seem to provide any kind of atomic operation that would include both
	# actions.  Still, we will at least take advantage of whatever facilities are
	# available, and hope that the kernel doesn't re-use PIDs for a long time after
	# they are freed up.
	if (child_is_alive($child->PID)) {
	    log_timed_message "FIX THIS:  OLD:  NOTICE:  Killing child process group that is trying to $action $object \"$name\":  ", $child->PID
	      if $DEBUG_NOTICE;
	    # We don't call $child->kill(); because that would only kill the direct
	    # child script, not also whatever processes it may have spawned.  In the
	    # presence of a timeout, we want instead to kill the entire process group.
	    # Note that the perlfunc documentation for kill suggests that negating the
	    # PID is non-portable, but then the perlipc documentation ignores that and
	    # recommends the practice.  The underlying kill(2) call in both Linux and
	    # Solaris supports it, so we use that here rather then going through a
	    # complicated dance with %Config to map a signal name to a signal number
	    # so we can negate it.
	    my $count = kill 'TERM', 0 - $child->PID;
	    log_timed_message "FIX THIS:  OLD:  DEBUG:  $count processes were sent a SIGTERM signal." if $DEBUG_DEBUG;
	}
    }
    # We don't delete the $child object from the heap now, leaving that cleanup
    # for the child termination processing.

    # Restore the old signal mask so we can reap zombies once again.
    sigprocmask(SIG_SETMASK, $oldblockset) or die "FATAL:  Could not restore SIGCHLD signal ($!),";
}

# Turn an object (application or database) name (and optionally, an action name) into a state name in a consistent fashion.
sub object_state_name {
    my $object_type = shift;
    my $object_name = shift;
    my $action_name = shift;

    my $state_name = "${unqualified_local_hostname}-${object_type}-${object_name}";
    $state_name .= "-${action_name}" if defined $action_name;
    return $state_name;
}

# This routine can be called at any stage of a sync action to see if it should continue.
sub may_not_run_replicate_action {
    my $args   = shift;
    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'
    my $name   = $args->{name};    # app or db name
    my $action = $args->{action};  # action ('obtain', 'stop', 'deploy', 'start', etc.)

    my @response = ();

    my $local_state  = fetch_local_state();
    my $remote_state = fetch_remote_state();

    my $remote_state_age = defined($remote_state->{'state_time'}) ? (time() - $remote_state->{'state_time'}) : undef;
    my $remote_state_is_current = !$got_remote_state_timeout
	&& defined($remote_state_age)
	&& $remote_state_age < $config->{'contact-heartbeat-period'} * 3;

    # Check to see if all the conditions needed for running a sync action step currently hold.
    # (*) This system is not operating in failure mode.
    # (*) This system is operating as a Slave.
    # (*) This particular object's action is not blocked.

    # To keep the log informative as to why we sometimes don't run replication actions at their
    # regularly scheduled times, we test each condition separately and log the first test that fails.

    if ( $local_state->{in_failure_mode} ) {
	my @message = fill_text (
	    "Not running $action action for $object \"$name\" because ",
	    "\"$unqualified_local_hostname\" is currently in failure mode."
	);
	my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	push @response, @$response;
    }
    # We only initiate replications from a Slave system, which we define as one not having
    # Master Configuration Authority but in conversation with a paired system that does.
    elsif ( !$remote_state_is_current
      || !$remote_state->{has_master_configuration_authority}
      ||   $local_state->{has_master_configuration_authority}) {
	my @message = fill_text (
	    "Not running $action action for $object \"$name\" because ",
	    "\"$unqualified_local_hostname\" is not operating in Slave mode."
	);
	my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	push @response, @$response;
    }
    # Replication on this side can formally only be blocked from this side, to allow locally-unblocked deploy
    # actions to proceed.  But we complain about blocking from the other side to make sure that blocking on
    # the DR side shows up in the Primary side's Event Console, which is where such warnings are likely to be
    # noticed.  Note that we only check this (we only get this far) if the remote state is current.
    elsif ( $local_state->{$abbrev}{$name}{blocked} ) {
	my @message = fill_text (
	    "Not running $action action for $object \"$name\" because replication for this object is blocked."
	);
	my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	push @response, @$response;
    }
    elsif ( $remote_state->{$abbrev}{$name}{blocked} ) {
	my $message  = "Replication for $object \"$name\" is remotely blocked (on \"$unqualified_remote_hostname\").";
	my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, $message);
    }

    return @response;
}

# We return \@response from this routine even though in its ordinary calling context
# as an asynchronous event handler, the return value will be ignored.  This is to
# provide for when it is called synchronously in an interactive user command.

sub sync_object {
    my $args   = $_[ARG0];
    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'
    my $name   = $args->{name};    # app or db name
    my $action = $args->{action};  # action ('obtain')

    my @response = ();

    my $local_state = fetch_local_state();

    # Check to see if all the requisite conditions hold before we begin this replication cycle.
    #
    # We couldn't include this test in may_not_run_replicate_action() because that routine will be called
    # for all subsequent steps of replication, at which time this object *should* be marked as active.
    if (defined( $local_state->{$abbrev}{$name}{active} ) && $local_state->{$abbrev}{$name}{active} eq 'active') {
	my @message = fill_text (
	    "Not running $action action for $object \"$name\" because ",
	    "replication for this object is still active from some previous cycle."
	);
	my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
	push @response, @$response;
    }
    else {
	# Set the default state for this object.  This will clear any stalled state.
	# This default state will be overridden if we actually start the replication action.
	$local_state->{$abbrev}{$name}{active} = 'skipped';
	# Record the last time at which we needed to make a decision as to whether to run a replication cycle
	# for this object, regardless of whether nor not we actually decided to do so.
	$local_state->{$abbrev}{$name}{last_sync_time} = time();
	# FIX MINOR:  should we test $subscribed_to_remote_heartbeat_server here instead?
	if (not $connected_to_remote_heartbeat_server) {
	    my @message = fill_text (
		"Not running $action action for $object \"$name\" because ",
		"we are not connected to the \"$unqualified_remote_hostname\" remote Replication Engine."
	    );
	    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
	    push @response, @$response;
	    # FIX MAJOR:  This is a potential fly in the ointment for replication.  If the remote system
	    # happens to be briefly down at the moment when an infrequent replication cycle is supposed
	    # to be down, then it will need to wait an entire cycle for another chance.  If a DR event
	    # happens in the meantime, the DR system could be significantly out of step with the Primary
	    # when it comes time to take over.  So, should we reschedule replication for some time earlier
	    # than usual in this case, hoping that we will be connected at that time?  That could be hard
	    # to get right, so we know we're not interfering with other unknown system activity, unless we
	    # explicitly specify a second-chance delay in the configuration file.  Or should we just note
	    # this circumstance and schedule replication more frequently than we otherwise might?
	}
	elsif (my @message = may_not_run_replicate_action($args)) {
	    push @response, @message;
	}
	else {
	    $local_state->{$abbrev}{$name}{active} = 'active';

	    log_timed_message "STATS:  Running sync for $object \"$name\"." if $DEBUG_STATS;
	    # Ask the remote server to capture the $object status and forward it to us.
	    $_[KERNEL]->yield('obtain_object', { object => $object, abbrev => $abbrev, name => $name, action => $action, });
	    push @response, "Starting replication for $object \"$name\".";
	}

	store_local_state ($local_state);
	# FIX THIS:  implement proper error detection on the save
	save_replication_state ();
    }

    # This rescheduling is for the next configured sync time, not to continue actions within the
    # present sync operation, if we in fact started one just above.  The next actions for that
    # will be triggered by the remote_capture_object() call or an associated callback timeout.
    my $obj_state_name = object_state_name($object, $name);
    my $event_time = next_event_time ('sync', $_[HEAP]{$abbrev}{$name}{'sync-period'}, $_[HEAP]{$abbrev}{$name}{'sync-phase'});
    my $phase_times = (ref $_[HEAP]{$abbrev}{$name}{'sync-phase'} eq 'ARRAY') ?
      ( '[' . join( ', ', @{ $_[HEAP]{$abbrev}{$name}{'sync-phase'} } ) . ']' ) : $_[HEAP]{$abbrev}{$name}{'sync-phase'};
    log_timed_message
      "STATS:  Rescheduling $obj_state_name for $_[HEAP]{$abbrev}{$name}{'sync-period'} $phase_times at ",
      scalar localtime $event_time if $DEBUG_STATS;
    $_[KERNEL]->alarm ($obj_state_name, $event_time, { object => $object, abbrev => $abbrev, name => $name, action => $action, });

    return \@response;
}

sub cleanup_object {
    my $args   = $_[ARG0];
    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'
    my $name   = $args->{name};    # app or db name
    my $action = $args->{action};  # action ('erase' or 'prune')

    log_timed_message "STATS:  Running $action for $object \"$name\"." if $DEBUG_STATS;

    # We don't currently implement any way to block cleanup actions, so there's no need to test for that here.
    # I suppose we might in the future block cleanup if replication for the underlying object is blocked, but
    # we'll think about that for some future release.

    my $cleanup_script =
	($action eq 'erase') ? "$scripts_path/erase_readies_obj" :
	($action eq 'prune') ? "$scripts_path/prune_backups_obj" :
	undef;
    if (defined $cleanup_script) {
	my $script = realpath ($cleanup_script);
	if ($script) {
	    if (-x $script) {
		log_timed_message "FERVID:  Ready to run the $action script for $object \"$name\"." if $DEBUG_FERVID;
		# Run the $action script to clean up the object.
		# FIX MAJOR:  pass in the $args here, adding/overriding the $action action, $action scripts,
		# $action timeouts, and $action callback, rather than constructing a new anonymous hash?
		# Most importantly, add the $action callback here?
		$_[KERNEL]->yield('cull_object', {
		    object   => $object,
		    abbrev   => $abbrev,
		    name     => $name,
		    action   => $action,
		    script   => $script,
		    timeouts => $_[HEAP]{$abbrev}{$name}{'cleanup-timeouts'},
		    callback => 'object_cleanup_callback'
		});
	    }
	    else {
		my $message  = "Replication:  The $object \"$name\" $action script is not executable.";
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
	    }
	}
	else {
	    my $message  = "Replication:  The path to the $object \"$name\" $action script does not exist.";
	    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
	}
    }
    else {
	my $message  = "Replication:  Unknown cleanup action \"$action\" called for $object \"$name\".";
	my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, $message);
    }

    # This rescheduling is for the next configured $action time, not to continue actions within the present $action operation.
    my $obj_state_name = object_state_name($object, $name, $action);
    my $event_time = next_event_time ('cleanup', $_[HEAP]{$abbrev}{$name}{'cleanup-period'}, $_[HEAP]{$abbrev}{$name}{'cleanup-phase'});
    my $phase_times = (ref $_[HEAP]{$abbrev}{$name}{'cleanup-phase'} eq 'ARRAY') ?
      ( '[' . join( ', ', @{ $_[HEAP]{$abbrev}{$name}{'cleanup-phase'} } ) . ']' ) : $_[HEAP]{$abbrev}{$name}{'cleanup-phase'};
    log_timed_message
      "STATS:  Rescheduling $obj_state_name for $_[HEAP]{$abbrev}{$name}{'cleanup-period'} $phase_times at ",
      scalar localtime $event_time if $DEBUG_STATS;
    $_[KERNEL]->alarm ($obj_state_name, $event_time, { object => $object, abbrev => $abbrev, name => $name, action => $action, });
}

sub initialize_sync_session {
    foreach (['application', 'app'], ['database', 'db']) {
	my $object = $_->[0];  # 'application' or 'database'
	my $abbrev = $_->[1];  # 'app' or 'db'
	foreach my $obj_config (@{ $config->{"${object}s"} }) {
	    my $name = $obj_config->{"${object}-name"};

	    # We save the scheduling parameters in the heap so we don't have
	    # to crawl the configuration array again every time we reschedule.
	    $_[HEAP]{$abbrev}{$name}{'sync-period'}      = $obj_config->{'sync-period'};
	    $_[HEAP]{$abbrev}{$name}{'sync-phase'}       = $obj_config->{'sync-phase'};
	    $_[HEAP]{$abbrev}{$name}{'obtain-timeouts'}  = $obj_config->{'obtain-timeouts'};
	    $_[HEAP]{$abbrev}{$name}{'capture-timeouts'} = $obj_config->{'capture-timeouts'};
	    $_[HEAP]{$abbrev}{$name}{'stop-timeouts'}    = $obj_config->{'stop-timeouts'};
	    $_[HEAP]{$abbrev}{$name}{'deploy-timeouts'}  = $obj_config->{'deploy-timeouts'};
	    $_[HEAP]{$abbrev}{$name}{'start-timeouts'}   = $obj_config->{'start-timeouts'};
	}
    }
    $_[KERNEL]->post('IKC', 'publish', "$unqualified_local_hostname-sync", [qw(remote_capture_object remote_capture_callback)]);
}

# Here we create a new state in this session for each unblocked object (application or database),
# and we schedule the first sychronization action for that object.  Subsequent scheduling will be
# handled by that sychronization action (rescheduling for the next invocation of the action).
# Scheduling an object action here says nothing about whether the actual invocation will have any
# effect when it runs.  At each stage of execution, the action needs to verify that it is not
# blocked, it is not already active, the system is not operating in failure mode, and this system
# is operating as a Slave.  If any of those conditions are not satisfied, then subsequent stages
# of the action processing should simply be aborted.  The top-level sync operation should continue
# to be rescheduled at the normal execution times even when some conditions are not satisfied, so
# the set of conditions can be tested again at regular intervals to see if replication actions
# should start or resume.
#
# There are some irreconcilable differences between handling of applications and databases,
# like the desire to support continuous database replication, not just periodic replication.
# So a future version of this routine may need to distinguish the object types and run
# certain code which is specific to each type.

sub object_replication_startup {
    my $args   = $_[ARG0];
    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'
    my $action = 'obtain';

    my $local_state = fetch_local_state();

    # We define states whether this is a Master or Slave system, because we don't know how the roles might change
    # back and forth in the future, and we won't be back here then.  And we schedule the first invocation of each
    # state regardless of whether it will make sense to run a replication cycle locally at that time.  That decision
    # will be made when that time rolls around, based on conditions as they stand then, and further replication
    # cycles will be scheduled automatically after that.  Also, replication action chains will be aborted early
    # if some step fails or if we sense that conditions have changed after a successful step, so there will be
    # no need to take extraordinary out-of-band effort to keep replication operating only when it should be.

    foreach my $obj_config (@{ $config->{"${object}s"} }) {
	my $name = $obj_config->{"${object}-name"};

	# We save the scheduling parameters in the heap so we don't have
	# to crawl the configuration array again every time we reschedule.
	$_[HEAP]{$abbrev}{$name}{'sync-period'}      = $obj_config->{'sync-period'};
	$_[HEAP]{$abbrev}{$name}{'sync-phase'}       = $obj_config->{'sync-phase'};
	$_[HEAP]{$abbrev}{$name}{'obtain-timeouts'}  = $obj_config->{'obtain-timeouts'};
	$_[HEAP]{$abbrev}{$name}{'capture-timeouts'} = $obj_config->{'capture-timeouts'};
	$_[HEAP]{$abbrev}{$name}{'stop-timeouts'}    = $obj_config->{'stop-timeouts'};
	$_[HEAP]{$abbrev}{$name}{'deploy-timeouts'}  = $obj_config->{'deploy-timeouts'};
	$_[HEAP]{$abbrev}{$name}{'start-timeouts'}   = $obj_config->{'start-timeouts'};

	# Forcibly set the object as inactive, in case we have a leftover "active" state from the last previous run
	# that would now interfere with running additional replication cycles.  Theoretically, we might have orphaned
	# some child replication processes when we last stopped, without shutting them down cleanly.  Or they might
	# even be still running now, but there's not much we can do about that, so we'll just ignore the possibility.
	delete $local_state->{$abbrev}{$name}{active};

	# Create a state for this $object, and schedule it.
	my $obj_state_name = object_state_name($object, $name);
	$_[KERNEL]->state($obj_state_name => sub { sync_object (@_); });
	my $event_time = next_event_time ('sync', $_[HEAP]{$abbrev}{$name}{'sync-period'}, $_[HEAP]{$abbrev}{$name}{'sync-phase'});
	my $phase_times = (ref $_[HEAP]{$abbrev}{$name}{'sync-phase'} eq 'ARRAY') ?
	  ( '[' . join( ', ', @{ $_[HEAP]{$abbrev}{$name}{'sync-phase'} } ) . ']' ) : $_[HEAP]{$abbrev}{$name}{'sync-phase'};
	log_timed_message
	  "STATS:  Scheduling $obj_state_name for $_[HEAP]{$abbrev}{$name}{'sync-period'} $phase_times at ",
	  scalar localtime $event_time if $DEBUG_STATS;
	$_[KERNEL]->alarm ($obj_state_name, $event_time, { object => $object, abbrev => $abbrev, name => $name, action => $action, });
    }

    store_local_state ($local_state);
    # FIX THIS:  implement proper error detection on the save
    save_replication_state ();

    $initialized{$object} = 1;
}

sub object_replication_shutdown {
    my $args   = $_[ARG0];
    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'

    my $local_state = fetch_local_state();

    foreach my $obj_config (@{ $config->{"${object}s"} }) {
	my $name = $obj_config->{"${object}-name"};

	# FIX THIS:  If the object is currently active, take some trouble to find out what is happening
	# and kill it (e.g., kill a child process group), as long as we don't think that could leave the
	# system in a possibly unstable state (e.g., partway through restoring a database).

	# Forcibly set the object as inactive, so that should be the clean last-saved state seen again on startup.
	delete $local_state->{$abbrev}{$name}{active};

	# Kill any outstanding alarm for this object.
	my $obj_state_name = object_state_name($object, $name);
	$_[KERNEL]->alarm ($obj_state_name);
    }

    store_local_state ($local_state);
    # FIX THIS:  implement proper error detection on the save
    save_replication_state ();
}

sub object_cleanup_startup {
    my $args   = $_[ARG0];
    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'

    foreach my $obj_config (@{ $config->{"${object}s"} }) {
	my $name = $obj_config->{"${object}-name"};

	# We save the scheduling and operating parameters in the heap so we don't
	# have to crawl the configuration array again every time we reschedule.
	$_[HEAP]{$abbrev}{$name}{'cleanup-period'}   = $obj_config->{'cleanup-period'};
	$_[HEAP]{$abbrev}{$name}{'cleanup-phase'}    = $obj_config->{'cleanup-phase'};
	$_[HEAP]{$abbrev}{$name}{'cleanup-timeouts'} = $obj_config->{'cleanup-timeouts'};
	$_[HEAP]{$abbrev}{$name}{'min-kept-backups'} = $obj_config->{'min-kept-backups'};
	$_[HEAP]{$abbrev}{$name}{'min-backup-age'}   = $obj_config->{'min-backup-age'};

	# For now, we don't implement any user control to block cleanup actions, so we don't need to test for that here.

	# Create states for this $object, and schedule them.  We schedule erase and prune operations separately;
	# there's no point in trying to cascade them, since they will run serially anyway.
	foreach my $action ('erase', 'prune') {
	    my $obj_state_name = object_state_name($object, $name, $action);
	    $_[KERNEL]->state($obj_state_name => sub { cleanup_object (@_); });
	    my $event_time = next_event_time ('cleanup', $_[HEAP]{$abbrev}{$name}{'cleanup-period'}, $_[HEAP]{$abbrev}{$name}{'cleanup-phase'});
	    my $phase_times = (ref $_[HEAP]{$abbrev}{$name}{'cleanup-phase'} eq 'ARRAY') ?
	      ( '[' . join( ', ', @{ $_[HEAP]{$abbrev}{$name}{'cleanup-phase'} } ) . ']' ) : $_[HEAP]{$abbrev}{$name}{'cleanup-phase'};
	    log_timed_message
	      "STATS:  Scheduling $obj_state_name for $_[HEAP]{$abbrev}{$name}{'cleanup-period'} $phase_times at ",
	      scalar localtime $event_time if $DEBUG_STATS;
	    $_[KERNEL]->alarm ($obj_state_name, $event_time, { object => $object, abbrev => $abbrev, name => $name, action => $action, });
	}
    }
}

sub object_cleanup_shutdown {
    my $args   = $_[ARG0];
    my $object = $args->{object};  # 'application' or 'database'
    my $abbrev = $args->{abbrev};  # 'app' or 'db'

    foreach my $obj_config (@{ $config->{"${object}s"} }) {
	my $name = $obj_config->{"${object}-name"};

	# FIX THIS:  If the object action is currently active, take some trouble to find out what is happening
	# and kill it (e.g., kill a child process group), as long as we don't think that could leave the system
	# in a possibly unstable state.  My guess is that cleanup actions have been carefully defined in a safe
	# manner in that respect (they can be asynchronously interrupted, and later cleanup cycles will be able
	# to cope with whatever partial mess was left behind).

	foreach my $action ('erase', 'prune') {
	    # Kill any outstanding alarm for this object action.
	    my $obj_state_name = object_state_name($object, $name, $action);
	    $_[KERNEL]->alarm ($obj_state_name);
	}
    }
}

# The purpose of the analysis below is partly to decide when to arrogate and relinquish
# Notification Authority, and partly to control replication actions.
#
# The analyze() routine is invoked on a schedule just like the heartbeats.  The two cannot
# be combined, so analysis is only done when we obtain new state from the remote side,
# in synchrony with attempts to do so.  That's because while the heartbeat gets triggered
# every so often to fetch the remote state on a regular basis, that just causes a request
# for the remote system to return its state.  The actual receipt of that state will be
# an asynchronous event which might time out instead of completing.  So eventually, this
# analyze() routine must effectively be triggered both at the completion of receiving
# remote state and at timeouts which occur because we did not receive the remote state
# when we thought we would.  Consequently, we fire this off on a regular basis and pretend
# that we just received remote state.  And part of the analysis must include testing for
# when a request for remote state timed out instead of completing in a reasonable time.
#
# There are two modes of operation (normal mode and failure mode), and two transitions between
# them (normal to failure, and failure to normal).  Broadly speaking, in normal mode, replication
# actions will occur, and only the Primary system will generate notifications, while in failure mode,
# all replication actions are suppressed, and both Primary and DR systems generate notifications.
# Generation of notifications is actually rather more subtle, being partly controlled as well by the
# Notification Authority Control settings on each of of the two systems; see "recover help notify"
# for details of those settings.
#
# The calculated current mode of operation is carried in the boolean $local_state->{in_failure_mode}
# value.  The manually-set Notification Authority Control setting is carried in the enumerated
# $local_state->{notification_authority_control} value, while the current calculated decision on
# whether to handle notifications is carried in the boolean $local_state->{has_notification_authority}
# value.  These and other settings are analyzed in depth during each heartbeat analysis to decide what
# state transitions to make, what warnings to issue, and whether to enable or disable notifications.
#
# The transition from normal to failure mode can occur under either of two
# conditions.  This transition is designed to not trigger if we encounter
# only a brief link failure or an outage (down state) on the remote side,
# as that could be due to an ordinary bounce operation (say, while a Commit
# operation is in progress).  So it takes either an extended outage, or
# the detection of flapping even if the down periods are individually never
# long enough to trigger the transition, to drive the transition.
#
# So to decide when to trigger a normal-to-failure transition, what we want
# are four simple integers to control a mode/transition model:
#
#   The heartbeat period (in seconds):
#       contact-heartbeat-period
#
#   The maximum number of consecutive missed or down heartbeats before an
#   extended outage is declared (the down-time threshold); this value must
#   be greater than 0, to allow at least a single skipped heartbeat before
#   we declare failure.  Translated to actual time, this is referred to as
#   the maximum impending failure period:
#       max-consecutive-bad-heartbeats-before-outage
#
#   The maximum number of missed or down heartbeats within the flapping
#   window before a flapping outage is declared (the flapping threshold);
#   this value must be greater than the down-time threshold for flapping
#   to be distinguished from simple outages, and must be less than the
#   flapping window size for flapping detection to make sense:
#       max-bad-heartbeats-before-flapping
#
#   The number of consecutive heartbeats over which flapping will be
#   computed (the flapping window size); this value must be greater than
#   the down-time threshold for flapping detection to make sense:
#       flapping-window-heartbeats
#
# The necessary ordering of values is:
#
#   0 < max-consecutive-bad-heartbeats-before-outage < max-bad-heartbeats-before-flapping < flapping-window-heartbeats
#
# The condition for transition from failure to normal mode is even harder
# to satisfy.  This transition will only occur once the remote system has
# been seen to be continuously accessible and up for an extended number
# of heartbeats.  So in this case, our transition model can be controlled
# using only a single configured value:
#
#   The number of consecutive successful and "up"-status heartbeats
#   needed during failure mode before normal mode is resumed
#       min-consecutive-good-heartbeats-before-normal
#
# For this to make proper sense in the mode transition calculations, we need
# the following relationships to hold:
#
#   (flapping-window-heartbeats - max-bad-heartbeats-before-flapping) < min-consecutive-good-heartbeats-before-normal
#                        max-consecutive-bad-heartbeats-before-outage < min-consecutive-good-heartbeats-before-normal

# FIX MAJOR:  review all the log level and severity settings in log_dispatch() calls within analyze()

# FIX MAJOR:  Take into account the effects of log message consolidation, now that we have settled
# on effectively using the severity and message data to consolidate messages from this application.
# Also, try to avoid cluttering the log with too many messages of low informational value while the
# system is operating in normal mode (e.g., we will generate 1440 messages per day at the rate of
# just one message per heartbeat analysis, and often there are even more messages per cycle).

# We return \@response from this routine even though in its ordinary calling context
# as an asynchronous event handler, the return value will be ignored.  This is to
# provide for when it is called synchronously in an interactive user command.

sub analyze {
    log_timed_message "STATS:  Analyzing current local and remote states." if $DEBUG_STATS;

    # We reschedule only if this invocation of analyze() is not being run interactively.
    # Which is to say, if we call this routine interactively, that invocation must pass
    # the routine a true first argument, thereby declaring the interactive context.
    if (not $_[ARG0]) {
	# Schedule the next iteration now, so we can easily return early with this already done.
	$_[KERNEL]->delay ('analyze', $config->{'contact-heartbeat-period'});
    }

    my @response = ();

    my $local_state             = fetch_local_state();
    my $remote_state            = fetch_remote_state();
    my $in_failure_mode         = $local_state->{in_failure_mode};
    my $have_transitioned_state = 0;
    my $now                     = time();
    my $current_run_time        = $now - $start_time;

    my $max_impending_failure_period = $config->{'max-consecutive-bad-heartbeats-before-outage'} * $config->{'contact-heartbeat-period'};

    my $remote_state_age = defined($remote_state->{'state_time'}) ? ($now - $remote_state->{'state_time'}) : undef;
    my $remote_state_is_current = !$got_remote_state_timeout
	&& defined($remote_state_age)
	&& $remote_state_age < $config->{'contact-heartbeat-period'} * 3;
    my $still_waiting_for_remote_status =
	(
	    # Have we seen a remote state since we started?
	    !defined($remote_state->{'state_time'}) || $remote_state->{'state_time'} < $start_time
	)
	&&
	(
	    # Are we still in the initial startup period during which failure won't be
	    # formally recognized even in the face of no contact with the remote side?
	    $current_run_time < $max_impending_failure_period
	)
    ;

    if ($remote_state_is_current && $remote_state->{'monitoring_is_up'}) {
	# FIX LATER:  $first_good_remote_state_time seems to serve no purpose in the current logic;
	# drop it if that is still true when we fix all the other issues with this code
	$first_good_remote_state_time = $remote_state->{'state_time'} if $consecutive_good_remote_states == 0;
	++$consecutive_good_remote_states;
    }
    else {
	$consecutive_good_remote_states = 0;
    }

    if ($still_waiting_for_remote_status) {
	# We won't wait forever before we start making hard decisions,
	# but for the time being, we pause to let the system settle in.
	my @message  = (
	    "Replication is waiting for contact with the remote server ",
	    "on \"$unqualified_remote_hostname\" before possibly changing Notification Authority."
	);
	my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	push @response, @$response if @$response;
	return \@response;
    }

    # Has the remote monitoring_is_up state been flapping within the flapping window?
    my $flapping_window_period = $config->{'contact-heartbeat-period'} * $config->{'flapping-window-heartbeats'};
    my $last_remote_up_state_times = $local_state->{last_remote_up_state_times};
    my $good_heartbeats = defined($last_remote_up_state_times) ? (scalar @$last_remote_up_state_times) : 0;
    my $remote_state_is_flapping = $current_run_time > $flapping_window_period && 
      ($config->{'flapping-window-heartbeats'} - $good_heartbeats) > $config->{'max-bad-heartbeats-before-flapping'};

    # Has the remote state has been up for a sufficient number of heartbeats for us to believe it is stable?
    # "Stable" in this context means that we haven't seen either down states or flapping during that time.
    my $remote_system_is_up_and_stable = $consecutive_good_remote_states >= $config->{'min-consecutive-good-heartbeats-before-normal'};

    my $failure_is_recognized =
	(
	    # Make sure we have seen good remote status lately, and often enough in the recent past.
	    !defined($remote_state_age) || $remote_state_age >= $max_impending_failure_period
	    || !defined($last_remote_up_state_times) || scalar(@$last_remote_up_state_times) == 0
	    || $last_remote_up_state_times->[$#$last_remote_up_state_times] < ($now - $max_impending_failure_period)
	    || $remote_state_is_flapping
	)
	&&
	(
	    # Make sure we have run long enough to collect enough data to analyze.
	    $current_run_time >= $max_impending_failure_period
	)
    ;

    # FIX LATER:  Should some of our tests somewhere involve the $remote_system_has_NA flag?
    my  $local_system_has_NA  =  $local_state->{has_notification_authority};
    my $remote_system_has_NA  = $remote_state->{has_notification_authority};
    my  $local_system_has_MCA =  $local_state->{has_master_configuration_authority};
    my $remote_system_has_MCA = $remote_state->{has_master_configuration_authority};

    my  $local_system_has_grabbed_notification_control = $local_state->{notification_authority_control} eq 'grabbed';
    my  $local_system_has_dynamic_notification_control = $local_state->{notification_authority_control} eq 'dynamic';
    my $local_system_has_released_notification_control = $local_state->{notification_authority_control} eq 'released';

    my  $remote_system_has_grabbed_notification_control = undef;
    my  $remote_system_has_dynamic_notification_control = undef;
    my $remote_system_has_released_notification_control = undef;

    if (defined $remote_state->{notification_authority_control}) {
	 $remote_system_has_grabbed_notification_control = $remote_state->{notification_authority_control} eq 'grabbed';
	 $remote_system_has_dynamic_notification_control = $remote_state->{notification_authority_control} eq 'dynamic';
	$remote_system_has_released_notification_control = $remote_state->{notification_authority_control} eq 'released';
    }

    if ($remote_state_is_current) {
	# Analyse the combination of local and remote Notification Authority Control settings,
	# to warn the administrators if an odd combination is currently in force.
	# FIX LATER:  In a future release, we might consider moving this part of the analysis
	# into the check_replication plugin, or sending status to Nagios from here, to generate
	# notifications if the Notification Authority Control configuration is seen to be bad.
	# FIX LATER:  More generally, we ought to scan through the script and figure out what
	# other kinds of conditions ought to be brought to the attention of Nagios, and how.
	# FIX LATER:  Possibly, don't repeat these messages on every heartbeat analysis if the
	# message content has not changed in the last 10 minutes or so.
	if ($local_system_has_grabbed_notification_control) {
	    if ($remote_system_has_grabbed_notification_control) {
		my @message = fill_text (
		    "Notification Authority is grabbed on both \"$unqualified_local_hostname\" and ",
		    "\"$unqualified_remote_hostname\"; notifications will be generated from both ",
		    "systems.  Use the \"notify\" command in the \"recover\" program to fix this."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    elsif ($remote_system_has_dynamic_notification_control) {
		# This is an acceptable setup, if not ideal.  If the local system is the DR
		# system, duplicate notifications will appear during normal-mode operation, but
		# that effect should be obvious when it occurs and needs no complaint here.
	    }
	    elsif ($remote_system_has_released_notification_control) {
		# No matter which side is Primary and which is DR, this is an okay setup and may be
		# useful under some conditions, though failover/failback cannot occur.  Warn about that.
		my @message = fill_text (
		    "Notification Authority is grabbed on \"$unqualified_local_hostname\" and ",
		    "released on \"$unqualified_remote_hostname\"; this is sometimes acceptable ",
		    "but Notification Authority failover/failback cannot occur.  Use the ",
		    "\"notify\" command in the \"recover\" program to change this."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    else {
		# error condition; should never occur
	    }
	}
	elsif ($local_system_has_dynamic_notification_control) {
	    if ($remote_system_has_grabbed_notification_control) {
		# This is an acceptable setup, if not ideal.  If the local system is the Primary
		# system, duplicate notifications will appear during normal-mode operation, but
		# that effect should be obvious when it occurs and needs no complaint here.
	    }
	    elsif ($remote_system_has_dynamic_notification_control) {
		# This is the standard setup; there is nothing to complain about.
	    }
	    elsif ($remote_system_has_released_notification_control) {
		# This setup is dangerous if we are the Primary system, as notifications may get
		# lost if the system goes into failure mode.  If we are the DR system, the setup
		# is not ideal, but it's at least acceptable, so we don't complain in that case.
		if ($local_system_has_MCA && $remote_system_has_MCA) {
		    # Evidently the two sides each believe they are in charge.  This is ripe for problems.
		    my @message = fill_text (
			"Notification Authority is dynamic on \"$unqualified_local_hostname\" and ",
			"released on \"$unqualified_remote_hostname\".  This is possibly dangerous ",
			"and certainly confused by the fact that both systems inappropriately ",
			"have Master Configuration Authority.  Use the \"notify\" and \"config\" ",
			"commands in the \"recover\" program to change this."
		    );
		    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		    push @response, @$response if @$response;
		}
		elsif ($local_system_has_MCA && !$remote_system_has_MCA) {
		    # Here we are evidently the Primary system.
		    my @message = fill_text (
			"Notification Authority is dynamic on \"$unqualified_local_hostname\" ",
			"and released on \"$unqualified_remote_hostname\"; this is dangerous as ",
			"notifications may get lost if the system goes into failure mode.",
			"Use the \"notify\" command in the \"recover\" program to change this."
		    );
		    my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_CRITICAL, @message);
		    push @response, @$response if @$response;
		}
		elsif (!$local_system_has_MCA && $remote_system_has_MCA) {
		    # Here we are evidently the DR system.
		}
		elsif (!$local_system_has_MCA && !$remote_system_has_MCA) {
		    # Evidently nobody is in charge at this point.  This is not necessarily an error, because the site
		    # could have disabled Master Configuration Authority on one system in preparation for enabling it
		    # on the other system.  But it means we cannot tell whether this is a Primary or DR system.  We are
		    # saved in that the logic below will arrogate Notification Authority in this situation, so we won't
		    # lose any notifications.  But still, the setup is not ideal.
		    my @message = fill_text (
			"Notification Authority is dynamic on \"$unqualified_local_hostname\" and ",
			"released on \"$unqualified_remote_hostname\".  This is confused by the fact that ",
			"neither system has Master Configuration Authority.  Use the \"notify\" ",
			"and \"config\" commands in the \"recover\" program to change this."
		    );
		    my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		    push @response, @$response if @$response;
		}
		else {
		    # If the chain of tests above is written correctly,
		    # this is a logical impossibility and we should never get here.
		    my @message = fill_text (
			"Replication on \"$unqualified_local_hostname\" is confused about local and remote ",
			"Master Configuration Authority states; use the \"config\" command ",
			"in the \"recover\" program to fix this.  Possibly, replication should ",
			"be restarted.  Report this problem to GroundWork Support."
		    );
		    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		    push @response, @$response if @$response;
		}
	    }
	    else {
		# error condition; should never occur
	    }
	}
	elsif ($local_system_has_released_notification_control) {
	    if ($remote_system_has_grabbed_notification_control) {
		# No matter which side is Primary and which is DR, this is an okay setup and may be
		# useful under some conditions, though failover/failback cannot occur.  Warn about that.
		my @message = fill_text (
		    "Notification Authority is released on \"$unqualified_local_hostname\" and ",
		    "grabbed on \"$unqualified_remote_hostname\"; this is sometimes acceptable but ",
		    "Notification Authority failover/failback cannot occur.  Use the ",
		    "\"notify\" command in the \"recover\" program to change this."
		);
		my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
	    }
	    elsif ($remote_system_has_dynamic_notification_control) {
		# This setup is dangerous if we are the DR system, as notifications may get lost
		# if the system goes into failure mode.  If we are the Primary system, the setup
		# is not ideal, but it's at least acceptable, so we don't complain in that case.
		if ($local_system_has_MCA && $remote_system_has_MCA) {
		    # Evidently the two sides each believe they are in charge.  This is ripe for problems.
		    my @message = fill_text (
			"Notification Authority is released on \"$unqualified_local_hostname\" and ",
			"dynamic on \"$unqualified_remote_hostname\".  This is possibly dangerous and ",
			"certainly confused by the fact that both systems inappropriately have ",
			"Master Configuration Authority.  Use the \"notify\" and \"config\" ",
			"commands in the \"recover\" program to change this."
		    );
		    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		    push @response, @$response if @$response;
		}
		elsif ($local_system_has_MCA && !$remote_system_has_MCA) {
		    # Here we are evidently the Primary system.
		}
		elsif (!$local_system_has_MCA && $remote_system_has_MCA) {
		    # Here we are evidently the DR system.
		    my @message = fill_text (
			"Notification Authority is released on \"$unqualified_local_hostname\" and ",
			"dynamic on \"$unqualified_remote_hostname\"; this is dangerous as notifications ",
			"may get lost if the system goes into failure mode.  Use the \"notify\" ",
			"command in the \"recover\" program to change this."
		    );
		    my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_CRITICAL, @message);
		    push @response, @$response if @$response;
		}
		elsif (!$local_system_has_MCA && !$remote_system_has_MCA) {
		    # Evidently nobody is in charge at this point.  This is not necessarily an error, because the site
		    # could have disabled Master Configuration Authority on one system in preparation for enabling it
		    # on the other system.  But it means we cannot tell whether this is a Primary or DR system.  We are
		    # saved in that the logic below will arrogate Notification Authority in this situation, so we won't
		    # lose any notifications.  But still, the setup is not ideal.
		    my @message = fill_text (
			"Notification Authority is released on \"$unqualified_local_hostname\" and ",
			"dynamic on \"$unqualified_remote_hostname\".  This is confused by the fact that",
			"neither system has Master Configuration Authority.  Use the \"notify\" ",
			"and \"config\" commands in the \"recover\" program to change this."
		    );
		    my $response = log_dispatch (LOG_LEVEL_WARNING, SEVERITY_WARNING, @message);
		    push @response, @$response if @$response;
		}
		else {
		    # If the chain of tests above is written correctly,
		    # this is a logical impossibility and we should never get here.
		    my @message = fill_text (
			"Replication on \"$unqualified_local_hostname\" is confused about local and remote ",
			"Master Configuration Authority states; use the \"config\" command ",
			"in the \"recover\" program to fix this.  Possibly, replication ",
			"should be restarted.  Report this problem to GroundWork Support."
		    );
		    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		    push @response, @$response if @$response;
		}
	    }
	    elsif ($remote_system_has_released_notification_control) {
		my @message = fill_text (
		    "Notification Authority is released on both \"$unqualified_local_hostname\" and ",
		    "\"$unqualified_remote_hostname\"; no notifications will be generated from either ",
		    "system.  Use the \"notify\" command in the \"recover\" program to fix this."
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
	    }
	    else {
		# error condition; should never occur
	    }
	}
	else {
	    # error condition; should never occur
	}
    }

    my   $arrogate_notification_authority = 0;
    my $relinquish_notification_authority = 0;

    if ($failure_is_recognized && !$in_failure_mode) {
	# Now is the time to transition into failure mode.
	$local_state->{in_failure_mode} = $in_failure_mode = true;
	$have_transitioned_state = 1;
	my @message = fill_text (
	    "\"$unqualified_remote_hostname\" is inaccessible or down; replication on ",
	    "\"$unqualified_local_hostname\" is transitioning into failure mode."
	);
	my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	push @response, @$response if @$response;
	if ($local_system_has_grabbed_notification_control) {
	    my @message = fill_text (
		"Notification Authority on \"$unqualified_local_hostname\" was previously grabbed ",
		"and will remain enabled."
	    );
	    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	    push @response, @$response if @$response;
	    if (!$local_system_has_NA) {
		# This cannot happen; it's an internal error condition.  We will correct the situation below.
		my $line = __LINE__;
		my @message = fill_text (
		    "Replication on \"$unqualified_local_hostname\" has encountered an impossible ",
		    "condition (local system has grabbed Notification Authority Control, but does not ",
		    "have Notification Authority).  Report this problem (Version $VERSION, ",
		    "line $line) to GroundWork Support."
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
		$arrogate_notification_authority = 1;
	    }
	}
	elsif ($local_system_has_dynamic_notification_control) {
	    my @message = fill_text (
		"Replication on \"$unqualified_local_hostname\" is arrogating ",
		"Notification Authority for this system."
	    );
	    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	    push @response, @$response if @$response;
	    $arrogate_notification_authority = 1;
	}
	elsif ($local_system_has_released_notification_control) {
	    my @message = fill_text (
		"Notification Authority on \"$unqualified_local_hostname\" was previously released ",
		"and will therefore not be arrogated now, even though ",
		"replication will now be operating in failure mode."
	    );
	    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	    push @response, @$response if @$response;
	    if ($local_system_has_NA) {
		# This cannot happen; it's an internal error condition.  We will correct the situation below.
		my $line = __LINE__;
		my @message = fill_text (
		    "Replication on \"$unqualified_local_hostname\" has encountered an impossible ",
		    "condition (local system has released Notification Authority Control, but has ",
		    "Notification Authority).  Report this problem (Version $VERSION, ",
		    "line $line) to GroundWork Support."
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
		$relinquish_notification_authority = 1;
	    }
	}
	else {
	    # We should never get here; this is an internal error state.
	    my @message = fill_text (
		"Replication on \"$unqualified_local_hostname\" is confused about its Notification ",
		"Authority Control setting; use the \"notify\" command in the \"recover\" program ",
		"to fix this.  Possibly, replication should be restarted.  Report this ",
		"problem to GroundWork Support."
	    );
	    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
	    push @response, @$response if @$response;
	}
    }
    elsif ($failure_is_recognized && $in_failure_mode) {
	if ($remote_system_is_up_and_stable) {
	    # Here we have contradictory evidence:  failure is recognized (meaning we should continue in failure mode)
	    # while the remote system is apparently up and stable (meaning we should now transition to normal mode).
	    # I don't necessarily believe this condition is even possible, though the calculations for these conditions
	    # test somewhat different facets of the historical state.  Quite possibly, if we get here, what we have is
	    # that the configured min-consecutive-good-heartbeats-before-normal value has been set too low.  In any case,
	    # if we do end up here, the contradiction should itself be treated as evidence of failure of a certain sort,
	    # even if it's really fundamentally a configuration failure, so we will remain in failure mode.
	    my @message = fill_text (
		'Replication found contradictory evidence of failure and normal operation; ',
		'will remain in failure mode.'
	    );
	    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
	    push @response, @$response if @$response;
	}
	else {
	    # Failure mode continues.
	    my @message = fill_text (
		"Replication continues to operate in failure mode on \"$unqualified_local_hostname\"."
	    );
	    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	    push @response, @$response if @$response;
	}
	# The Notification Authority setting should have been established by the last change of Notification Authority
	# Control or by the entry into failure mode.  We're going to depend on that here and only correct the simplest
	# of potential errors, one which we don't believe ought to every happen anyway.  On the other hand, we might
	# not have Notification Authority yet because analyze() is being called as part of setting dynamic Notification
	# Authority Control, to determine whether we should now arrogate it, so this test and complaint could be pointless.
	if ($local_system_has_dynamic_notification_control && !$local_system_has_NA) {
	    my $line = __LINE__;
	    my @message = fill_text (
		"Replication on \"$unqualified_local_hostname\" has encountered an impossible ",
		"condition (local system has dynamic Notification Authority Control, but ",
		"does not have Notification Authority in failure mode).  Report ",
		"this problem (Version $VERSION, line $line) to GroundWork Support."
	    );
	    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
	    push @response, @$response if @$response;
	    $arrogate_notification_authority = 1;
	}
    }
    elsif (!$failure_is_recognized && $in_failure_mode && !$remote_system_is_up_and_stable) {
	# We might soon be coming out of failure mode.  But we don't want to do so
	# until we're sure the remote system is up and stable.
	my @message = fill_text (
	    "Replication continues to operate in failure mode on \"$unqualified_local_hostname\", ",
	    "but may soon revert to normal mode ",
	    "($consecutive_good_remote_states of $config->{'min-consecutive-good-heartbeats-before-normal'} ",
	    "required consecutive good heartbeats have been seen so far)."
	);
	my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	push @response, @$response if @$response;
	# The Notification Authority setting should have been established by the last change of Notification Authority
	# Control or by the entry into failure mode.  We're going to depend on that here and only correct the simplest
	# of potential errors, one which we don't believe ought to every happen anyway.  On the other hand, we might
	# not have Notification Authority yet because analyze() is being called as part of setting dynamic Notification
	# Authority Control, to determine whether we should now arrogate it, so this test and complaint could be pointless.
	if ($local_system_has_dynamic_notification_control && !$local_system_has_NA) {
	    my $line = __LINE__;
	    my @message = fill_text (
		"Replication on \"$unqualified_local_hostname\" has encountered an impossible ",
		"condition (local system has dynamic Notification Authority Control, but does not ",
		"have Notification Authority in failure mode).  Report this problem ",
		"(Version $VERSION, line $line) to GroundWork Support."
	    );
	    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
	    push @response, @$response if @$response;
	    $arrogate_notification_authority = 1;
	}
    }
    elsif (!$failure_is_recognized && $in_failure_mode && $remote_system_is_up_and_stable) {
	# Now is the time to transition out of failure mode back to normal mode.  We don't bother for the moment
	# comparing local and remote Master Configuration Authority settings if Notification Authority is either
	# grabbed or released, because that will be done repeatedly in the future when we remain in normal mode.
	$local_state->{in_failure_mode} = $in_failure_mode = false;
	$have_transitioned_state = 1;
	my @message = fill_text (
	    "Remote \"$unqualified_remote_hostname\" is back up; replication on local ",
	    "\"$unqualified_local_hostname\" is transitioning into normal mode."
	);
	my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	push @response, @$response if @$response;
	if ($local_system_has_grabbed_notification_control) {
	    my @message = fill_text (
		"Notification Authority on \"$unqualified_local_hostname\" was previously grabbed ",
		"and will remain enabled."
	    );
	    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	    push @response, @$response if @$response;
	    if (!$local_system_has_NA) {
		# This cannot happen; it's an internal error condition.  We will correct the situation below.
		my $line = __LINE__;
		my @message = fill_text (
		    "Replication on \"$unqualified_local_hostname\" has encountered an impossible ",
		    "condition (local system has grabbed Notification Authority Control, but ",
		    "does not have Notification Authority).  Report this problem ",
		    "(Version $VERSION, line $line) to GroundWork Support."
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
		$arrogate_notification_authority = 1;
	    }
	}
	elsif ($local_system_has_dynamic_notification_control) {
	    # We only relinquish Notification Authority if it looks like we're operating as the DR system, without
	    # Master Configuration Authority.  If we're operating as the Primary system, with Master Configuration
	    # Authority, then we need to keep Notification Authority.  And if both systems or neither system has
	    # Master Configuration Authority, the current DR configuration is messed up and we will keep Notification
	    # Authority because there is no good way to decide which one end of the connection should keep it.
	    # It's okay to use the remote-system state here in our calculations because the remote state must be
	    # current for us to be in this code branch.
	    if ($local_system_has_MCA && $remote_system_has_MCA) {
		# Evidently the two sides each believe they are in charge.  This is ripe for problems,
		# so keep local Notification Authority turned on.
		my @message = fill_text (
		    "Both \"$unqualified_local_hostname\" and \"$unqualified_remote_hostname\" have Master Configuration ",
		    "Authority, so Notification Authority is being retained on \"$unqualified_local_hostname\"."
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
		$arrogate_notification_authority = 1;
	    }
	    elsif ($local_system_has_MCA && !$remote_system_has_MCA) {
		# Here we are evidently the Primary system.  Keep local Notification Authority turned on.
		my @message = fill_text (
		    "\"$unqualified_local_hostname\" is apparently the Primary system, ",
		    "and will retain Notification Authority."
		);
		my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_OK, @message);
		push @response, @$response if @$response;
		$arrogate_notification_authority = 1;
	    }
	    elsif (!$local_system_has_MCA && $remote_system_has_MCA) {
		# Here we are evidently the DR system.
		if ($remote_system_has_released_notification_control) {
		    # The remote (Primary) system won't pick up Notification Authority, so we'd better retain it here.
		    my @message = fill_text (
			"\"$unqualified_local_hostname\" is apparently the DR system, but will keep ",
			"Notification Authority because it was previously released ",
			"on \"$unqualified_remote_hostname\"."
		    );
		    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
		    push @response, @$response if @$response;
		    $arrogate_notification_authority = 1;
		}
		else {
		    # It seems safe to let the Primary system take over Notification Authority now.
		    my @message = fill_text (
			"\"$unqualified_local_hostname\" is apparently the DR system, and will drop ",
			"Notification Authority."
		    );
		    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_OK, @message);
		    push @response, @$response if @$response;
		    $relinquish_notification_authority = 1;
		}
	    }
	    elsif (!$local_system_has_MCA && !$remote_system_has_MCA) {
		# Evidently nobody is in charge at this point.  This is not necessarily an error, because the site
		# could have disabled Master Configuration Authority on one system in preparation for enabling it
		# on the other system.  For safety's sake, keep local Notification Authority turned on, because we
		# cannot tell which system ought to have it enabled.
		my @message = fill_text (
		    "Neither \"$unqualified_local_hostname\" nor \"$unqualified_remote_hostname\" has ",
		    "Master Configuration Authority, so Notification Authority ",
		    "is being retained on \"$unqualified_local_hostname\"."
		);
		my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
		$arrogate_notification_authority = 1;
	    }
	    else {
		# If the chain of tests above is written correctly,
		# this is a logical impossibility and we should never get here.
		my @message = fill_text (
		    "Replication on \"$unqualified_local_hostname\" is confused about local and ",
		    "remote Master Configuration Authority states; use the \"config\" ",
		    "command in the \"recover\" program to fix this.  Possibly, replication ",
		    "should be restarted.  Report this problem to GroundWork Support."
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
		$arrogate_notification_authority = 1;
	    }
	}
	elsif ($local_system_has_released_notification_control) {
	    # Whether or not we would have relinquished Notification Authority if dynamic control were in play is a
	    # complex calculation (just above) we don't want to bother with here, so our message will remain simple
	    # and not suggest whether or not the future state would have been different had it not been locked down.
	    my @message = fill_text (
		"Notification Authority on \"$unqualified_local_hostname\" was previously released and will ",
		"therefore remain disabled while replication now runs in normal mode."
	    );
	    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	    push @response, @$response if @$response;
	    if ($local_system_has_NA) {
		# This cannot happen; it's an internal error condition.  We will correct the situation below.
		my $line = __LINE__;
		my @message = fill_text (
		    "Replication on \"$unqualified_local_hostname\" has encountered an impossible ",
		    "condition (local system has released Notification Authority Control, but has ",
		    "Notification Authority).  Report this problem (Version $VERSION, ",
		    "line $line) to GroundWork Support."
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
		$relinquish_notification_authority = 1;
	    }
	}
	else {
	    # We should never get here; this is an internal error state.
	    my @message = fill_text (
		"Replication on \"$unqualified_local_hostname\" is confused about its Notification ",
		"Authority Control setting; use the \"notify\" command in the \"recover\" program ",
		"to fix this.  Possibly, replication should be restarted.  Report this ",
		"problem to GroundWork Support."
	    );
	    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
	    push @response, @$response if @$response;
	}
    }
    elsif (!$failure_is_recognized && !$in_failure_mode) {
	# We are operating in normal mode, and there is no failure to contend with.  The Notification Authority setting
	# should have been established by the last change of Notification Authority Control or by the entry into normal mode,
	# but that calculation might have been complicated by weird stuff going on with the configuration of Master Configuration
	# Authority.  And for that matter, the configuration of Master Configuration Authority may well have changed since
	# we entered normal mode.  So just continue on in that mode, but warn if we see some strange setup (complain if both
	# systems have or neither system has Master Configuration Authority), and recalculate the Notification Authority
	# setting in any case if we have dynamic control (arrogating it if we see weird Master Configuration Authority).
	if ($local_system_has_grabbed_notification_control) {
	    if ($remote_state_is_current && !($local_system_has_MCA xor $remote_system_has_MCA)) {
		if ($local_system_has_MCA) {
		    # This is an apparent error.  It should never be the case that both local and remote systems
		    # have Master Configuration Authority at the same time.  Possibly there could be some kind
		    # of race condition in the setting of Master Configuration Authority and the remote-state
		    # heartbeats we are analyzing that might apparently bring this about temporarily (I haven't
		    # thought that through), but certainly it's a serious problem if the condition persists.
		    my @message = fill_text (
			"Both \"$unqualified_local_hostname\" and \"$unqualified_remote_hostname\" ",
			"have Master Configuration Authority."
		    );
		    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		    push @response, @$response if @$response;
		}
		else {
		    # This is somewhat weird but not necessarily an error.  The site may be transitioning MCA from
		    # one system to the other, and has released it from one before grabbing it from the other.
		    my @message = fill_text (
			"Neither \"$unqualified_local_hostname\" nor \"$unqualified_remote_hostname\" ",
			"has Master Configuration Authority."
		    );
		    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
		    push @response, @$response if @$response;
		}
	    }
	    my @message = fill_text (
		"Notification Authority on \"$unqualified_local_hostname\" was previously grabbed ",
		"and will remain enabled."
	    );
	    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	    push @response, @$response if @$response;
	    if (!$local_system_has_NA) {
		# This cannot happen; it's an internal error condition.  We will correct the situation below.
		my $line = __LINE__;
		my @message = fill_text (
		    "Replication on \"$unqualified_local_hostname\" has encountered an impossible ",
		    "condition (local system has grabbed Notification Authority Control, but does not ",
		    "have Notification Authority).  Report this problem (Version $VERSION, ",
		    "line $line) to GroundWork Support."
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
		$arrogate_notification_authority = 1;
	    }
	}
	elsif ($local_system_has_dynamic_notification_control) {
	    # We only disable Notification Authority if it looks like we're operating as the DR system, without
	    # Master Configuration Authority.  If we're operating as the Primary system, with Master Configuration
	    # Authority, then we need to enable Notification Authority.  And if both systems or neither system has
	    # Master Configuration Authority, the current DR configuration is messed up and we arrogate Notification
	    # Authority because there is no good way to decide which one end of the connection should keep it.
	    if ($local_system_has_MCA && $remote_system_has_MCA) {
		# Evidently the two sides each believe they are in charge.  This is ripe for problems,
		# so enable local Notification Authority.
		my @message = fill_text (
		    "Both \"$unqualified_local_hostname\" and \"$unqualified_remote_hostname\" have ",
		    "Master Configuration Authority, so Notification Authority ",
		    "will be enabled on \"$unqualified_local_hostname\"."
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
		$arrogate_notification_authority = 1;
	    }
	    elsif ($local_system_has_MCA && !$remote_system_has_MCA) {
		# Here we are evidently the Primary system.  Enable local Notification Authority.
		my @message = fill_text (
		    "\"$unqualified_local_hostname\" is apparently the Primary system, ",
		    "so Notification Authority will be enabled."
		);
		my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
		$arrogate_notification_authority = 1;
	    }
	    elsif (!$local_system_has_MCA && $remote_system_has_MCA) {
		# Here we are evidently the DR system.
		if ($remote_system_has_released_notification_control) {
		    # The remote (Primary) system won't handle Notification Authority, so we'd better do so here.
		    my @message = fill_text (
			"\"$unqualified_local_hostname\" is apparently the DR system, but will keep Notification",
			"Authority because it was previously released on \"$unqualified_remote_hostname\"."
		    );
		    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
		    push @response, @$response if @$response;
		    $arrogate_notification_authority = 1;
		}
		else {
		    # It seems safe to let the Primary system handle Notification Authority.
		    my @message = fill_text (
			"\"$unqualified_local_hostname\" is apparently the DR system, ",
			"so Notification Authority will be disabled."
		    );
		    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_OK, @message);
		    push @response, @$response if @$response;
		    $relinquish_notification_authority = 1;
		}
	    }
	    elsif (!$local_system_has_MCA && !$remote_system_has_MCA) {
		# Evidently nobody is in charge at this point.  This is not necessarily an error, because the site
		# could have disabled Master Configuration Authority on one system in preparation for enabling it
		# on the other system.  For safety's sake, arrogate local Notification Authority, because we cannot
		# tell which system ought to have it enabled.
		my @message = fill_text (
		    "Neither \"$unqualified_local_hostname\" nor \"$unqualified_remote_hostname\" has Master Configuration ",
		    "Authority, so Notification Authority will be enabled on \"$unqualified_local_hostname\"."
		);
		my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
		push @response, @$response if @$response;
		$arrogate_notification_authority = 1;
	    }
	    else {
		# If the chain of tests above is written correctly,
		# this is a logical impossibility and we should never get here.
		my @message = fill_text (
		    "Replication on \"$unqualified_local_hostname\" is confused about local and remote ",
		    "Master Configuration Authority states; use the \"config\" command in the ",
		    "\"recover\" program to fix this.  Possibly, replication should be ",
		    "restarted.  Report this problem to GroundWork Support."
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
		$arrogate_notification_authority = 1;
	    }
	}
	elsif ($local_system_has_released_notification_control) {
	    if ($remote_state_is_current && !($local_system_has_MCA xor $remote_system_has_MCA)) {
		if ($local_system_has_MCA) {
		    # This is an apparent error.  It should never be the case that both local and remote systems
		    # have Master Configuration Authority at the same time.  Possibly there could be some kind
		    # of race condition in the setting of Master Configuration Authority and the remote-state
		    # heartbeats we are analyzing that might apparently bring this about temporarily (I haven't
		    # thought that through), but certainly it's a serious problem if the condition persists.
		    my @message = fill_text (
			"Both \"$unqualified_local_hostname\" and \"$unqualified_remote_hostname\" ",
			"have Master Configuration Authority."
		    );
		    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		    push @response, @$response if @$response;
		}
		else {
		    # This is somewhat weird but not necessarily an error.  The site may be transitioning MCA from
		    # one system to the other, and has released it from one before grabbing it from the other.
		    my @message = fill_text (
			"Neither \"$unqualified_local_hostname\" nor \"$unqualified_remote_hostname\" ",
			"has Master Configuration Authority."
		    );
		    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
		    push @response, @$response if @$response;
		}
	    }
	    # Whether or not we would have relinquished Notification Authority if dynamic control were in play is a
	    # complex calculation (just above) we don't want to bother with here, so our message will remain simple
	    # and not suggest whether or not the future state would have been different had it not been locked down.
	    my @message = fill_text (
		"Notification Authority on \"$unqualified_local_hostname\" was previously released and ",
		"will therefore remain disabled while replication runs in normal mode."
	    );
	    my $response = log_dispatch (LOG_LEVEL_NOTICE, SEVERITY_WARNING, @message);
	    push @response, @$response if @$response;
	    if ($local_system_has_NA) {
		# This cannot happen; it's an internal error condition.  We will correct the situation below.
		my $line = __LINE__;
		my @message = fill_text (
		    "Replication on \"$unqualified_local_hostname\" has encountered an impossible ",
		    "condition (local system has released Notification Authority Control, but has ",
		    "Notification Authority).  Report this problem (Version $VERSION, ",
		    "line $line) to GroundWork Support."
		);
		my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
		push @response, @$response if @$response;
		$relinquish_notification_authority = 1;
	    }
	}
	else {
	    # We should never get here; this is an internal error state.
	    my @message = fill_text (
		"Replication on \"$unqualified_local_hostname\" is confused about its Notification ",
		"Authority Control setting; use the \"notify\" command in the \"recover\" program ",
		"to fix this.  Possibly, replication should be restarted.  Report this ",
		"problem to GroundWork Support."
	    );
	    my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
	    push @response, @$response if @$response;
	}
    }
    else {
	# We should never get here.  This is an internal error state.
	my @message = fill_text (
	    "Replication on \"$unqualified_local_hostname\" is confused about failure states.  Suggest",
	    "replication be restarted.  Report this problem to GroundWork Support."
	);
	my $response = log_dispatch (LOG_LEVEL_ERROR, SEVERITY_CRITICAL, @message);
	push @response, @$response if @$response;
    }

    if ($arrogate_notification_authority) {
	$local_state->{has_notification_authority} = true;
	$have_transitioned_state = 1;
    }
    if ($relinquish_notification_authority) {
	$local_state->{has_notification_authority} = false;
	$have_transitioned_state = 1;
    }

    # The state transitions we cover here are normal-to-failure, failure-to-normal, and changes
    # to notification authority regardless of whether the normal/failure mode has changed.
    if ($have_transitioned_state) {
	store_local_state ($local_state);
	# FIX THIS:  implement proper error detection on the save
	save_replication_state ();
    }

    # We need to send a command to Nagios on every analysis cycle, telling it whether to
    # enable or disable notifications.  That may be vital in case the original setting does
    # not go through because Nagios happens to be down when a transitional message is sent.
    my $error = $local_state->{has_notification_authority} ? $nagios->enable_notifications() : $nagios->disable_notifications();
    if ($error) {
	push @response, $error;

	# If we cannot enable or disable notifications, then we need to send that message to
	# Foundation, so somebody notices.  The message has already been sent to our log file.
	# The most likely explanation, though, is that a Commit operation is taking place, and
	# Nagios is briefly down.
	my $errors = $foundation->send_message(SEVERITY_CRITICAL, REPLICATION_ENGINE, $error);
	push @response, @$errors if $errors;
    }

    return \@response;
}
