# -*-  indent-tabs-mode:nil;  -*-
# vim: ts=8:sw=4:expandtab:ai:
# Leave the two lines above as-is.  They ensure that leading tabs are
# expanded to spaces, as required by YAML, when this file is edited
# using either emacs or vim.

# This file is formatted according to the YAML standard (http://www.yaml.org/).
# Most importantly, consistent indentation for parallel items is critical, and
# sometimes you need to enclose certain values in quotes so the parser will
# swallow them.

# ================================================================
# Replication Configuration File
# ================================================================

# Copyright 2010 GroundWork Open Source, Inc. ("GroundWork").  All rights
# reserved.  Use is subject to GroundWork commercial license terms.

# ----------------------------------------------------------------
# Replicator setup and operation
# ----------------------------------------------------------------

# * enable processing on this server
#   [yes/no]
enable-processing: no

# * debug level for logging
# No debug = 0, Statistics = 5, Normal debug = 6, Detail debug = 7.
# More precisely, specify one of the following numbers:
# NONE    = 0; turn off all debug info
# FATAL   = 1; the application is about to die
# ERROR   = 2; the application has found a serious problem, but will attempt to recover
# WARNING = 3; the application has found an anomaly, but will try to handle it
# NOTICE  = 4; the application wants to inform you of a significant event
# STATS   = 5; the application wants to log statistical data for later analysis
# INFO    = 6; the application wants to log a potentially interesting event
# DEBUG   = 7; the application wants to log detailed debugging data
# FERVID  = 8; the application wants to log an extreme amount of debugging data
debug-level: 7

# * The user on the remote machine whose account we will use to log in
#   when we replicate files there.  Must be nagios for production use.
remote-user: nagios

# * Names of the servers between which replication will be maintained.
#   On a virgin installation, the machine established here as the primary-server
#   will default to having both Master Configuration Authority and Notification
#   Authority, while the machine established here as the secondary-server will
#   default to not having either.  Both types of authority can/will be subsequently
#   changed over the life of the replication, but the assignments here should
#   reflect the usual production roles for these servers.
primary-server  : work.groundwork.groundworkopensource.com
secondary-server: honor.groundwork.groundworkopensource.com

# * port number on which UI commands are served
primary-command-port    : 43210
secondary-command-port  : 43210

# * port number on which heartbeat commands are served
primary-heartbeat-port  : 43211
secondary-heartbeat-port: 43211

# * Location of the replication state file (for simple viewing of the current state
#   by other applications; and for storage of the replication state across bounces
#   of the Replication Engine).
#   In production, use an absolute path here.  A relative path is interpreted by
#   prepending additional path components not specified here, relative to where the
#   replication_state_engine script lives.  Such a setup can be used in production
#   but is generally intended for use in a development environment.
# replication-state-file: ./replication_state
replication-state-file: /usr/local/groundwork/replication/var/replication_state

# * DR heartbeat period (seconds)
contact-heartbeat-period: 60

# * the maximum number of consecutive missed or down heartbeats before an extended
#   outage is declared (the down-time threshold); this value must be greater than 0
max-consecutive-bad-heartbeats-before-outage: 3

# * the maximum number of missed or down heartbeats within the flapping window before
#   a flapping outage is declared (the flapping threshold); this value must be greater
#   than max-consecutive-bad-heartbeats-before-outage and must also be less than
#   flapping-window-heartbeats
max-bad-heartbeats-before-flapping: 5

# * the number of consecutive heartbeats over which flapping will be computed (the flapping
#   window size); this value must be greater than max-consecutive-bad-heartbeats-before-outage
flapping-window-heartbeats: 10

# * the number of consecutive successful and "up"-status heartbeats needed during
#   failure mode before paired-server up status is believed, normal mode is resumed,
#   and Notification Authority is relinquished if it is dynamically controlled
min-consecutive-good-heartbeats-before-normal: 10

# * locations of the pending and backup configuration repositories
pending-config-base-dir: /usr/local/groundwork/replication/pending/
backups-config-base-dir: /usr/local/groundwork/replication/backups/

# * Base location of the actions scripts (a tree of files must be present under
#   this parent directory, containing action scripts to start, stop, and capture
#   data for both applications and databases).
#   In production, use an absolute path here.  A relative path is interpreted by
#   prepending additional path components not specified here, relative to where the
#   replication_state_engine script lives.  Such a setup can be used in production
#   but is generally intended for use in a development environment.
# actions-base-dir: ./actions/
actions-base-dir: /usr/local/groundwork/replication/actions/

# * where special commands such as enabling/disabing notifications or sending
#   state-transition notifications or Replication Engine alarms must be sent
nagios-command-pipe: /usr/local/groundwork/nagios/var/spool/nagios.cmd

# The maximum size in bytes for any single write operation to the output command pipe.
# The value chosen here must be no larger than PIPE_BUF (getconf -a | fgrep PIPE_BUF)
# on your platform, unless you have an absolute guarantee that no other process will
# ever write to the command pipe (which is highly unlikely).
max-command-pipe-write-size: 4096

# The maximum time in seconds to wait for any single write to the nagios command pipe
# to complete.
max-command-pipe-wait-time: 30

# * where to probe Nagios state to ensure that it is consistent with what the
#   Replication Engine believes (look at programstatus.enable_notifications)
#   (possible future capability; currently unused)
# notification-state-file: /usr/local/groundwork/nagios/var/status.log

# * script to run to tell whether paired databases can be brought into sync by
#   processing existing transaction logs, or whether wholesale truncation and
#   re-population is needed (possible future capability; currently unused)
# database-sync-probe: /usr/local/groundwork/replication/scripts/database-sync-status

# * Replication Engine log file path: where to log ordinary operational messages,
#   especially for debugging.  A relative pathname specified here will be
#   interpreted relative to the directory in which the Replication State Engine
#   lives and runs.
#   In production, use an absolute path here.  A relative path is interpreted by
#   prepending additional path components not specified here, relative to where the
#   replication_state_engine script lives.  Such a setup can be used in production
#   but is generally intended for use in a development environment.
# replication-log-file: ./replication_state_engine.log
replication-log-file: /usr/local/groundwork/replication/logs/replication_state_engine.log

# * How large (in MBytes) the logfile is allowed to get before it is automatically
#   rotated.  The actual size may climb a bit above this, as we will only check
#   periodically to see if the size limit has been exceeded.
max-logfile-size: 10

# * How many total logfiles will be retained when the logfile is rotated.
#   Set this to 1 to just truncate the existing file and not retain any
#   additional copies.  Otherwise, set it to some larger small integer.
max-logfiles-to-retain: 5

# ----------------------------------------------------------------
# Specific data to replicate
# ----------------------------------------------------------------

# The type of stuff that is specified here:
#
# * files and file trees to be replicated, and how they are to be grouped
#   with respect to the applications that are affected by them
#
# * database tables to be replicated
#
# * replication schedules for various application files and databases
#
# * (future) validation scripts to run before installing new files and file
#   trees (to help protect against grabbing and installing inconsistent or
#   incomplete configurations)
#
# * (future) what processes need to be quiesced or blocked for the duration
#   of sets of replication activities
#
# * (future) local ports or scripts or other identifying information used to
#   quiesce, block, or bounce applications
#
# * (future) how to trigger other applications into re-reading their configurations
#
# * (future) how to trigger other applications into reading the replication
#   state and enter or exit read-only mode with respect to allowing changes
#   to their own configurations to be made through those applications
#
# * (future) what data transformations need to be made to which files,
#   and how they should be carried out
#
# * (future) scripts and programs to call for data transformations, for
#   replication transport, for replicated file installation, etc.
#
# * the number of old configuration sets to preserve for each application or
#   database, as replication rolls forward

# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
# Action scheduling specifications
# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

# Replication and cleanup action scheduling is specified by a combination of
# {sync-period, sync-phase} and {cleanup-period, cleanup-phase} settings.
# Allowable combinations are:
#
#   xxx-period  xxx-phase                      Notes                               Support
#   ======================================================================================
#   daily       hh:mm                          (exactly once per day)              now
#   daily       [ "hh:mm", "hh:mm", "hh:mm" ]  (multiple specific times per day)   now
#   minicron    mm                             (this many minute past the hour)    now
#   minicron    "*/ss"                         (every ss minutes within the hour)  now
#   minicron    "*/ss+pp"                      (same, but offset by pp minutes)    now
#   cron        "min hour mday month wday"     (like standard crontab)             future
#
# The extended syntax we support for a minicron entry includes "*/ss+pp" (with required
# enclosing quotes), where ss (the step value) is the number of minutes which should
# elapse between events, and pp is the phase offset from zero for the first such event
# within the hour.  So for instance, "*/30" would be equivalent to "0,30" as an ordinary
# cron minutes specification, while "*/30+10" would be equivalent to "10,40".  Someday,
# we intend to offer standard crontab specifications, and we might also allow lists of
# names (for month and day-of-week fields) in our "cron"-like syntax.
#
# All times are specified in local time, which may include Daylight Savings Time
# adjustments.  It is therefore recommended that you not schedule daily events
# between 01:00 and 03:00, to avoid any confusion about what might happen in a
# transition into or out of Daylight Savings Time.

# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
# Action timeout specifications
# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

# Scripted actions are limited in duration by timeouts.  These actions are:
#
#     obtain:   acquire a configuration from the remote system
#     capture:  save the local configuration; this subsumes all aspects of
#               stopping the object if needed, storing its configuration,
#               restarting the object again, and sending the configuration
#               to the remote system that made the capture request
#     stop:     quiesce the local object so a new configuration can be installed
#     deploy:   install a new local configuration (obtained from the remote system)
#     start:    activate the local object after a new configuration is installed
#
# The obtain and capture operations are clearly linked.  An obtain operation on the
# local machine triggers a capture operation on the remote machine.  Because of this
# relationship, the abandon timeout for an obtain operation must be somewhat more
# generous than either of the remote side's termination and execution timeouts for
# the capture operation.  That allows the remote side to time out first and return
# its timeout status.  This policy is a polite way of accepting the remote timeout
# instead of shutting the door just as the remote system is about to send its
# timed-out response.
#
# The action timeout for a configuration obtain action is specified as an array of
# 2 values, both specified in seconds.  A zero for either of these values will disable
# that portion of the timeout behavior.
#
# * warning timeout:  at this much time since the action started, if the action is
#   still running, a warning message will be logged and sent to Foundation
# * abandon timeout:  at this much time since the action started, if the action is
#   still running, it will be abandoned; this deed will be logged and sent to Foundation
#
# Action-script timeouts (for capture, stop, deploy, and start actions) are specified
# as an array of 3 values, all specified in seconds.  A zero for any of these values
# will disable that portion of the timeout behavior.
#
# * warning timeout:  at this much time since the process started, if the script is
#   still alive, a warning message will be logged and sent to Foundation
# * termination timeout:  at this much time since the process started, if the script is
#   still alive, it will be sent a SIGTERM; this deed will be logged and sent to Foundation
# * execution timeout:  at this much time since the process started, if the script is
#   still alive, it will be sent a SIGKILL; this deed will be logged and sent to Foundation
#
# So for example, a timeout of [0, 30, 45] will not send a warning, but will send SIGTERM
# 30 seconds after the process starts, and SIGKILL 45 seconds after the process starts.
#
# Action scripts are always run as process group leaders.  SIGTERM or SIGKILL will be sent
# to the entire process group, so that all descendants of the script will receive such
# signals as well unless they take steps to remove themself from this process group.
# Any daemon process you initiate via a start action script ought to be doing so as a
# matter of course, and most likely the running of such a daemon will be the last step
# in a start-action script.

# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
# Data deletion specifications
# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

# Cleanup comes in two forms:  erasing of old areas used for replication, and
# pruning of old backups taken of data before rolling it out of production.
# Scheduling of cleanup actions for each application or database is configured
# via the cleanup-period and cleanup-phase settings for that object.
#
# Backups of data that has been rolled out of production are kept for a period which
# is configurable on a per-application, per-database basis.  Backups will be deleted
# in an oldest-first order, and two minimum thresholds must both be met for a backup
# to be removed.  One (min-kept-backups) is based on a count of how many backups
# exist in total for the object, and one (min-backup-time) is based on the age of
# the backup under consideration for deletion.

# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
# Replicated data specifications
# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

# Which components are replicated for an application is specified using these
# possible directives:
#
#     include-trees     names a set of directories to be recursively replicated
#     exclude-trees     names a set of directories to be recursively barred from replication
#     include-files     names a set of individual files to be replicated
#     exclude-files     names a set of individual files to be barred from replication
#
# The individual values for these directives may be specified as ordinary shell glob
# patterns.  Exclusion of trees and files may be useful when a parent directory is to
# be replicated and is thus specified as one of the include-trees components, but some
# sub-trees or individual files are to be ignored.  Exclusion of individual files can
# also be useful when a glob pattern used for some component of include-files would
# sweep up certain files which also ought to be ignored.  When deciding which
# directories and files are to be replicated, the components of the configured values
# are applied in the order shown:  all include-trees, followed by all exclude-trees,
# followed by all include-files, followed by all exclude-files.
#
# Any file remaining after application of all those components will be replicated,
# with a single exception which is automatically applied and cannot be overridden.
# That is, any files which are either setuid and setgid will be silently dropped,
# as they are considered to be a potential security risk.

# Which components are replicated for a database is specified using these
# possible directives:
#
#     include-tables    names a set of tables to be replicated
#     exclude-tables    names a set of tables to be barred from replication
#
# The individual values for these directives may be specified as ordinary database
# string wildcard patterns, to be matched against table names.  Thus a typical pattern
# is "%" to list all tables in a MySQL database.  When deciding which tables are to be
# replicated in a given database, the components of the configured values are applied
# in the order shown:  all include-tables, followed by all exclude-tables.  Any table
# whose name remains after application of these components will be replicated.

# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
# Object dependencies
# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

# In the current release of the Replication Engine, dependencies between objects
# (applications and databases) can be noted here, but the code does not take advantage
# of such knowledge when it needs to bounce applications.  This may result in some
# applications being bounced more times than you might otherwise expect.  Also, the
# present code does not detect or deal with scheduling conflicts as to when the same
# applications must be bounced in association with replication of different objects.
# The upshot is that prevention of conflicts is therefore currently a manually managed
# constraint, imposed by ensuring that the scheduling of replication for possibly
# conflicting objects does not overlap.
#
# When a particular database is involved, typically you will want to schedule the
# replication of application files first, and then the database shortly thereafter.

# ----------------------------------------------------------------
# Applications to replicate
# ----------------------------------------------------------------

applications:
    -
        # To bounce during source replication:  gwservices.
        # To bounce during target replication:  gwservices.
        #
        # This category subsumes the Foundation services, Event Console,
        # Status Viewer, and Reports.
        #
        application-name: foundation
        aliases         : [gw, gwservices]
        conditions      : always
        databases       : [GWCollageDB, jbossportal]
        cleanup-period  : daily
        cleanup-phase   : 03:12
        sync-period     : daily
        sync-phase      : 03:17
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [  0,  30,  45]
        deploy-timeouts : [200, 220, 230]
        start-timeouts  : [  0,  30,  45]
        cleanup-timeouts: [  0,  90, 100]
        min-kept-backups: 5
        min-backup-time : 15 days
        include-trees   :
                        # config files
                        - /usr/local/groundwork/config/
                        # custom BIRT reports
                        - /usr/local/groundwork/gwreports/
                        # performance view definitions
                        - /usr/local/groundwork/core/performance/performance_views/
                        # log reports
                        # deprecated and probably not working
        include-files   :
                        # JBoss login configuration, already subsumed above:
                        # /usr/local/groundwork/config/jboss/login-config.xml
                        # Event Console config files, already subsumed above:
                        # /usr/local/groundwork/config/console-admin-config.xml
                        # /usr/local/groundwork/config/console.properties
                        # Status Viewer config file, already subsumed above:
                        # /usr/local/groundwork/config/statusviewer.properties

    -
        # To bounce during source replication:  nothing.
        # To bounce during target replication:  snmptrapd and snmptt.
        #
        # /usr/local/groundwork/common/sbin/snmptt
        # /usr/local/groundwork/common/sbin/snmptrapd
        #
        application-name: snmp-trap-handling
        aliases         : [snmp]
        conditions      : always
        cleanup-period  : minicron
        cleanup-phase   : "*/30+2"
        sync-period     : minicron
        sync-phase      : "*/30+7"
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [0, 30, 45]
        deploy-timeouts : [0, 30, 45]
        start-timeouts  : [0, 30, 45]
        cleanup-timeouts: [0, 90, 100]
        min-kept-backups: 5
        min-backup-time : 3 days
        include-trees   :
                        # main repository of MIBs
                        - /usr/local/groundwork/common/share/snmp/mibs/
                        # config files, other MIBs
                        - /usr/local/groundwork/common/etc/snmp/
        include-files   :
                        # persistent data file (not to be locally edited; see the file)
                        # /usr/local/groundwork/common/var/net-snmp/snmptrapd.conf

    -
        # To bounce during source replication:  nothing.
        # To bounce during target replication:  syslog-ng daemon.
        #
        # /usr/local/groundwork/common/sbin/syslog-ng
        #
        application-name: syslog-ng
        aliases         : [syslog]
        conditions      : always
        cleanup-period  : daily
        cleanup-phase   : 04:52
        sync-period     : daily
        sync-phase      : 04:57
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [0, 30, 45]
        deploy-timeouts : [0, 30, 45]
        start-timeouts  : [0, 30, 45]
        cleanup-timeouts: [0, 90, 100]
        min-kept-backups: 5
        min-backup-time : 3 days
        include-files   :
                        # config file
                        - /usr/local/groundwork/common/etc/syslog-ng.conf

    -
        # To bounce during source replication:  all monarch-related scripting
        # To bounce during target replication:  all monarch-related scripting
        #
        # Theoretically, we would like to block monarch.cgi, monarch_auto.cgi,
        # and monarch_discover.cgi somehow, including commit operations, while
        # we copy the files here.  But those scripts are accessed via our UI,
        # and we don't currently have controls in place to provide such an
        # application-level blocking capability.  In the absence of those
        # blocks, it will be up to the site to understand when the replication
        # operations here are scheduled to occur, and to avoid running any
        # Monarch-related operations at those times.
        #
        # This category subsumes configuration and related files for both Monarch and
        # Auto-Discovery.
        #
        application-name: monarch
        conditions      : always
        databases       : [monarch]
        cleanup-period  : daily
        cleanup-phase   : ["06:12", "18:12"]
        sync-period     : daily
        sync-phase      : ["06:17", "18:17"]
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [15, 30, 45]
        deploy-timeouts : [15, 30, 45]
        start-timeouts  : [15, 30, 45]
        cleanup-timeouts: [0, 90, 100]
        min-kept-backups: 5
        min-backup-time : 15 days
        include-trees   :
                        # backups (as manually instigated during Commit operations)
                        # Theoretically, we might want to transfer these files, but
                        # in practice the set as a whole turns out to be huge (for
                        # instance, we have seen 100 MB of such backups at a site),
                        # and that will tie up a long thin transmission pipe between
                        # the Primary and DR sites for a long time on each cycle.
                        # Given that we already back up the existing configuration
                        # data on the target system on each replication cycle, it
                        # is likely that we will have the need for such backups
                        # already covered in that way.
                        # - /usr/local/groundwork/core/monarch/backup/
                        # profiles
                        - /usr/local/groundwork/core/profiles/
                        # I thought that perhaps there was something under
                        # /usr/local/groundwork/core/monarch/automation/
                        # that needed replication (discovery schemas, automation schemas,
                        # and perhaps other kinds of templates), but perhaps not.
                        # Custom automation scripts (if any) live here:
                        - /usr/local/groundwork/core/monarch/automation/scripts/
                        # Saved auto-discovery searches, not yet processed:
                        - /usr/local/groundwork/core/monarch/automation/data/

    -
        # To bounce during source replication:  nothing.
        # To bounce during target replication:  nagios.
        #
        # Given that there is nothing to bounce on the source side when we replicate, we
        # replicate the nagios application on a fairly frequent basis (hourly, in our
        # standard setup here), to ensure that we pick up the user comments, commands,
        # and scheduled downtime with a limited lag time and thus limited loss of data
        # should a DR event occur.  We don't worry about Commit initiated from the
        # Primary server, and needing to interlock with that, because the Primary/DR
        # setup won't be constructed as one would a Standby Server.  Instead, all the
        # nagios/etc/ files will be copied directly here during replication.
        #
        application-name: nagios
        conditions      : always
        cleanup-period  : daily
        cleanup-phase   : 05:02
        sync-period     : minicron
        sync-phase      : 37
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [0, 30, 45]
        deploy-timeouts : [0, 50, 60]
        start-timeouts  : [0, 30, 45]
        cleanup-timeouts: [0, 90, 100]
        min-kept-backups: 5
        min-backup-time : 15 days
        include-trees   :
                        # Nagios configuration
                        - /usr/local/groundwork/nagios/etc/
                        # plugins (may be some custom plugins)
                        - /usr/local/groundwork/nagios/libexec/
                        # CGIs (may be some custom CGIs)
                        - /usr/local/groundwork/nagios/sbin/
                        # images, stylesheets, and similar stuff that might be needed by CGIs
                        - /usr/local/groundwork/nagios/share/
        include-files   :
                        # possible config files for plugins
                        # - (none standard yet)
                        # possible locally-configured regex files used for plugins (no
                        # standard path exists for such files, so we won't show one here)
                        # - (none standard yet)
                        # potentially chocolate version of a script (or config file for the
                        # script); generally these sorts of things will already be subsumed
                        # above, but check for your specific files of concern
                        # - /usr/local/groundwork/nagios/libexec/check_syslog_gw.pl
                        # For the time being, this file is how we will transfer user comments,
                        # commands, and scheduled downtime from the Primary to the DR server.
                        # In the future, we might transform parent/child dependencies and other
                        # info in this file when we install it on the secondary server.
                        /usr/local/groundwork/nagios/var/nagiosstatus.sav

    -
        # To bounce during source replication:  block/kill cron jobs.
        # To bounce during target replication:  block/kill cron jobs.
        #
        # temporarily block cron script:  /usr/local/groundwork/common/bin/cacti_cron.sh
        # temporarily block cron script:  /usr/local/groundwork/foundation/feeder/find_cacti_graphs
        # temporarily block cron script:  /usr/local/groundwork/nms/tools/automation/scripts/extract_cacti.pl
        #
        application-name: cacti
        conditions      : NMS-installed
        databases       : [cacti]
        cleanup-period  : daily
        cleanup-phase   : ["06:02", "18:02"]
        sync-period     : daily
        sync-phase      : ["06:07", "18:07"]
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [0, 30, 45]
        deploy-timeouts : [0, 30, 45]
        start-timeouts  : [0, 30, 45]
        cleanup-timeouts: [0, 90, 100]
        min-kept-backups: 5
        min-backup-time : 15 days
        include-trees   :
                        # resources
                        - /usr/local/groundwork/nms/applications/cacti/resource/
                        # scripts
                        - /usr/local/groundwork/nms/applications/cacti/scripts/
                        # plugins
                        - /usr/local/groundwork/nms/applications/cacti/plugins/
                        # If you want to replicate Cacti RRA files, this is where they are found.
                        # However, you are advised against doing this if you run Cacti replication
                        # more than twice a day, given that this could be a large volume of
                        # replication and blocking Cacti during the replication may cause an
                        # undefined point to appear in the graphs at that time.  Instead, just
                        # let the alternate server maintain its own RRA files, based on its own
                        # probing of the network and machines.
                        # /usr/local/groundwork/nms/applications/cacti/rra/

    -
        # To bounce during source replication:  kill cron script if running; restart iff was killed.
        # To bounce during target replication:  kill cron script if running; restart iff was killed.
        #
        # block cron job:  /usr/local/groundwork/nms/applications/nedi/nedi.pl
        # block cron job:  /usr/local/groundwork/nms/tools/automation/scripts/extract_nedi.pl
        # block cron job:  /usr/local/groundwork/nms/applications/nedi/nedi.pl  (second copy)
        #
        # FIX MAJOR:  Note that the nedi.pl script is one of the scripts that itself is intentionally
        # replicated, so we cannot play games with renaming it during the replication actions.
        #
        application-name: nedi
        conditions      : NMS-installed
        databases       : [nedi]
        cleanup-period  : daily
        cleanup-phase   : 05:32
        sync-period     : daily
        sync-phase      : 05:37
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [0, 30, 45]
        deploy-timeouts : [0, 30, 45]
        start-timeouts  : [0, 30, 45]
        cleanup-timeouts: [0, 90, 100]
        min-kept-backups: 5
        min-backup-time : 15 days
        include-trees   :
                        # system objects
                        - /usr/local/groundwork/nms/applications/nedi/sysobj/
        include-files   :
                        # script
                        - /usr/local/groundwork/nms/applications/nedi/nedi.pl
                        # config file
                        - /usr/local/groundwork/nms/applications/nedi/nedi.conf

    -
        # To bounce during source replication:  nothing (just an extension to Cacti).
        # To bounce during target replication:  nothing (just an extension to Cacti).
        #
        application-name: weathermap
        aliases         : [weather, map]
        conditions      : NMS-installed
        cleanup-period  : minicron
        cleanup-phase   : 21
        sync-period     : minicron
        sync-phase      : 23
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [0, 30, 45]
        deploy-timeouts : [0, 30, 45]
        start-timeouts  : [0, 30, 45]
        cleanup-timeouts: [0, 90, 100]
        min-kept-backups: 5
        min-backup-time : 15 days
        include-trees   :
                        # all things weathermap
                        - /usr/local/groundwork/nms/applications/cacti/plugins/weathermap/
                        # maps (already subsumed above)
                        # /usr/local/groundwork/nms/applications/cacti/plugins/weathermap/configs/
                        # custom images and images referred to by config files (already subsumed above)
                        # /usr/local/groundwork/nms/applications/cacti/plugins/weathermap/images/
        exclude-trees   :
                        #
                        - /usr/local/groundwork/nms/applications/cacti/plugins/weathermap/output/

    -
        # To bounce during source replication:  nothing.
        # To bounce during target replication:  ntop daemon (/etc/init.d/nms-ntop)
        #
        application-name: ntop
        conditions      : NMS-installed
        cleanup-period  : daily
        cleanup-phase   : 05:12
        sync-period     : daily
        sync-phase      : 05:17
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [0, 30, 45]
        deploy-timeouts : [0, 30, 45]
        start-timeouts  : [0, 30, 45]
        cleanup-timeouts: [0, 90, 100]
        min-kept-backups: 5
        min-backup-time : 15 days
        include-trees   :
                        # plugins
                        - /usr/local/groundwork/nms/applications/ntop/lib/ntop/plugins/
                        - /usr/local/groundwork/nms/applications/ntop/lib/plugins/
                        # config file
                        - /usr/local/groundwork/common/etc/ntop/
                        - /usr/local/groundwork/nms/applications/ntop/etc/ntop/
                        # Preferably, you would generate ntop RRDs on both servers.  But if
                        # you chose to replicate them instead, here is where they reside.
                        # /usr/local/groundwork/nms/applications/ntop/db/rrd/
        exclude-files   :
                        # gdbm databases
                        - /usr/local/groundwork/nms/applications/ntop/db/*.db

# ----------------------------------------------------------------
# Databases to replicate
# ----------------------------------------------------------------

databases:
    -
        # To bounce during source replication:  gwservices.
        # To bounce during target replication:  gwservices.
        #
        database-name   : GWCollageDB
        aliases         : [foundation, collage]
        conditions      : always
        applications    : [groundwork-monitor]
        databases       : [jbossportal]
        replication-type: periodic
        cleanup-period  : daily
        cleanup-phase   : 03:22
        sync-period     : daily
        sync-phase      : 03:27
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [  0,  30,  45]
        deploy-timeouts : [200, 220, 230]
        start-timeouts  : [  0,  30,  45]
        cleanup-timeouts: [  0,  90, 100]
        min-kept-backups: 5
        min-backup-time : 15 days
        credentials-file: /usr/local/groundwork/config/db.properties
        credentials-type: db-properties
        include-tables  :
                        - Action
                        - ActionParameter
                        - ActionProperty
                        - ActionType
                        - ApplicationAction
                        - ApplicationType
                        - ConsolidationCriteria
                        - OperationStatus

    -
        # To bounce during source replication:  gwservices.
        # To bounce during target replication:  gwservices.
        #
        database-name   : jbossportal
        conditions      : always
        databases       : [GWCollageDB]
        replication-type: periodic
        cleanup-period  : daily
        cleanup-phase   : 03:32
        sync-period     : daily
        sync-phase      : 03:37
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [  0,  30,  45]
        deploy-timeouts : [200, 220, 230]
        start-timeouts  : [  0,  30,  45]
        cleanup-timeouts: [  0,  90, 100]
        min-kept-backups: 5
        min-backup-time : 15 days
        credentials-file: /usr/local/groundwork/foundation/container/webapps/jboss/portal-ds.xml
        credentials-type: portal-ds-xml
        include-tables  : "%"

    -
        # To bounce during source replication:  all monarch-related scripting
        # To bounce during target replication:  all monarch-related scripting;
        #     synchronize Nagios and Foundation with Monarch by running a
        #     Commit operation on the target side
        #
        # Theoretically, we would like to block monarch.cgi, monarch_auto.cgi,
        # and monarch_discover.cgi somehow, including commit operations, while
        # we copy the files here.  But those scripts are accessed via our UI,
        # and we don't currently have controls in place to provide such an
        # application-level blocking capability.  In the absence of those
        # blocks, it will be up to the site to understand when the replication
        # operations here are scheduled to occur, and to avoid running any
        # Monarch-related operations at those times.
        #
        # If you want more frequent saving of the monarch database on the
        # secondary system, perhaps the way to do so is to use replication
        # of the monarch application instead, to provide copies saved on the
        # secondary system.  Replicating the monarch application copies the
        # /usr/local/groundwork/core/monarch/backup/ directory.  One might
        # need to restore the latest backup manually on the secondary system,
        # but at least the data would be available there.
        #
        database-name   : monarch
        conditions      : always
        applications    : [monarch]
        # Someday we'd like to use continuous replication at the database level for
        # the monarch database, but that day awaits a robust and tested implementation.
        # replication-type: continuous
        replication-type: periodic
        cleanup-period  : daily
        cleanup-phase   : ["06:17", "18:17"]
        sync-period     : daily
        sync-phase      : ["06:22", "18:22"]
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [  0,  30,  45]
        deploy-timeouts : [280, 300, 320]
        start-timeouts  : [  0,  30,  45]
        cleanup-timeouts: [  0,  90, 100]
        min-kept-backups: 5
        min-backup-time : 15 days
        credentials-file: /usr/local/groundwork/config/db.properties
        credentials-type: db-properties
        include-tables  : "%"
        exclude-tables  :
                        - datatype
                        - host_service
                        - sessions

    -
        # To bounce during source replication:  block/kill cron jobs.
        # To bounce during target replication:  block/kill cron jobs.
        #
        # temporarily block cron script:  /usr/local/groundwork/common/bin/cacti_cron.sh
        # temporarily block cron script:  /usr/local/groundwork/foundation/feeder/find_cacti_graphs
        # temporarily block cron script:  /usr/local/groundwork/nms/tools/automation/scripts/extract_cacti.pl
        #
        database-name   : cacti
        conditions      : NMS-installed
        applications    : [cacti]
        replication-type: periodic
        cleanup-period  : daily
        cleanup-phase   : ["06:32", "18:32"]
        sync-period     : daily
        sync-phase      : ["06:37", "18:37"]
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [0, 30, 45]
        deploy-timeouts : [0, 30, 45]
        start-timeouts  : [0, 30, 45]
        cleanup-timeouts: [0, 90, 100]
        min-kept-backups: 5
        min-backup-time : 15 days
        credentials-file: /usr/local/groundwork/enterprise/config/enterprise.properties
        credentials-type: enterprise-properties
        # FIX MINOR:  the set of tables in this database which ought to be replicated
        # is subject to modification after testing
        include-tables  : "%"

    -
        # To bounce during source replication:  kill cron script if running; restart iff was killed.
        # To bounce during target replication:  kill cron script if running; restart iff was killed.
        #
        # block cron job:  /usr/local/groundwork/nms/applications/nedi/nedi.pl
        # block cron job:  /usr/local/groundwork/nms/tools/automation/scripts/extract_nedi.pl
        # block cron job:  /usr/local/groundwork/nms/applications/nedi/nedi.pl  (second copy)
        #
        database-name   : nedi
        conditions      : NMS-installed
        applications    : [nedi]
        replication-type: periodic
        cleanup-period  : daily
        cleanup-phase   : 05:52
        sync-period     : daily
        sync-phase      : 05:57
        obtain-timeouts : [320, 350]
        capture-timeouts: [280, 300, 330]
        stop-timeouts   : [0, 30, 45]
        deploy-timeouts : [0, 30, 45]
        start-timeouts  : [0, 30, 45]
        cleanup-timeouts: [0, 90, 100]
        min-kept-backups: 5
        min-backup-time : 15 days
        credentials-file: /usr/local/groundwork/enterprise/config/enterprise.properties
        credentials-type: enterprise-properties
        include-tables  : "%"

